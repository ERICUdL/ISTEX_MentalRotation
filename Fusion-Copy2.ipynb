{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, pickle, argparse, json, os, urllib2\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "########################################################################################\n",
    "def query_from(q, f):\n",
    "    q = q+'&from='+str(f)\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    subject_ids = np.array(range(len(data['hits'])), dtype=np.object)\n",
    "    for (i, hit) in enumerate(data['hits']):\n",
    "        subject_ids[i] = hit['id']\n",
    "    return subject_ids\n",
    "\n",
    "def query(q):\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    nb_requests = 1 + data['total'] / 1000\n",
    "    if nb_requests > 10: # maximum number of pages due to API pagination restrection\n",
    "        nb_requests = 10\n",
    "    subject_ids = query_from(q, 0)\n",
    "    for i in range(nb_requests)[1:]:\n",
    "        f = i * 1000\n",
    "        next_request = query_from(q, f)\n",
    "        subject_ids = np.hstack((subject_ids, next_request))\n",
    "    return subject_ids.tolist()\n",
    "\n",
    "def find_intersection(list_a, list_b):\n",
    "    return list(set(list_a) & set(list_b))\n",
    "\n",
    "def term2url(string):\n",
    "    string = string.split(' ')\n",
    "    res = '%22'\n",
    "    for s in string:\n",
    "        res = res + s + '%20'\n",
    "    res = res[:-3]\n",
    "    res = res + '%22'\n",
    "    return res\n",
    "\n",
    "def babel_synset(synset):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    for syn in synset:\n",
    "        syn = term2url(syn)\n",
    "        q = q + 'title:' + syn + '%20OR%20abstract:' + syn + '%20OR%20'\n",
    "    q = q[:-8]\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    "\n",
    "def babel_subj_keyword(topic):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    topic = term2url(topic)\n",
    "    q = q+ 'subject.value:' + topic + '%20OR%20keywords:' + topic\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    " \n",
    "def babel_title_abst(topic):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    topic = term2url(topic)\n",
    "    q = q+ 'title:' + topic + '%20OR%20abstract:' + topic\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    " \n",
    "def babelnet_syn_get_input(topic, synset):\n",
    "    results = query(babel_synset(synset))\n",
    "    _gs = query(babel_subj_keyword(topic))\n",
    "    results = find_intersection(results, inversed_index.keys())\n",
    "    _abst_title = query(babel_title_abst(topic))\n",
    "    test_set = _inter = {x for x in _gs if x not in _abst_title}\n",
    "    test_set = find_intersection(test_set, inversed_index.keys())\n",
    "    results = list(results)\n",
    "    test = list(test_set)\n",
    "    print 'initial_corpus size:', len(find_intersection(_abst_title, inversed_index.keys()))\n",
    "    return results, test\n",
    "\n",
    "def babelnet_eval(topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    print 'results size of the topic \"' + topic + '\":', len(babelnet_results) \n",
    "    print 'ground truth size', len(test)\n",
    "    babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "    babel_test_intersection_size = len(babel_test_intersection)\n",
    "    print 'intersection with the ground truth:', babel_test_intersection_size\n",
    "    \n",
    "def top_thresh(ordered_dict_pickle, thresh):\n",
    "    ranked_all = pickle.load(open(ordered_dict_pickle, 'rb'))\n",
    "    ranked_all_np = np.array(ranked_all.items())\n",
    "    ranked_all_df = pd.DataFrame(data=ranked_all_np, index=None, columns=['istex_id', 'score'])\n",
    "    ranked_all_df['score'] = ranked_all_df[['score']].astype(float)\n",
    "    return ranked_all_df[ranked_all_df['score'] > thresh]\n",
    "\n",
    "def top_thresh_lst(res_lst_pickle, thresh):\n",
    "    ranked_all = pickle.load(open(res_lst_pickle, 'rb'))\n",
    "    if type(ranked_all) is OrderedDict:\n",
    "        ranked_all = ranked_all.items()[:100000]\n",
    "    ranked_all_np = np.array(ranked_all)\n",
    "    ranked_all_df = pd.DataFrame(data=ranked_all_np, index=None, columns=['istex_id', 'score'])\n",
    "    ranked_all_df['score'] = ranked_all_df[['score']].astype(float)\n",
    "    return ranked_all_df[ranked_all_df['score'] > thresh]\n",
    "\n",
    "def babelnet_eval_PR(topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    print 'babelnet results size of the topic \"' + topic + '\":', len(babelnet_results) \n",
    "    print 'ground truth size', len(test)\n",
    "    babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "    babel_test_intersection_size = len(babel_test_intersection)\n",
    "    print 'intersection with the ground truth:', babel_test_intersection_size\n",
    "    precision = babel_test_intersection_size / float(len(babelnet_results))\n",
    "    recall = babel_test_intersection_size / float(len(test))\n",
    "    if babel_test_intersection_size is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0.0 \n",
    "    print \"F1\", F1\n",
    "    print 'babel precision: ', precision\n",
    "    print 'babel recall: ', recall\n",
    "    \n",
    "\n",
    "#Evaluate 3SH results at treshold\n",
    "def eval_all_at_thresh(ordered_dict_pickle, topic, synset, thresh):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    if n > 10000:\n",
    "        thresh = thresh + 0.1\n",
    "        top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    elif n < 1000:\n",
    "        thresh = thresh - 0.1\n",
    "        top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t, thresh\n",
    "\n",
    "#Evaluate 3SH results list at treshold\n",
    "def eval_all_at_thresh_lst(res_pickle, topic, synset, thresh=0.75):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    if n > 10000:\n",
    "        thresh = thresh + 0.1\n",
    "        top_res = top_thresh_lst(res_pickle, thresh)\n",
    "        if len(top_res) > 10000:\n",
    "            thresh = thresh + 0.05\n",
    "            top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    elif n < 1000:\n",
    "        thresh = thresh - 0.1\n",
    "        top_res = top_thresh_lst(res_pickle, thresh)\n",
    "        if len(top_res) > 1000:\n",
    "            thresh = thresh - 0.05\n",
    "            top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"length of s3h results: \", n, \"length of test set\", t\n",
    "    print \"F1: \", F1\n",
    "    print \"precision s3h: \", precision, \"recall s3h: \", recall\n",
    "\n",
    "def get_fusion_res3(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res4(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * 2 * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res5(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(range(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res6(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(range(100000))\n",
    "    fus = fus * 3 * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (2*i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "#Evaluate fusion df\n",
    "def eval_all(fusion_df, topic, synset, res_pickle, thresh=0.75):\n",
    "    eval_all_at_thresh_lst(res_pickle, topic, synset, thresh)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    t = len(test)\n",
    "\n",
    "    print \"length of results: \", n, \"length of test set\", t\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 2 * len(babelnet_res)\n",
    "    print \"for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 3 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "    \n",
    "    n = 4 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "    \n",
    "    n = 5 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1  \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original inversed_index\n",
      "[(u'ISTEX_D89FA3AC3521074D46F4245762153DF497BFFA1F', 2002320), (u'ISTEX_18EAF4D6A126B077EB38667801D1B7292F32FF49', 2483732), (u'ISTEX_5F91044435FCC4FABB9F02E31467DCFE75F4A7BE', 1429049)]\n",
      "processed inversed_index\n",
      "[(u'FCF1393F9B8136AC08FB67E88F94F3CF62C17288', 3517138), (u'482E1102A1114327A744FD2ADB4D9F8FF7E9A70B', 751643), (u'A81022B6295AE66F68A10222C3B94A06B033C1BA', 3983232)]\n"
     ]
    }
   ],
   "source": [
    "#loading SDV of istex articles\n",
    "inv_index = json.load(open('../RecSys_Exp_files/182_381_vec150_results/output_paragraph_inversed_index.json','rb'))\n",
    "print 'original inversed_index'\n",
    "print inv_index.items()[:3]\n",
    "inversed_index = dict()\n",
    "for (k, v) in inv_index.items():\n",
    "    key = k.split('_')[1]\n",
    "    inversed_index[key] = v\n",
    "print 'processed inversed_index'\n",
    "print inversed_index.items()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = (fus + len(babel_results)) / 2\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res3(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'organ transplant, transplant, transplantation, Organ transplantation, Medical Transplantation, Transplant Surgery, Black market organs, First transplant, First transplantation, Intestinal transplant, Live organ transplants, Mixed chimerism, Organ and Tissue Donor, Organ doner, Organ farming, Organ transplantation in different countries, Organ transplantation therapy, Organ transplants, Skin transplant, Tissue transplant, Transplant Tourism and Organ Trafficking, Transplantation medicine, Transplantation surgery, Transplantation therapy, Transplanted organs, Transplantology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Transplantation'\n",
    "s3h_res_pickle = \"results/res_Transplantation\"\n",
    "Transplantation_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'spectroscopy, spectrographic analysis, spectrometry, spectroscopic analysis, spectrum analysis, Fingerprint region, Laser spectroscopy, Optical spectroscopy, Atomic line, Atomic line spectra, Atomic spectra, Atomic spectral line, Electromagnetic spectroscopy, Emission spectrochemical analysis, Spectrochemical Analysis, Spectrography, Spectrology, Spectroscopic, Spectroscopist, Spectroscopists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Spectroscopy'\n",
    "s3h_res_pickle = \"results/res__Spectroscopy\"\n",
    "Spectroscopy_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = ' surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "s3h_res_pickle = \"results/res__Surgery\"\n",
    "Surgery_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'optics, Classical optics, Light physics, Optical, Optical system'\n",
    "synset = bab.split(',')\n",
    "topic = 'Optics'\n",
    "s3h_res_pickle = \"results/res__Optics\"\n",
    "Optics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "s3h_res_pickle = \"results/res__Literature\"\n",
    "Literature_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "s3h_res_pickle = \"results/res__Toxicology\"\n",
    "Toxicology_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b4eb30782151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'artificial intelligence'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms3h_res_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results/AI_results.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mAI_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3h_res_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-2f95e6519d8b>\u001b[0m in \u001b[0;36mget_fusion_res\u001b[0;34m(s3h_res_pickle, topic, synset)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3h\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_s3h_top100k_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms3h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mfus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "s3h_res_pickle = \"results/AI_results.pickle\"\n",
    "AI_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "s3h_res_pickle = \"results/res__Cybernetics\"\n",
    "Cybernetics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'information system, data system, information systems, system info, Business computing, Computer information system, Computer information systems, Elements of Information System, Information in Computer Science, Information systems and technology, Information systems discipline, Information systems theory, Informationssystem'\n",
    "synset = bab.split(',')\n",
    "topic = 'Information Systems'\n",
    "s3h_res_pickle = \"results/infosys_results.pickle\"\n",
    "Information_Systems_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "s3h_res_pickle = \"results/res__Immunology\"\n",
    "Immunology_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'infection, infectious disease, communicable diseases, contagion, Infectious diseases, Acute infection, AIDS-related bacterial infections, AIDS-related viral infections, Anti-infective, Anti-infectives, Antiinfective, Bacterial Infections, Communicable disease, Contagious diseases, Definition to contagious, Infect, Infecting, Infectiology, Infections, Infectious, Infectious disease epidemiology, Infectious disease medicine, Infectology, Local infection, Primary infection, Rochalimea infections, Secondary infection, Tropical bacterial infections, Tropical infections, Viral Infections, Wound colonization, Wound infection'\n",
    "synset = bab.split(',')\n",
    "topic = 'Infectious Diseases'\n",
    "s3h_res_pickle = \"results/Infectious_results.pickle\"\n",
    "Infectious_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "s3h_res_pickle = \"results/res__Biomaterials\"\n",
    "Biomaterials_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'ceramics, Ceramic art, Art pottery, Art ware, Ceramic artist, Ceramic paint, Ceramics art, Fine art pot, Vase painting'\n",
    "synset = bab.split(',')\n",
    "topic = 'Ceramics'\n",
    "s3h_res_pickle = \"results/res__Ceramics\"\n",
    "Ceramics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic = \"biophysics\"\n",
    "synset_text = \"biophysics, Biological physics, Biophysical, Biophysicists, History of biophysics\"\n",
    "synset = synset_text.split(',')\n",
    "s3h_res_pickle = 'results/res__biophysics'\n",
    "biophysics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_dict = OrderedDict((k,i) for i,k in enumerate(biophysics_fusion_res))\n",
    "babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "inter = set(ind_dict).intersection(babel_results)\n",
    "indices = [ind_dict[x] for x in inter ]\n",
    "cutt = np.array(indices).max()\n",
    "cutt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slic = len(indices) * 0.8\n",
    "indices_slic = indices[:int(slic)]\n",
    "#indices_slic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fusion_res = fusion_df.sort_values(\"fusin_rank\")[\"istex_id\"].tolist()\n",
    "#fusion_res[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df with manual cut\n",
    "def eval_fusin_at_thresh_lst_1k(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    if n < 1000:\n",
    "        n = 1000\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_5k(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = 5000#len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_2_bablesize((fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = 2 * len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_bablesize(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res_2(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_top100k_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = (fus + 100000) / 2\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_all(fusion_df, topic, synset, res_pickle, tresh=0.75):\n",
    "    eval_all_at_thresh_lst(res_pickle, topic, synset, thresh)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    t = len(test)\n",
    "\n",
    "    print \"length of results: \", n, \"length of test set\", t\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 2 * len(babelnet_res)\n",
    "    print \"for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 3 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004853496314937983, 0.017396907216494846, 5563, 1552)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "get_fusion_res_2(\"results/res__Toxicology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004853496314937983, 0.017396907216494846, 5563, 1552)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "get_fusion_res_2(\"results/res__Toxicology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Toxicology_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00356261989586188, 0.012745098039215686, 3649, 1020)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "get_fusion_res_2(\"results/res__Biomaterials\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Biomaterials_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n",
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 7298, 1020)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "get_fusion_res_2(\"results/res__Biomaterials\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Biomaterials_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.010356208034596564, 0.03816313692598029, 17574, 4769)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "get_fusion_res_2(\"results/res__Immunology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Immunology_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "0.0124 0.0130006290627 5000 4769\n"
     ]
    }
   ],
   "source": [
    "eval_fusin_at_thresh_lst(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "0.0119494708091 0.0220171943804 8787 4769\n"
     ]
    }
   ],
   "source": [
    "eval_fusin_at_thresh_lst3(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "get_fusion_res_2(\"results/res__Immunology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Cybernetics\": 511\n",
      "ground truth size 596\n",
      "intersection with the ground truth: 47\n",
      "babel precision:  0.0919765166341\n",
      "babel recall:  0.0788590604027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004, 0.006711409395973154, 1000, 596)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "get_fusion_res_2(\"\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Cybernetics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Cybernetics\": 511\n",
      "ground truth size 596\n",
      "intersection with the ground truth: 47\n",
      "babel precision:  0.0919765166341\n",
      "babel recall:  0.0788590604027\n",
      "0.0283757338552 0.0486577181208 1022 596\n"
     ]
    }
   ],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "Cybernetics_fusion_res = get_fusion_res_2(\"results/res__Cybernetics\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Cybernetics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_all_at_thresh('results/AI_results.pickle', topic , synset, thresh = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Literature\": 7357\n",
      "ground truth size 860\n",
      "intersection with the ground truth: 107\n",
      "babel precision:  0.0145439717276\n",
      "babel recall:  0.124418604651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0020388745412532284, 0.01744186046511628, 7357, 860)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Literature_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Optics\": 8730\n",
      "ground truth size 3286\n",
      "intersection with the ground truth: 271\n",
      "babel precision:  0.0310423825888\n",
      "babel recall:  0.0824710894705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.015005727376861398, 0.039866098600121726, 8730, 3286)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'optics, Classical optics, Light physics, Optical, Optical system'\n",
    "synset = bab.split(',')\n",
    "topic = 'Optics'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Optics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Surgery\": 8271\n",
      "ground truth size 6412\n",
      "intersection with the ground truth: 565\n",
      "babel precision:  0.0683109660259\n",
      "babel recall:  0.0881160324392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.027687099504292104, 0.03571428571428571, 8271, 6412)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Surgery_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Spectroscopy\": 8513\n",
      "ground truth size 7294\n",
      "intersection with the ground truth: 287\n",
      "babel precision:  0.0337131446024\n",
      "babel recall:  0.0393474088292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.025138024198284977, 0.029339182890046615, 8513, 7294)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'spectroscopy, spectrographic analysis, spectrometry, spectroscopic analysis, spectrum analysis, Fingerprint region, Laser spectroscopy, Optical spectroscopy, Atomic line, Atomic line spectra, Atomic spectra, Atomic spectral line, Electromagnetic spectroscopy, Emission spectrochemical analysis, Spectrochemical Analysis, Spectrography, Spectrology, Spectroscopic, Spectroscopist, Spectroscopists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Spectroscopy'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Spectroscopy_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 581\n",
      "initial_corpus size: 581\n",
      "babelnet results size of the topic \"Condensed Matter\": 1523\n",
      "ground truth size 1514\n",
      "intersection with the ground truth: 1\n",
      "babel precision:  0.000656598818122\n",
      "babel recall:  0.000660501981506\n",
      "initial_corpus size: 581\n",
      "length of s3h results:  9988 length of test set 1514\n",
      "F1:  0.0153016866632\n",
      "precision s3h:  0.00881057268722 recall s3h:  0.0581241743725\n",
      "initial_corpus size: 581\n",
      "length of results:  1523 length of test set 1514\n",
      "F1:  0.00329272308199\n",
      "precision fusion:  0.00328299409061 recall fusion:  0.00330250990753\n",
      "for length of results:  3046\n",
      "F1:  0.00394736842105\n",
      "precision fusion:  0.00295469468155 recall fusion:  0.00594451783355\n",
      " for length of results:  4569\n",
      "F1:  0.00493177708368\n",
      "precision fusion:  0.00328299409061 recall fusion:  0.00990752972259\n"
     ]
    }
   ],
   "source": [
    "bab = \"Condensed matter physics, Condensed-matter physics, Condensed matter, Bulk matter, Condenced matter, Condensed matter physicist, Condensed matter system, Condensed matter theory, Condensed phase, History of condensed matter physics, Physics of condensed matter, Theoretical condensed matter physics\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Condensed Matter'\n",
    "ID_fusion_res = get_fusion_res3(\"results/res__Condensed_matter\", topic, synset)\n",
    "eval_all(ID_fusion_res, topic, synset, \"results/res__Condensed_matter\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 810\n",
      "initial_corpus size: 810\n",
      "babelnet results size of the topic \"Emergency medicine\": 1379\n",
      "ground truth size 285\n",
      "intersection with the ground truth: 14\n",
      "F1 0.0168269230769\n",
      "babel precision:  0.010152284264\n",
      "babel recall:  0.0491228070175\n",
      "initial_corpus size: 810\n",
      "length of s3h results:  2793 length of test set 285\n",
      "F1:  0.00779727095517\n",
      "precision s3h:  0.00429645542427 recall s3h:  0.0421052631579\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -3] Temporary failure in name resolution>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7e2aed5fd036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mres_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results/Emergence_results.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mID_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID_fusion_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-75a801d7cb86>\u001b[0m in \u001b[0;36meval_all\u001b[0;34m(fusion_df, topic, synset, res_pickle, thresh)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0meval_all_at_thresh_lst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mbabelnet_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabelnet_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-75a801d7cb86>\u001b[0m in \u001b[0;36mbabelnet_syn_get_input\u001b[0;34m(topic, synset)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0m_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_subj_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_intersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minversed_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-75a801d7cb86>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnb_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 447\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             return self.do_open(httplib.HTTPSConnection, req,\n\u001b[0;32m-> 1241\u001b[0;31m                 context=self._context)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# XXX what error?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>"
     ]
    }
   ],
   "source": [
    "bab = \"emergency medicine, Emergency care, Emergency med, Emergency treatment, Emergent condition, Emergentology, Er physician\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Emergency medicine'\n",
    "res_pickle = \"results/Emergence_results.pickle\"\n",
    "ID_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(ID_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 810\n",
      "babelnet results size of the topic \"Emergency medicine\": 1379\n",
      "ground truth size 285\n",
      "intersection with the ground truth: 14\n",
      "F1 0.0168269230769\n",
      "babel precision:  0.010152284264\n",
      "babel recall:  0.0491228070175\n",
      "initial_corpus size: 810\n",
      "length of s3h results:  2793 length of test set 285\n",
      "F1:  0.00779727095517\n",
      "precision s3h:  0.00429645542427 recall s3h:  0.0421052631579\n",
      "initial_corpus size: 810\n",
      "length of results:  1379 length of test set 285\n",
      "F1:  0.00120192307692\n",
      "precision fusion:  0.000725163161711 recall fusion:  0.00350877192982\n",
      "for length of results:  2758\n",
      "F1:  0.00131449227736\n",
      "precision fusion:  0.000725163161711 recall fusion:  0.00701754385965\n",
      " for length of results:  4137\n",
      "F1:  0.00226142017187\n",
      "precision fusion:  0.00120860526952 recall fusion:  0.0175438596491\n",
      " for length of results:  5516\n",
      "F1:  0.00206860886054\n",
      "precision fusion:  0.00108774474257 recall fusion:  0.0210526315789\n",
      " for length of results:  6895\n",
      "F1:  0.00222841225627\n",
      "precision fusion:  0.00116026105874 recall fusion:  0.0280701754386\n"
     ]
    }
   ],
   "source": [
    "eval_all(ID_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "initial_corpus size: 344\n",
      "babelnet results size of the topic \"Biodiversity Conservation\": 7481\n",
      "ground truth size 55\n",
      "intersection with the ground truth: 12\n",
      "F1 0.0031847133758\n",
      "babel precision:  0.00160406362786\n",
      "babel recall:  0.218181818182\n",
      "initial_corpus size: 344\n",
      "length of s3h results:  3207 length of test set 55\n",
      "F1:  0.00613120784795\n",
      "precision s3h:  0.00311817898347 recall s3h:  0.181818181818\n",
      "initial_corpus size: 344\n",
      "length of results:  7481 length of test set 55\n",
      "F1:  0.00132696390658\n",
      "precision fusion:  0.000668359844941 recall fusion:  0.0909090909091\n",
      "for length of results:  14962\n",
      "F1:  0.000665911966438\n",
      "precision fusion:  0.00033417992247 recall fusion:  0.0909090909091\n",
      " for length of results:  22443\n",
      "F1:  0.000622277535781\n",
      "precision fusion:  0.000311901260972 recall fusion:  0.127272727273\n",
      " for length of results:  29924\n",
      "F1:  0.00046699356216\n",
      "precision fusion:  0.000233925945729 recall fusion:  0.127272727273\n",
      " for length of results:  37405\n",
      "F1:  0.000373731980779\n",
      "precision fusion:  0.000187140756583 recall fusion:  0.127272727273\n"
     ]
    }
   ],
   "source": [
    "bab = \"conservation biology, Animal conservation, Biological Conservation, Conservation, Biodiversity conservation, Conservation biologist, Conservation biologists, Conservation of natural resources, Conservation of wildlife, Conservation priority, Conservation science, Earth biologist, Earth biology, Ecological conservation, History of conservation biology, Wildlife Conservation\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Biodiversity Conservation'\n",
    "res_pickle = \"results/Biodiversity_Conservation_results.pickle\"\n",
    "BC_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(BC_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8999\n",
      "initial_corpus size: 8999\n",
      "babelnet results size of the topic \"Transplantation\": 8975\n",
      "ground truth size 4997\n",
      "intersection with the ground truth: 910\n",
      "F1 0.130260521042\n",
      "babel precision:  0.10139275766\n",
      "babel recall:  0.182109265559\n",
      "initial_corpus size: 8999\n",
      "length of s3h results:  8281 length of test set 4997\n",
      "F1:  0.105889441181\n",
      "precision s3h:  0.0848931288492 recall s3h:  0.140684410646\n",
      "initial_corpus size: 8999\n",
      "length of results:  8975 length of test set 4997\n",
      "F1:  0.0586888061838\n",
      "precision fusion:  0.0456824512535 recall fusion:  0.0820492295377\n",
      "for length of results:  17950\n",
      "F1:  0.0446245696605\n",
      "precision fusion:  0.0285236768802 recall fusion:  0.102461476886\n",
      " for length of results:  26925\n",
      "F1:  0.0412881398409\n",
      "precision fusion:  0.0244753946147 recall fusion:  0.131879127476\n",
      " for length of results:  35900\n",
      "F1:  0.0357972467418\n",
      "precision fusion:  0.0203899721448 recall fusion:  0.146487892736\n",
      " for length of results:  44875\n",
      "F1:  0.0319217196022\n",
      "precision fusion:  0.0177381615599 recall fusion:  0.159295577346\n"
     ]
    }
   ],
   "source": [
    "bab = \"organ transplant, transplant, transplantation, Organ transplantation, Medical Transplantation, Transplant Surgery, Black market organs, First transplant, First transplantation, Intestinal transplant, Live organ transplants, Mixed chimerism, Organ and Tissue Donor, Organ doner, Organ farming, Organ transplantation in different countries, Organ transplantation therapy, Organ transplants, Skin transplant, Tissue transplant, Transplant Tourism and Organ Trafficking, Transplantation medicine, Transplantation surgery, Transplantation therapy, Transplanted organs, Transplantology\"\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Transplantation'\n",
    "res_pickle = \"results/res_Transplantation\"\n",
    "Tr_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(Tr_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8455\n",
      "initial_corpus size: 8455\n",
      "babelnet results size of the topic \"Surgery\": 8271\n",
      "ground truth size 6412\n",
      "intersection with the ground truth: 565\n",
      "F1 0.07695974937\n",
      "babel precision:  0.0683109660259\n",
      "babel recall:  0.0881160324392\n",
      "initial_corpus size: 8455\n",
      "length of s3h results:  13185 length of test set 6412\n",
      "F1:  0.0702148288003\n",
      "precision s3h:  0.0521805081532 recall s3h:  0.107298814722\n",
      "initial_corpus size: 8455\n",
      "length of results:  8271 length of test set 6412\n",
      "F1:  0.0377307089832\n",
      "precision fusion:  0.0334905090074 recall fusion:  0.0432002495321\n",
      "for length of results:  16542\n",
      "F1:  0.0407772065871\n",
      "precision fusion:  0.0282916213275 recall fusion:  0.072988147224\n",
      " for length of results:  24813\n",
      "F1:  0.0491273018415\n",
      "precision fusion:  0.0309112158949 recall fusion:  0.119619463506\n",
      " for length of results:  33084\n",
      "F1:  0.0480554992911\n",
      "precision fusion:  0.0286845605126 recall fusion:  0.148003742982\n",
      " for length of results:  41355\n",
      "F1:  0.045847551657\n",
      "precision fusion:  0.0264780558578 recall fusion:  0.170773549595\n"
     ]
    }
   ],
   "source": [
    "bab = 'surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "res_pickle = \"results/res__Surgery\"\n",
    "Sr_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(Sr_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 7941\n",
      "initial_corpus size: 7941\n",
      "babelnet results size of the topic \"Literature\": 7357\n",
      "ground truth size 860\n",
      "intersection with the ground truth: 107\n",
      "F1 0.0260435682122\n",
      "babel precision:  0.0145439717276\n",
      "babel recall:  0.124418604651\n",
      "initial_corpus size: 7941\n",
      "length of s3h results:  8723 length of test set 860\n",
      "F1:  0.00918292810185\n",
      "precision s3h:  0.00504413619168 recall s3h:  0.0511627906977\n",
      "initial_corpus size: 7941\n",
      "length of results:  7357 length of test set 860\n",
      "F1:  0.00340756967263\n",
      "precision fusion:  0.00190294957184 recall fusion:  0.0162790697674\n",
      "for length of results:  14714\n",
      "F1:  0.0039809939643\n",
      "precision fusion:  0.00210683702596 recall fusion:  0.0360465116279\n",
      " for length of results:  22071\n",
      "F1:  0.00383759975579\n",
      "precision fusion:  0.00199356621811 recall fusion:  0.0511627906977\n",
      " for length of results:  29428\n",
      "F1:  0.00442419440042\n",
      "precision fusion:  0.00227674323773 recall fusion:  0.0779069767442\n",
      " for length of results:  36785\n",
      "F1:  0.00435648824545\n",
      "precision fusion:  0.00222916949844 recall fusion:  0.0953488372093\n"
     ]
    }
   ],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "res_pickle = \"results/res__Literature\"\n",
    "Lit_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(Lit_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 581\n",
      "initial_corpus size: 581\n",
      "babelnet results size of the topic \"Condensed Matter\": 1523\n",
      "ground truth size 1514\n",
      "intersection with the ground truth: 1\n",
      "F1 0.000658544616398\n",
      "babel precision:  0.000656598818122\n",
      "babel recall:  0.000660501981506\n",
      "initial_corpus size: 581\n",
      "length of s3h results:  9988 length of test set 1514\n",
      "F1:  0.0153016866632\n",
      "precision s3h:  0.00881057268722 recall s3h:  0.0581241743725\n",
      "initial_corpus size: 581\n",
      "length of results:  1523 length of test set 1514\n",
      "F1:  0.00197563384919\n",
      "precision fusion:  0.00196979645437 recall fusion:  0.00198150594452\n",
      "for length of results:  3046\n",
      "F1:  0.00789473684211\n",
      "precision fusion:  0.0059093893631 recall fusion:  0.0118890356671\n",
      " for length of results:  4569\n",
      "F1:  0.00986355416735\n",
      "precision fusion:  0.00656598818122 recall fusion:  0.0198150594452\n",
      " for length of results:  6092\n",
      "F1:  0.0128845648172\n",
      "precision fusion:  0.008043335522 recall fusion:  0.0323645970938\n",
      " for length of results:  7615\n",
      "F1:  0.0148975791434\n",
      "precision fusion:  0.00892974392646 recall fusion:  0.0449141347424\n"
     ]
    }
   ],
   "source": [
    "bab = \"Condensed matter physics, Condensed-matter physics, Condensed matter, Bulk matter, Condenced matter, Condensed matter physicist, Condensed matter system, Condensed matter theory, Condensed phase, History of condensed matter physics, Physics of condensed matter, Theoretical condensed matter physics\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Condensed Matter'\n",
    "ID_fusion_res = get_fusion_res5(\"results/res__Condensed_matter\", topic, synset)\n",
    "eval_all(ID_fusion_res, topic, synset, \"results/res__Condensed_matter\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 810\n",
      "initial_corpus size: 810\n",
      "babelnet results size of the topic \"Emergency medicine\": 1379\n",
      "ground truth size 285\n",
      "intersection with the ground truth: 14\n",
      "F1 0.0168269230769\n",
      "babel precision:  0.010152284264\n",
      "babel recall:  0.0491228070175\n",
      "initial_corpus size: 810\n",
      "length of s3h results:  2793 length of test set 285\n",
      "F1:  0.00779727095517\n",
      "precision s3h:  0.00429645542427 recall s3h:  0.0421052631579\n",
      "initial_corpus size: 810\n",
      "length of results:  1379 length of test set 285\n",
      "F1:  0.0144230769231\n",
      "precision fusion:  0.00870195794054 recall fusion:  0.0421052631579\n",
      "for length of results:  2758\n",
      "F1:  0.0144594150509\n",
      "precision fusion:  0.00797679477883 recall fusion:  0.0771929824561\n",
      " for length of results:  4137\n",
      "F1:  0.0117593848937\n",
      "precision fusion:  0.0062847474015 recall fusion:  0.0912280701754\n",
      " for length of results:  5516\n",
      "F1:  0.00999827615928\n",
      "precision fusion:  0.00525743292241 recall fusion:  0.101754385965\n",
      " for length of results:  6895\n",
      "F1:  0.00974930362117\n",
      "precision fusion:  0.00507614213198 recall fusion:  0.122807017544\n"
     ]
    }
   ],
   "source": [
    "bab = \"emergency medicine, Emergency care, Emergency med, Emergency treatment, Emergent condition, Emergentology, Er physician\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Emergency medicine'\n",
    "res_pickle = \"results/Emergence_results.pickle\"\n",
    "Em_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Em_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "initial_corpus size: 344\n",
      "babelnet results size of the topic \"Biodiversity Conservation\": 7481\n",
      "ground truth size 55\n",
      "intersection with the ground truth: 12\n",
      "F1 0.0031847133758\n",
      "babel precision:  0.00160406362786\n",
      "babel recall:  0.218181818182\n",
      "initial_corpus size: 344\n",
      "length of s3h results:  3207 length of test set 55\n",
      "F1:  0.00613120784795\n",
      "precision s3h:  0.00311817898347 recall s3h:  0.181818181818\n",
      "initial_corpus size: 344\n",
      "length of results:  7481 length of test set 55\n",
      "F1:  0.00530785562633\n",
      "precision fusion:  0.00267343937976 recall fusion:  0.363636363636\n",
      "for length of results:  14962\n",
      "F1:  0.0031963774389\n",
      "precision fusion:  0.00160406362786 recall fusion:  0.436363636364\n",
      " for length of results:  22443\n",
      "F1:  0.00231131656147\n",
      "precision fusion:  0.0011584903979 recall fusion:  0.472727272727\n",
      " for length of results:  29924\n",
      "F1:  0.00186797424864\n",
      "precision fusion:  0.000935703782917 recall fusion:  0.509090909091\n",
      " for length of results:  37405\n",
      "F1:  0.00154831820609\n",
      "precision fusion:  0.000775297420131 recall fusion:  0.527272727273\n"
     ]
    }
   ],
   "source": [
    "bab = \"conservation biology, Animal conservation, Biological Conservation, Conservation, Biodiversity conservation, Conservation biologist, Conservation biologists, Conservation of natural resources, Conservation of wildlife, Conservation priority, Conservation science, Earth biologist, Earth biology, Ecological conservation, History of conservation biology, Wildlife Conservation\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Biodiversity Conservation'\n",
    "res_pickle = \"results/Biodiversity_Conservation_results.pickle\"\n",
    "BC_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(BC_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8999\n",
      "initial_corpus size: 8999\n",
      "babelnet results size of the topic \"Transplantation\": 8975\n",
      "ground truth size 4997\n",
      "intersection with the ground truth: 910\n",
      "F1 0.130260521042\n",
      "babel precision:  0.10139275766\n",
      "babel recall:  0.182109265559\n",
      "initial_corpus size: 8999\n",
      "length of s3h results:  8281 length of test set 4997\n",
      "F1:  0.105889441181\n",
      "precision s3h:  0.0848931288492 recall s3h:  0.140684410646\n",
      "initial_corpus size: 8999\n",
      "length of results:  8975 length of test set 4997\n",
      "F1:  0.130546807902\n",
      "precision fusion:  0.101615598886 recall fusion:  0.182509505703\n",
      "for length of results:  17950\n",
      "F1:  0.156708937988\n",
      "precision fusion:  0.100167130919 recall fusion:  0.359815889534\n",
      " for length of results:  26925\n",
      "F1:  0.143098803333\n",
      "precision fusion:  0.0848282265552 recall fusion:  0.457074244547\n",
      " for length of results:  35900\n",
      "F1:  0.128224564149\n",
      "precision fusion:  0.0730362116992 recall fusion:  0.524714828897\n",
      " for length of results:  44875\n",
      "F1:  0.114252486365\n",
      "precision fusion:  0.0634874651811 recall fusion:  0.570142085251\n"
     ]
    }
   ],
   "source": [
    "bab = \"organ transplant, transplant, transplantation, Organ transplantation, Medical Transplantation, Transplant Surgery, Black market organs, First transplant, First transplantation, Intestinal transplant, Live organ transplants, Mixed chimerism, Organ and Tissue Donor, Organ doner, Organ farming, Organ transplantation in different countries, Organ transplantation therapy, Organ transplants, Skin transplant, Tissue transplant, Transplant Tourism and Organ Trafficking, Transplantation medicine, Transplantation surgery, Transplantation therapy, Transplanted organs, Transplantology\"\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Transplantation'\n",
    "res_pickle = \"results/res_Transplantation\"\n",
    "Tr_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Tr_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8455\n",
      "initial_corpus size: 8455\n",
      "babelnet results size of the topic \"Surgery\": 8271\n",
      "ground truth size 6412\n",
      "intersection with the ground truth: 565\n",
      "F1 0.07695974937\n",
      "babel precision:  0.0683109660259\n",
      "babel recall:  0.0881160324392\n",
      "initial_corpus size: 8455\n",
      "length of s3h results:  13185 length of test set 6412\n",
      "F1:  0.0702148288003\n",
      "precision s3h:  0.0521805081532 recall s3h:  0.107298814722\n",
      "initial_corpus size: 8455\n",
      "length of results:  8271 length of test set 6412\n",
      "F1:  0.0753252060206\n",
      "precision fusion:  0.0668601136501 recall fusion:  0.0862445414847\n",
      "for length of results:  16542\n",
      "F1:  0.0864337370393\n",
      "precision fusion:  0.0599685648652 recall fusion:  0.154709918902\n",
      " for length of results:  24813\n",
      "F1:  0.0912089671737\n",
      "precision fusion:  0.0573892717527 recall fusion:  0.222083593263\n",
      " for length of results:  33084\n",
      "F1:  0.0949969617176\n",
      "precision fusion:  0.0567041470197 recall fusion:  0.292576419214\n",
      " for length of results:  41355\n",
      "F1:  0.0963007934348\n",
      "precision fusion:  0.0556160077379 recall fusion:  0.358702432938\n"
     ]
    }
   ],
   "source": [
    "bab = 'surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "res_pickle = \"results/res__Surgery\"\n",
    "Sr_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Sr_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 4073\n",
      "initial_corpus size: 4073\n",
      "babelnet results size of the topic \"Religion\": 6956\n",
      "ground truth size 587\n",
      "intersection with the ground truth: 98\n",
      "F1 0.0259843563569\n",
      "babel precision:  0.0140885566417\n",
      "babel recall:  0.166950596252\n",
      "initial_corpus size: 4073\n",
      "length of s3h results:  17721 length of test set 587\n",
      "F1:  0.011579637317\n",
      "precision s3h:  0.00598160374697 recall s3h:  0.180579216354\n",
      "initial_corpus size: 4073\n",
      "length of results:  6956 length of test set 587\n",
      "F1:  0.0201511335013\n",
      "precision fusion:  0.0109258194365 recall fusion:  0.129471890971\n",
      "for length of results:  13912\n",
      "F1:  0.016139044072\n",
      "precision fusion:  0.00841000575043 recall fusion:  0.199318568995\n",
      " for length of results:  20868\n",
      "F1:  0.0132370076905\n",
      "precision fusion:  0.00680467701744 recall fusion:  0.241908006814\n",
      " for length of results:  27824\n",
      "F1:  0.0114744289184\n",
      "precision fusion:  0.00585825186889 recall fusion:  0.277683134583\n",
      " for length of results:  34780\n",
      "F1:  0.0107444793169\n",
      "precision fusion:  0.00546290971823 recall fusion:  0.323679727428\n"
     ]
    }
   ],
   "source": [
    "bab = \"faith, religion, religious belief, belief, creed, Faiths, Religions, Religious beliefs, Allegory of faith, Co-religionism, Co-religionist, Co-religionists, Coreligionism, Coreligionist, Coreligionists, Creating Stories, Dereligionization, Faithful, Faithfully, Fictitous, Magical thinking/Revised, Magickal thinking, Relegious, Relgion, Relig, Relig., Religionistic, Religionistical, Religionistically, Religionists, Religious concepts, Religious faith, Religious issues, Religious tradition, Religious traditions, Religiousity, Religon, Relligion, Totalitarian religious group\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Religion'\n",
    "res_pickle = \"results/res__Religion\"\n",
    "Re_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Re_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8525\n",
      "initial_corpus size: 8525\n",
      "babelnet results size of the topic \"Physiology\": 8494\n",
      "ground truth size 2761\n",
      "intersection with the ground truth: 123\n",
      "F1 0.0218569524656\n",
      "babel precision:  0.0144808099835\n",
      "babel recall:  0.0445490764216\n",
      "initial_corpus size: 8525\n",
      "length of s3h results:  2208 length of test set 2761\n",
      "F1:  0.00120748641578\n",
      "precision s3h:  0.00135869565217 recall s3h:  0.00108656283955\n",
      "initial_corpus size: 8525\n",
      "length of results:  8494 length of test set 2761\n",
      "F1:  0.00941803642825\n",
      "precision fusion:  0.00623969861078 recall fusion:  0.0191959434987\n",
      "for length of results:  16988\n",
      "F1:  0.00951946934022\n",
      "precision fusion:  0.00553331763598 recall fusion:  0.0340456356393\n",
      " for length of results:  25482\n",
      "F1:  0.00977233296746\n",
      "precision fusion:  0.00541558747351 recall fusion:  0.0499818906193\n",
      " for length of results:  33976\n",
      "F1:  0.00881944633476\n",
      "precision fusion:  0.00476807157994 recall fusion:  0.0586743933357\n",
      " for length of results:  42470\n",
      "F1:  0.0083128827574\n",
      "precision fusion:  0.00442665410878 recall fusion:  0.0680912712785\n"
     ]
    }
   ],
   "source": [
    "bab = \" physiology, animal physiology, History of physiology, Institutes of Medicine, Phisiology, Physiologic, Physiological, Physiologists\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Physiology'\n",
    "res_pickle = \"results/res__Physiology\"\n",
    "Phys_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Phys_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8384\n",
      "initial_corpus size: 8384\n",
      "babelnet results size of the topic \"Pathology\": 8544\n",
      "ground truth size 2726\n",
      "intersection with the ground truth: 174\n",
      "F1 0.0308784383319\n",
      "babel precision:  0.0203651685393\n",
      "babel recall:  0.063829787234\n",
      "initial_corpus size: 8384\n",
      "length of s3h results:  7065 length of test set 2726\n",
      "F1:  0.0157287304668\n",
      "precision s3h:  0.0108987968861 recall s3h:  0.0282465150404\n",
      "initial_corpus size: 8384\n",
      "length of results:  8544 length of test set 2726\n",
      "F1:  0.0204081632653\n",
      "precision fusion:  0.0134597378277 recall fusion:  0.0421863536317\n",
      "for length of results:  17088\n",
      "F1:  0.0212980720703\n",
      "precision fusion:  0.0123478464419 recall fusion:  0.0774027879677\n",
      " for length of results:  25632\n",
      "F1:  0.0214401579801\n",
      "precision fusion:  0.0118601747815 recall fusion:  0.111518708731\n",
      " for length of results:  34176\n",
      "F1:  0.020703484906\n",
      "precision fusion:  0.0111774344569 recall fusion:  0.140132061629\n",
      " for length of results:  42720\n",
      "F1:  0.0203758306562\n",
      "precision fusion:  0.0108380149813 recall fusion:  0.1698459281\n"
     ]
    }
   ],
   "source": [
    "bab = \" pathology, Pathology as a medical specialty, General pathology, Autopsy Surgeon, Pathoanatomy, Pathobiology, Pathologic processes, Pathological, Pathological case, Pathologically, Pathologies, Pathologism, Pathologisms, Pathology as a science, Study of disease\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Pathology'\n",
    "res_pickle = \"results/res__Pathology\"\n",
    "Path_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Path_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 179\n",
      "initial_corpus size: 179\n",
      "babelnet results size of the topic \"Mycology\": 542\n",
      "ground truth size 530\n",
      "intersection with the ground truth: 3\n",
      "F1 0.00559701492537\n",
      "babel precision:  0.00553505535055\n",
      "babel recall:  0.00566037735849\n",
      "initial_corpus size: 179\n",
      "length of s3h results:  4269 length of test set 530\n",
      "F1:  0.0041675349031\n",
      "precision s3h:  0.00234246896229 recall s3h:  0.0188679245283\n",
      "initial_corpus size: 179\n",
      "length of results:  542 length of test set 530\n",
      "F1:  0.00373134328358\n",
      "precision fusion:  0.00369003690037 recall fusion:  0.00377358490566\n",
      "for length of results:  1084\n",
      "F1:  0.00371747211896\n",
      "precision fusion:  0.00276752767528 recall fusion:  0.00566037735849\n",
      " for length of results:  1626\n",
      "F1:  0.00556586270872\n",
      "precision fusion:  0.00369003690037 recall fusion:  0.011320754717\n",
      " for length of results:  2168\n",
      "F1:  0.00444773906597\n",
      "precision fusion:  0.00276752767528 recall fusion:  0.011320754717\n",
      " for length of results:  2710\n",
      "F1:  0.00432098765432\n",
      "precision fusion:  0.00258302583026 recall fusion:  0.0132075471698\n"
     ]
    }
   ],
   "source": [
    "bab = \"mycology, fungology, History of mycology, Micology, Mycological, Mycologists, Study of fungi\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Mycology'\n",
    "res_pickle = \"results/res__Mycology\"\n",
    "Myc_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Myc_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 1514\n",
      "initial_corpus size: 1514\n",
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "F1 0.0299498377102\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "initial_corpus size: 1514\n",
      "length of s3h results:  4262 length of test set 4769\n",
      "F1:  0.0301184807884\n",
      "precision s3h:  0.0319099014547 recall s3h:  0.0285175089117\n",
      "initial_corpus size: 1514\n",
      "length of results:  8787 length of test set 4769\n",
      "F1:  0.0346709943936\n",
      "precision fusion:  0.0267440537157 recall fusion:  0.0492765778989\n",
      "for length of results:  17574\n",
      "F1:  0.0360739381462\n",
      "precision fusion:  0.0229316035052 recall fusion:  0.0845040889075\n",
      " for length of results:  26361\n",
      "F1:  0.0352714423386\n",
      "precision fusion:  0.0208262205531 recall fusion:  0.115118473475\n",
      " for length of results:  35148\n",
      "F1:  0.0333191372097\n",
      "precision fusion:  0.0189199954478 recall fusion:  0.139442231076\n",
      " for length of results:  43935\n",
      "F1:  0.031208935611\n",
      "precision fusion:  0.0172982815523 recall fusion:  0.159362549801\n"
     ]
    }
   ],
   "source": [
    "bab = \" immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunology\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "res_pickle = \"results/res__Immunology\"\n",
    "Imm_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Imm_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2442\n",
      "initial_corpus size: 2442\n",
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "F1 0.0394088669951\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n",
      "initial_corpus size: 2442\n",
      "length of s3h results:  6157 length of test set 1020\n",
      "F1:  0.0359481677581\n",
      "precision s3h:  0.0209517622219 recall s3h:  0.126470588235\n",
      "initial_corpus size: 2442\n",
      "length of results:  3649 length of test set 1020\n",
      "F1:  0.0334118654958\n",
      "precision fusion:  0.0213757193752 recall fusion:  0.0764705882353\n",
      "for length of results:  7298\n",
      "F1:  0.040153883145\n",
      "precision fusion:  0.0228829816388 recall fusion:  0.163725490196\n",
      " for length of results:  10947\n",
      "F1:  0.0402774295981\n",
      "precision fusion:  0.0220151639719 recall fusion:  0.236274509804\n",
      " for length of results:  14596\n",
      "F1:  0.038550204918\n",
      "precision fusion:  0.0206220882434 recall fusion:  0.295098039216\n",
      " for length of results:  18245\n",
      "F1:  0.0366467687516\n",
      "precision fusion:  0.0193477665114 recall fusion:  0.346078431373\n"
     ]
    }
   ],
   "source": [
    "bab = \"Biomaterial, Biomaterials Engineering, Bio material, Biomaterials\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "res_pickle = \"results/res__Biomaterials\"\n",
    "bioma_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(bioma_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8160\n",
      "initial_corpus size: 8160\n",
      "babelnet results size of the topic \"Nursing\": 8252\n",
      "ground truth size 3282\n",
      "intersection with the ground truth: 276\n",
      "F1 0.0478585052887\n",
      "babel precision:  0.0334464372273\n",
      "babel recall:  0.0840950639854\n",
      "initial_corpus size: 8160\n",
      "length of s3h results:  12509 length of test set 3282\n",
      "F1:  0.117661959344\n",
      "precision s3h:  0.0742665280998 recall s3h:  0.283059110299\n",
      "initial_corpus size: 8160\n",
      "length of results:  8252 length of test set 3282\n",
      "F1:  0.0508063117739\n",
      "precision fusion:  0.0355065438682 recall fusion:  0.0892748324193\n",
      "for length of results:  16504\n",
      "F1:  0.124229253007\n",
      "precision fusion:  0.0744667959283 recall fusion:  0.374466788544\n",
      " for length of results:  24756\n",
      "F1:  0.115557457736\n",
      "precision fusion:  0.0654386815317 recall fusion:  0.493601462523\n",
      " for length of results:  33008\n",
      "F1:  0.103224028658\n",
      "precision fusion:  0.0567438196801 recall fusion:  0.570688604509\n",
      " for length of results:  41260\n",
      "F1:  0.0924969691527\n",
      "precision fusion:  0.0499272903539 recall fusion:  0.627666057282\n"
     ]
    }
   ],
   "source": [
    "bab = \"nursing, Nursing Science, Staff nurse, Adult nursing, Flightnurse, nurse, Nursing History, Nursing Officer, Nursing practice, Nursing skills, Nursing staff, Nursing Student, Nursing unit, Nurxing, Practice of nursing\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Nursing'\n",
    "res_pickle = \"results/Nursing_results.pickle\"\n",
    "Nurs_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Nurs_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, pickle, argparse, json, os, urllib2\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "########################################################################################\n",
    "def query_from(q, f):\n",
    "    q = q+'&from='+str(f)\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    subject_ids = np.array(range(len(data['hits'])), dtype=np.object)\n",
    "    for (i, hit) in enumerate(data['hits']):\n",
    "        subject_ids[i] = hit['id']\n",
    "    return subject_ids\n",
    "\n",
    "def query(q):\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    nb_requests = 1 + data['total'] / 1000\n",
    "    if nb_requests > 10: # maximum number of pages due to API pagination restrection\n",
    "        nb_requests = 10\n",
    "    subject_ids = query_from(q, 0)\n",
    "    for i in range(nb_requests)[1:]:\n",
    "        f = i * 1000\n",
    "        next_request = query_from(q, f)\n",
    "        subject_ids = np.hstack((subject_ids, next_request))\n",
    "    return subject_ids.tolist()\n",
    "\n",
    "def find_intersection(list_a, list_b):\n",
    "    return list(set(list_a) & set(list_b))\n",
    "\n",
    "def term2url(string):\n",
    "    string = string.split(' ')\n",
    "    res = '%22'\n",
    "    for s in string:\n",
    "        res = res + s + '%20'\n",
    "    res = res[:-3]\n",
    "    res = res + '%22'\n",
    "    return res\n",
    "\n",
    "def babel_synset(synset):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    for syn in synset:\n",
    "        syn = term2url(syn)\n",
    "        q = q + 'title:' + syn + '%20OR%20abstract:' + syn + '%20OR%20'\n",
    "    q = q[:-8]\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    "\n",
    "def babel_subj_keyword(topic):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    topic = term2url(topic)\n",
    "    q = q+ 'subject.value:' + topic + '%20OR%20keywords:' + topic\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    " \n",
    "def babel_title_abst(topic):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    topic = term2url(topic)\n",
    "    q = q+ 'title:' + topic + '%20OR%20abstract:' + topic\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    " \n",
    "def babelnet_syn_get_input(topic, synset):\n",
    "    results = query(babel_synset(synset))\n",
    "    _gs = query(babel_subj_keyword(topic))\n",
    "    results = find_intersection(results, inversed_index.keys())\n",
    "    _abst_title = query(babel_title_abst(topic))\n",
    "    test_set = _inter = {x for x in _gs if x not in _abst_title}\n",
    "    test_set = find_intersection(test_set, inversed_index.keys())\n",
    "    results = list(results)\n",
    "    test = list(test_set)\n",
    "    print 'initial_corpus size:', len(find_intersection(_abst_title, inversed_index.keys()))\n",
    "    return results, test\n",
    "\n",
    "def babelnet_eval(topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    print 'results size of the topic \"' + topic + '\":', len(babelnet_results) \n",
    "    print 'ground truth size', len(test)\n",
    "    babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "    babel_test_intersection_size = len(babel_test_intersection)\n",
    "    print 'intersection with the ground truth:', babel_test_intersection_size\n",
    "    \n",
    "def top_thresh(ordered_dict_pickle, thresh):\n",
    "    ranked_all = pickle.load(open(ordered_dict_pickle, 'rb'))\n",
    "    ranked_all_np = np.array(ranked_all.items())\n",
    "    ranked_all_df = pd.DataFrame(data=ranked_all_np, index=None, columns=['istex_id', 'score'])\n",
    "    ranked_all_df['score'] = ranked_all_df[['score']].astype(float)\n",
    "    return ranked_all_df[ranked_all_df['score'] > thresh]\n",
    "\n",
    "def top_thresh_lst(res_lst_pickle, thresh):\n",
    "    ranked_all = pickle.load(open(res_lst_pickle, 'rb'))\n",
    "    if type(ranked_all) is OrderedDict:\n",
    "        ranked_all = ranked_all.items()[:100000]\n",
    "    ranked_all_np = np.array(ranked_all)\n",
    "    ranked_all_df = pd.DataFrame(data=ranked_all_np, index=None, columns=['istex_id', 'score'])\n",
    "    ranked_all_df['score'] = ranked_all_df[['score']].astype(float)\n",
    "    return ranked_all_df[ranked_all_df['score'] > thresh]\n",
    "\n",
    "def babelnet_eval_PR(topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    print 'babelnet results size of the topic \"' + topic + '\":', len(babelnet_results) \n",
    "    print 'ground truth size', len(test)\n",
    "    babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "    babel_test_intersection_size = len(babel_test_intersection)\n",
    "    print 'intersection with the ground truth:', babel_test_intersection_size\n",
    "    precision = babel_test_intersection_size / float(len(babelnet_results))\n",
    "    recall = babel_test_intersection_size / float(len(test))\n",
    "    if babel_test_intersection_size is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0.0 \n",
    "    print \"F1\", F1\n",
    "    print 'babel precision: ', precision\n",
    "    print 'babel recall: ', recall\n",
    "    \n",
    "\n",
    "#Evaluate 3SH results at treshold\n",
    "def eval_all_at_thresh(ordered_dict_pickle, topic, synset, thresh):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    if n > 10000:\n",
    "        thresh = thresh + 0.1\n",
    "        top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    elif n < 1000:\n",
    "        thresh = thresh - 0.1\n",
    "        top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t, thresh\n",
    "\n",
    "#Evaluate 3SH results list at treshold\n",
    "def eval_all_at_thresh_lst(res_pickle, topic, synset, thresh=0.75):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    if n > 10000:\n",
    "        thresh = thresh + 0.1\n",
    "        top_res = top_thresh_lst(res_pickle, thresh)\n",
    "        if len(top_res) > 10000:\n",
    "            thresh = thresh + 0.05\n",
    "            top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    elif n < 1000:\n",
    "        thresh = thresh - 0.1\n",
    "        top_res = top_thresh_lst(res_pickle, thresh)\n",
    "        if len(top_res) > 1000:\n",
    "            thresh = thresh - 0.05\n",
    "            top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"length of s3h results: \", n, \"length of test set\", t\n",
    "    print \"F1: \", F1\n",
    "    print \"precision s3h: \", precision, \"recall s3h: \", recall\n",
    "\n",
    "def get_fusion_res3(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res4(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * 2 * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res5(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(range(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res6(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(range(100000))\n",
    "    fus = fus * 3 * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (2*i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "#Evaluate fusion df\n",
    "def eval_all(fusion_df, topic, synset, res_pickle, thresh=0.75):\n",
    "    eval_all_at_thresh_lst(res_pickle, topic, synset, thresh)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    t = len(test)\n",
    "\n",
    "    print \"length of results: \", n, \"length of test set\", t\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 2 * len(babelnet_res)\n",
    "    print \"for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 3 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "    \n",
    "    n = 4 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "    \n",
    "    n = 5 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1  \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original inversed_index\n",
      "[(u'ISTEX_D89FA3AC3521074D46F4245762153DF497BFFA1F', 2002320), (u'ISTEX_18EAF4D6A126B077EB38667801D1B7292F32FF49', 2483732), (u'ISTEX_5F91044435FCC4FABB9F02E31467DCFE75F4A7BE', 1429049)]\n",
      "processed inversed_index\n",
      "[(u'FCF1393F9B8136AC08FB67E88F94F3CF62C17288', 3517138), (u'482E1102A1114327A744FD2ADB4D9F8FF7E9A70B', 751643), (u'A81022B6295AE66F68A10222C3B94A06B033C1BA', 3983232)]\n"
     ]
    }
   ],
   "source": [
    "#loading SDV of istex articles\n",
    "inv_index = json.load(open('../RecSys_Exp_files/182_381_vec150_results/output_paragraph_inversed_index.json','rb'))\n",
    "print 'original inversed_index'\n",
    "print inv_index.items()[:3]\n",
    "inversed_index = dict()\n",
    "for (k, v) in inv_index.items():\n",
    "    key = k.split('_')[1]\n",
    "    inversed_index[key] = v\n",
    "print 'processed inversed_index'\n",
    "print inversed_index.items()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = (fus + len(babel_results)) / 2\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res3(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'organ transplant, transplant, transplantation, Organ transplantation, Medical Transplantation, Transplant Surgery, Black market organs, First transplant, First transplantation, Intestinal transplant, Live organ transplants, Mixed chimerism, Organ and Tissue Donor, Organ doner, Organ farming, Organ transplantation in different countries, Organ transplantation therapy, Organ transplants, Skin transplant, Tissue transplant, Transplant Tourism and Organ Trafficking, Transplantation medicine, Transplantation surgery, Transplantation therapy, Transplanted organs, Transplantology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Transplantation'\n",
    "s3h_res_pickle = \"results/res_Transplantation\"\n",
    "Transplantation_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'spectroscopy, spectrographic analysis, spectrometry, spectroscopic analysis, spectrum analysis, Fingerprint region, Laser spectroscopy, Optical spectroscopy, Atomic line, Atomic line spectra, Atomic spectra, Atomic spectral line, Electromagnetic spectroscopy, Emission spectrochemical analysis, Spectrochemical Analysis, Spectrography, Spectrology, Spectroscopic, Spectroscopist, Spectroscopists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Spectroscopy'\n",
    "s3h_res_pickle = \"results/res__Spectroscopy\"\n",
    "Spectroscopy_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = ' surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "s3h_res_pickle = \"results/res__Surgery\"\n",
    "Surgery_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'optics, Classical optics, Light physics, Optical, Optical system'\n",
    "synset = bab.split(',')\n",
    "topic = 'Optics'\n",
    "s3h_res_pickle = \"results/res__Optics\"\n",
    "Optics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "s3h_res_pickle = \"results/res__Literature\"\n",
    "Literature_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "s3h_res_pickle = \"results/res__Toxicology\"\n",
    "Toxicology_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b4eb30782151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'artificial intelligence'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms3h_res_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results/AI_results.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mAI_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3h_res_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-2f95e6519d8b>\u001b[0m in \u001b[0;36mget_fusion_res\u001b[0;34m(s3h_res_pickle, topic, synset)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3h\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_s3h_top100k_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms3h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mfus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "s3h_res_pickle = \"results/AI_results.pickle\"\n",
    "AI_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "s3h_res_pickle = \"results/res__Cybernetics\"\n",
    "Cybernetics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'information system, data system, information systems, system info, Business computing, Computer information system, Computer information systems, Elements of Information System, Information in Computer Science, Information systems and technology, Information systems discipline, Information systems theory, Informationssystem'\n",
    "synset = bab.split(',')\n",
    "topic = 'Information Systems'\n",
    "s3h_res_pickle = \"results/infosys_results.pickle\"\n",
    "Information_Systems_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "s3h_res_pickle = \"results/res__Immunology\"\n",
    "Immunology_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'infection, infectious disease, communicable diseases, contagion, Infectious diseases, Acute infection, AIDS-related bacterial infections, AIDS-related viral infections, Anti-infective, Anti-infectives, Antiinfective, Bacterial Infections, Communicable disease, Contagious diseases, Definition to contagious, Infect, Infecting, Infectiology, Infections, Infectious, Infectious disease epidemiology, Infectious disease medicine, Infectology, Local infection, Primary infection, Rochalimea infections, Secondary infection, Tropical bacterial infections, Tropical infections, Viral Infections, Wound colonization, Wound infection'\n",
    "synset = bab.split(',')\n",
    "topic = 'Infectious Diseases'\n",
    "s3h_res_pickle = \"results/Infectious_results.pickle\"\n",
    "Infectious_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "s3h_res_pickle = \"results/res__Biomaterials\"\n",
    "Biomaterials_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'ceramics, Ceramic art, Art pottery, Art ware, Ceramic artist, Ceramic paint, Ceramics art, Fine art pot, Vase painting'\n",
    "synset = bab.split(',')\n",
    "topic = 'Ceramics'\n",
    "s3h_res_pickle = \"results/res__Ceramics\"\n",
    "Ceramics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic = \"biophysics\"\n",
    "synset_text = \"biophysics, Biological physics, Biophysical, Biophysicists, History of biophysics\"\n",
    "synset = synset_text.split(',')\n",
    "s3h_res_pickle = 'results/res__biophysics'\n",
    "biophysics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_dict = OrderedDict((k,i) for i,k in enumerate(biophysics_fusion_res))\n",
    "babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "inter = set(ind_dict).intersection(babel_results)\n",
    "indices = [ind_dict[x] for x in inter ]\n",
    "cutt = np.array(indices).max()\n",
    "cutt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slic = len(indices) * 0.8\n",
    "indices_slic = indices[:int(slic)]\n",
    "#indices_slic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fusion_res = fusion_df.sort_values(\"fusin_rank\")[\"istex_id\"].tolist()\n",
    "#fusion_res[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df with manual cut\n",
    "def eval_fusin_at_thresh_lst_1k(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    if n < 1000:\n",
    "        n = 1000\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_5k(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = 5000#len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_2_bablesize((fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = 2 * len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_bablesize(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res_2(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_top100k_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = (fus + 100000) / 2\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_all(fusion_df, topic, synset, res_pickle, tresh=0.75):\n",
    "    eval_all_at_thresh_lst(res_pickle, topic, synset, thresh)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    t = len(test)\n",
    "\n",
    "    print \"length of results: \", n, \"length of test set\", t\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 2 * len(babelnet_res)\n",
    "    print \"for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 3 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004853496314937983, 0.017396907216494846, 5563, 1552)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "get_fusion_res_2(\"results/res__Toxicology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004853496314937983, 0.017396907216494846, 5563, 1552)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "get_fusion_res_2(\"results/res__Toxicology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Toxicology_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00356261989586188, 0.012745098039215686, 3649, 1020)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "get_fusion_res_2(\"results/res__Biomaterials\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Biomaterials_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n",
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 7298, 1020)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "get_fusion_res_2(\"results/res__Biomaterials\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Biomaterials_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.010356208034596564, 0.03816313692598029, 17574, 4769)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "get_fusion_res_2(\"results/res__Immunology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Immunology_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "0.0124 0.0130006290627 5000 4769\n"
     ]
    }
   ],
   "source": [
    "eval_fusin_at_thresh_lst(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "0.0119494708091 0.0220171943804 8787 4769\n"
     ]
    }
   ],
   "source": [
    "eval_fusin_at_thresh_lst3(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "get_fusion_res_2(\"results/res__Immunology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Cybernetics\": 511\n",
      "ground truth size 596\n",
      "intersection with the ground truth: 47\n",
      "babel precision:  0.0919765166341\n",
      "babel recall:  0.0788590604027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004, 0.006711409395973154, 1000, 596)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "get_fusion_res_2(\"\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Cybernetics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Cybernetics\": 511\n",
      "ground truth size 596\n",
      "intersection with the ground truth: 47\n",
      "babel precision:  0.0919765166341\n",
      "babel recall:  0.0788590604027\n",
      "0.0283757338552 0.0486577181208 1022 596\n"
     ]
    }
   ],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "Cybernetics_fusion_res = get_fusion_res_2(\"results/res__Cybernetics\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Cybernetics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_all_at_thresh('results/AI_results.pickle', topic , synset, thresh = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Literature\": 7357\n",
      "ground truth size 860\n",
      "intersection with the ground truth: 107\n",
      "babel precision:  0.0145439717276\n",
      "babel recall:  0.124418604651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0020388745412532284, 0.01744186046511628, 7357, 860)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Literature_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Optics\": 8730\n",
      "ground truth size 3286\n",
      "intersection with the ground truth: 271\n",
      "babel precision:  0.0310423825888\n",
      "babel recall:  0.0824710894705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.015005727376861398, 0.039866098600121726, 8730, 3286)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'optics, Classical optics, Light physics, Optical, Optical system'\n",
    "synset = bab.split(',')\n",
    "topic = 'Optics'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Optics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Surgery\": 8271\n",
      "ground truth size 6412\n",
      "intersection with the ground truth: 565\n",
      "babel precision:  0.0683109660259\n",
      "babel recall:  0.0881160324392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.027687099504292104, 0.03571428571428571, 8271, 6412)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = ' surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Surgery_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Spectroscopy\": 8513\n",
      "ground truth size 7294\n",
      "intersection with the ground truth: 287\n",
      "babel precision:  0.0337131446024\n",
      "babel recall:  0.0393474088292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.025138024198284977, 0.029339182890046615, 8513, 7294)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'spectroscopy, spectrographic analysis, spectrometry, spectroscopic analysis, spectrum analysis, Fingerprint region, Laser spectroscopy, Optical spectroscopy, Atomic line, Atomic line spectra, Atomic spectra, Atomic spectral line, Electromagnetic spectroscopy, Emission spectrochemical analysis, Spectrochemical Analysis, Spectrography, Spectrology, Spectroscopic, Spectroscopist, Spectroscopists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Spectroscopy'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Spectroscopy_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "AI_fusion_res = get_fusion_res3(\"results/AI_results.pickle\", topic, synset)\n",
    "eval_all(AI_fusion_res, topic, synset, \"results/AI_results.pickle\", tresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2588\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -3] Temporary failure in name resolution>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9ca65e0e4683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Infectious Diseases'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mID_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/Infectious_results.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID_fusion_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"results/Infectious_results.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-4cb70aaaf842>\u001b[0m in \u001b[0;36meval_all\u001b[0;34m(fusion_df, topic, synset, res_pickle, thresh)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;31m#Evaluate fusion df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0meval_all_at_thresh_lst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mbabelnet_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabelnet_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4cb70aaaf842>\u001b[0m in \u001b[0;36meval_all_at_thresh_lst\u001b[0;34m(res_pickle, topic, synset, thresh)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m#Evaluate 3SH results list at treshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_all_at_thresh_lst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mbabelnet_eval_PR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4cb70aaaf842>\u001b[0m in \u001b[0;36mbabelnet_eval_PR\u001b[0;34m(topic, synset)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbabelnet_eval_PR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mbabelnet_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'babelnet results size of the topic \"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\":'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabelnet_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'ground truth size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4cb70aaaf842>\u001b[0m in \u001b[0;36mbabelnet_syn_get_input\u001b[0;34m(topic, synset)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0m_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_subj_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_intersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minversed_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4cb70aaaf842>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnb_requests\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# maximum number of pages due to API pagination restrection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnb_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msubject_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_requests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4cb70aaaf842>\u001b[0m in \u001b[0;36mquery_from\u001b[0;34m(q, f)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'&from='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msubject_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 447\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             return self.do_open(httplib.HTTPSConnection, req,\n\u001b[0;32m-> 1241\u001b[0;31m                 context=self._context)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# XXX what error?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>"
     ]
    }
   ],
   "source": [
    "bab = 'infection, infectious disease, communicable diseases, contagion, Infectious diseases, Acute infection, AIDS-related bacterial infections, AIDS-related viral infections, Anti-infective, Anti-infectives, Antiinfective, Bacterial Infections, Communicable disease, Contagious diseases, Definition to contagious, Infect, Infecting, Infectiology, Infections, Infectious, Infectious disease epidemiology, Infectious disease medicine, Infectology, Local infection, Primary infection, Rochalimea infections, Secondary infection, Tropical bacterial infections, Tropical infections, Viral Infections, Wound colonization, Wound infection'\n",
    "synset = bab.split(',')\n",
    "topic = 'Infectious Diseases'\n",
    "ID_fusion_res = get_fusion_res3(\"results/Infectious_results.pickle\", topic, synset)\n",
    "eval_all(ID_fusion_res, topic, synset, \"results/Infectious_results.pickle\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 795\n",
      "initial_corpus size: 795\n",
      "babelnet results size of the topic \"Respiratory system\": 8968\n",
      "ground truth size 189\n",
      "intersection with the ground truth: 18\n",
      "F1 0.00393141858687\n",
      "babel precision:  0.00200713648528\n",
      "babel recall:  0.0952380952381\n",
      "initial_corpus size: 795\n",
      "length of s3h results:  1120 length of test set 189\n",
      "F1:  0.0061115355233\n",
      "precision s3h:  0.00357142857143 recall s3h:  0.021164021164\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -3] Temporary failure in name resolution>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-40efbe7f4734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Respiratory system'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mID_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/Respiratory_system_results.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID_fusion_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"results/Respiratory_system_results.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-75a801d7cb86>\u001b[0m in \u001b[0;36meval_all\u001b[0;34m(fusion_df, topic, synset, res_pickle, thresh)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0meval_all_at_thresh_lst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mbabelnet_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabelnet_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-75a801d7cb86>\u001b[0m in \u001b[0;36mbabelnet_syn_get_input\u001b[0;34m(topic, synset)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0m_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_subj_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_intersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minversed_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-75a801d7cb86>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnb_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 447\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             return self.do_open(httplib.HTTPSConnection, req,\n\u001b[0;32m-> 1241\u001b[0;31m                 context=self._context)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# XXX what error?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>"
     ]
    }
   ],
   "source": [
    "bab = \"respiratory system, systema respiratorium, Respiration organ, Breathing organ, Breathing system, Development of respiratory system, Human Respiration, Human respiratory system, Pulmonary respiration, Pulmonary system, Respatory system, Respiration of human, Respiration system, Respiratory, Respiratory organs, Respiratory system agents, Respiratory system disorders, Respiratory systems, Respitory System, Subglottic airway, The respiratory system\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Respiratory system'\n",
    "ID_fusion_res = get_fusion_res3(\"results/Respiratory_system_results.pickle\", topic, synset)\n",
    "eval_all(ID_fusion_res, topic, synset, \"results/Respiratory_system_results.pickle\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 795\n",
      "babelnet results size of the topic \"Respiratory system\": 8968\n",
      "ground truth size 189\n",
      "intersection with the ground truth: 18\n",
      "F1 0.00393141858687\n",
      "babel precision:  0.00200713648528\n",
      "babel recall:  0.0952380952381\n",
      "initial_corpus size: 795\n",
      "length of s3h results:  1120 length of test set 189\n",
      "F1:  0.0061115355233\n",
      "precision s3h:  0.00357142857143 recall s3h:  0.021164021164\n",
      "initial_corpus size: 795\n",
      "length of results:  8968 length of test set 189\n",
      "F1:  0.00174729714972\n",
      "precision fusion:  0.000892060660125 recall fusion:  0.042328042328\n",
      "for length of results:  17936\n",
      "F1:  0.00110344827586\n",
      "precision fusion:  0.000557537912578 recall fusion:  0.0529100529101\n",
      " for length of results:  26904\n",
      "F1:  0.000959657476101\n",
      "precision fusion:  0.000483199524234 recall fusion:  0.0687830687831\n",
      " for length of results:  35872\n",
      "F1:  0.000831923684867\n",
      "precision fusion:  0.000418153434434 recall fusion:  0.0793650793651\n",
      " for length of results:  44840\n",
      "F1:  0.000843900597393\n",
      "precision fusion:  0.000423728813559 recall fusion:  0.100529100529\n"
     ]
    }
   ],
   "source": [
    "eval_all(ID_fusion_res, topic, synset, \"results/Respiratory_system_results.pickle\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 724\n",
      "initial_corpus size: 724\n",
      "babelnet results size of the topic \"international relations\": 983\n",
      "ground truth size 60\n",
      "intersection with the ground truth: 5\n",
      "F1 0.00958772770853\n",
      "babel precision:  0.00508646998983\n",
      "babel recall:  0.0833333333333\n",
      "initial_corpus size: 724\n",
      "length of s3h results:  5867 length of test set 60\n",
      "F1:  0.00269951071368\n",
      "precision s3h:  0.0013635588887 recall s3h:  0.133333333333\n",
      "initial_corpus size: 724\n",
      "length of results:  983 length of test set 60\n",
      "F1:  0.00191754554171\n",
      "precision fusion:  0.00101729399797 recall fusion:  0.0166666666667\n",
      "for length of results:  1966\n",
      "F1:  0.000987166831194\n",
      "precision fusion:  0.000508646998983 recall fusion:  0.0166666666667\n",
      " for length of results:  2949\n",
      "F1:  0.000664672648721\n",
      "precision fusion:  0.000339097999322 recall fusion:  0.0166666666667\n",
      " for length of results:  3932\n",
      "F1:  0.000501002004008\n",
      "precision fusion:  0.000254323499491 recall fusion:  0.0166666666667\n",
      " for length of results:  4915\n",
      "F1:  0.000402010050251\n",
      "precision fusion:  0.000203458799593 recall fusion:  0.0166666666667\n"
     ]
    }
   ],
   "source": [
    "bab = \"international affairs, international relations, world affairs, International politics, Intrel, Diplomacy and Statecraft, Diplomatic relationship, Global relations, International relation, International Relation Studies, International trade relations, Interstate relations, Levels of Analysis in international relations, Study of International Relations\"\n",
    "synset = bab.split(',')\n",
    "topic = 'international relations'\n",
    "res_pickle = \"results/international_relations_results.pickle\"\n",
    "IR_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(IR_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2497\n",
      "initial_corpus size: 2497\n",
      "babelnet results size of the topic \"Neuroimaging\": 3679\n",
      "ground truth size 522\n",
      "intersection with the ground truth: 52\n",
      "F1 0.0247560104737\n",
      "babel precision:  0.0141342756184\n",
      "babel recall:  0.0996168582375\n",
      "initial_corpus size: 2497\n",
      "length of s3h results:  9294 length of test set 522\n",
      "F1:  0.0350448247759\n",
      "precision s3h:  0.0185065633742 recall s3h:  0.329501915709\n",
      "initial_corpus size: 2497\n",
      "length of results:  3679 length of test set 522\n",
      "F1:  0.00476077124494\n",
      "precision fusion:  0.00271812992661 recall fusion:  0.0191570881226\n",
      "for length of results:  7358\n",
      "F1:  0.00304568527919\n",
      "precision fusion:  0.00163087795597 recall fusion:  0.0229885057471\n",
      " for length of results:  11037\n",
      "F1:  0.00294143091963\n",
      "precision fusion:  0.00154027362508 recall fusion:  0.0325670498084\n",
      " for length of results:  14716\n",
      "F1:  0.00301876886731\n",
      "precision fusion:  0.0015629247078 recall fusion:  0.044061302682\n",
      " for length of results:  18395\n",
      "F1:  0.00296030025903\n",
      "precision fusion:  0.0015221527589 recall fusion:  0.0536398467433\n"
     ]
    }
   ],
   "source": [
    "bab = \"neuroimaging, Brain imaging, Brain scanning, Brain function map, Brain scan, Brain scans, Functional neurological mapping, Neuro-imaging, Neuroradiography\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Neuroimaging'\n",
    "res_pickle = \"results/Neuroimaging_results.pickle\"\n",
    "Ne_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(Ne_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2550\n",
      "initial_corpus size: 2550\n",
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "F1 0.0410400562193\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n",
      "initial_corpus size: 2550\n",
      "length of s3h results:  8547 length of test set 1552\n",
      "F1:  0.0243588474106\n",
      "precision s3h:  0.014391014391 recall s3h:  0.0792525773196\n",
      "initial_corpus size: 2550\n",
      "length of results:  5563 length of test set 1552\n",
      "F1:  0.010681658468\n",
      "precision fusion:  0.00683084666547 recall fusion:  0.0244845360825\n",
      "for length of results:  11126\n",
      "F1:  0.00914970815586\n",
      "precision fusion:  0.00521301456049 recall fusion:  0.0373711340206\n",
      " for length of results:  16689\n",
      "F1:  0.00964859382709\n",
      "precision fusion:  0.00527293426808 recall fusion:  0.0567010309278\n",
      " for length of results:  22252\n",
      "F1:  0.00957822214754\n",
      "precision fusion:  0.0051231349991 recall fusion:  0.0734536082474\n",
      " for length of results:  27815\n",
      "F1:  0.00912588960398\n",
      "precision fusion:  0.00481754449038 recall fusion:  0.0863402061856\n"
     ]
    }
   ],
   "source": [
    "bab = \"Toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "res_pickle = \"results/res__Toxicology\"\n",
    "To_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(To_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2550\n",
      "initial_corpus size: 2550\n",
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "F1 0.0410400562193\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n",
      "initial_corpus size: 2550\n",
      "length of s3h results:  8547 length of test set 1552\n",
      "F1:  0.0243588474106\n",
      "precision s3h:  0.014391014391 recall s3h:  0.0792525773196\n",
      "initial_corpus size: 2550\n",
      "length of results:  5563 length of test set 1552\n",
      "F1:  0.0137737174982\n",
      "precision fusion:  0.008808197016 recall fusion:  0.0315721649485\n",
      "for length of results:  11126\n",
      "F1:  0.0110427512226\n",
      "precision fusion:  0.00629156929714 recall fusion:  0.0451030927835\n",
      " for length of results:  16689\n",
      "F1:  0.0107450249438\n",
      "precision fusion:  0.005872131344 recall fusion:  0.0631443298969\n",
      " for length of results:  22252\n",
      "F1:  0.00999831961015\n",
      "precision fusion:  0.00534783390257 recall fusion:  0.076675257732\n",
      " for length of results:  27815\n",
      "F1:  0.00973882248783\n",
      "precision fusion:  0.00514111091138 recall fusion:  0.0921391752577\n"
     ]
    }
   ],
   "source": [
    "bab = \"Toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "res_pickle = \"results/res__Toxicology\"\n",
    "To_fusion_res = get_fusion_res4(res_pickle, topic, synset)\n",
    "eval_all(To_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2550\n",
      "initial_corpus size: 2550\n",
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "F1 0.0410400562193\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n",
      "initial_corpus size: 2550\n",
      "length of s3h results:  8547 length of test set 1552\n",
      "F1:  0.0243588474106\n",
      "precision s3h:  0.014391014391 recall s3h:  0.0792525773196\n",
      "initial_corpus size: 2550\n",
      "length of results:  5563 length of test set 1552\n",
      "F1:  0.0309205903022\n",
      "precision fusion:  0.0197735035053 recall fusion:  0.0708762886598\n",
      "for length of results:  11126\n",
      "F1:  0.0310774570121\n",
      "precision fusion:  0.0177062735934 recall fusion:  0.126932989691\n",
      " for length of results:  16689\n",
      "F1:  0.0288361383696\n",
      "precision fusion:  0.0157588830967 recall fusion:  0.169458762887\n",
      " for length of results:  22252\n",
      "F1:  0.0261300621744\n",
      "precision fusion:  0.0139762717958 recall fusion:  0.200386597938\n",
      " for length of results:  27815\n",
      "F1:  0.0237000715088\n",
      "precision fusion:  0.0125112349452 recall fusion:  0.224226804124\n"
     ]
    }
   ],
   "source": [
    "bab = \"Toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "res_pickle = \"results/res__Toxicology\"\n",
    "To_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(To_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2550\n",
      "initial_corpus size: 2550\n",
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "F1 0.0410400562193\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n",
      "initial_corpus size: 2550\n",
      "length of s3h results:  8547 length of test set 1552\n",
      "F1:  0.0243588474106\n",
      "precision s3h:  0.014391014391 recall s3h:  0.0792525773196\n",
      "initial_corpus size: 2550\n",
      "length of results:  5563 length of test set 1552\n",
      "F1:  0.0309205903022\n",
      "precision fusion:  0.0197735035053 recall fusion:  0.0708762886598\n",
      "for length of results:  11126\n",
      "F1:  0.0310774570121\n",
      "precision fusion:  0.0177062735934 recall fusion:  0.126932989691\n",
      " for length of results:  16689\n",
      "F1:  0.0288361383696\n",
      "precision fusion:  0.0157588830967 recall fusion:  0.169458762887\n",
      " for length of results:  22252\n",
      "F1:  0.0261300621744\n",
      "precision fusion:  0.0139762717958 recall fusion:  0.200386597938\n",
      " for length of results:  27815\n",
      "F1:  0.0237000715088\n",
      "precision fusion:  0.0125112349452 recall fusion:  0.224226804124\n"
     ]
    }
   ],
   "source": [
    "bab = \"Toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "res_pickle = \"results/res__Toxicology\"\n",
    "To_fusion_res = get_fusion_res6(res_pickle, topic, synset)\n",
    "eval_all(To_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 795\n",
      "initial_corpus size: 795\n",
      "babelnet results size of the topic \"Respiratory system\": 8968\n",
      "ground truth size 189\n",
      "intersection with the ground truth: 18\n",
      "F1 0.00393141858687\n",
      "babel precision:  0.00200713648528\n",
      "babel recall:  0.0952380952381\n",
      "initial_corpus size: 795\n",
      "length of s3h results:  1120 length of test set 189\n",
      "F1:  0.0061115355233\n",
      "precision s3h:  0.00357142857143 recall s3h:  0.021164021164\n",
      "initial_corpus size: 795\n",
      "length of results:  8968 length of test set 189\n",
      "F1:  0.00502347930545\n",
      "precision fusion:  0.00256467439786 recall fusion:  0.121693121693\n",
      "for length of results:  17936\n",
      "F1:  0.00342068965517\n",
      "precision fusion:  0.00172836752899 recall fusion:  0.164021164021\n",
      " for length of results:  26904\n",
      "F1:  0.00250987339903\n",
      "precision fusion:  0.00126375260184 recall fusion:  0.179894179894\n",
      " for length of results:  35872\n",
      "F1:  0.00216300158066\n",
      "precision fusion:  0.00108719892953 recall fusion:  0.206349206349\n",
      " for length of results:  44840\n",
      "F1:  0.00195429612028\n",
      "precision fusion:  0.000981266726137 recall fusion:  0.232804232804\n"
     ]
    }
   ],
   "source": [
    "bab = \"respiratory system, systema respiratorium, Respiration organ, Breathing organ, Breathing system, Development of respiratory system, Human Respiration, Human respiratory system, Pulmonary respiration, Pulmonary system, Respatory system, Respiration of human, Respiration system, Respiratory, Respiratory organs, Respiratory system agents, Respiratory system disorders, Respiratory systems, Respitory System, Subglottic airway, The respiratory system\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Respiratory system'\n",
    "ID_fusion_res = get_fusion_res5(\"results/Respiratory_system_results.pickle\", topic, synset)\n",
    "eval_all(ID_fusion_res, topic, synset, \"results/Respiratory_system_results.pickle\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 724\n",
      "initial_corpus size: 724\n",
      "babelnet results size of the topic \"international relations\": 983\n",
      "ground truth size 60\n",
      "intersection with the ground truth: 5\n",
      "F1 0.00958772770853\n",
      "babel precision:  0.00508646998983\n",
      "babel recall:  0.0833333333333\n",
      "initial_corpus size: 724\n",
      "length of s3h results:  5867 length of test set 60\n",
      "F1:  0.00269951071368\n",
      "precision s3h:  0.0013635588887 recall s3h:  0.133333333333\n",
      "initial_corpus size: 724\n",
      "length of results:  983 length of test set 60\n",
      "F1:  0.00958772770853\n",
      "precision fusion:  0.00508646998983 recall fusion:  0.0833333333333\n",
      "for length of results:  1966\n",
      "F1:  0.00691016781836\n",
      "precision fusion:  0.00356052899288 recall fusion:  0.116666666667\n",
      " for length of results:  2949\n",
      "F1:  0.00465270854104\n",
      "precision fusion:  0.00237368599525 recall fusion:  0.116666666667\n",
      " for length of results:  3932\n",
      "F1:  0.00400801603206\n",
      "precision fusion:  0.00203458799593 recall fusion:  0.133333333333\n",
      " for length of results:  4915\n",
      "F1:  0.00402010050251\n",
      "precision fusion:  0.00203458799593 recall fusion:  0.166666666667\n"
     ]
    }
   ],
   "source": [
    "bab = \"international affairs, international relations, world affairs, International politics, Intrel, Diplomacy and Statecraft, Diplomatic relationship, Global relations, International relation, International Relation Studies, International trade relations, Interstate relations, Levels of Analysis in international relations, Study of International Relations\"\n",
    "synset = bab.split(',')\n",
    "topic = 'international relations'\n",
    "res_pickle = \"results/international_relations_results.pickle\"\n",
    "IR_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(IR_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2497\n",
      "initial_corpus size: 2497\n",
      "babelnet results size of the topic \"Neuroimaging\": 3679\n",
      "ground truth size 522\n",
      "intersection with the ground truth: 52\n",
      "F1 0.0247560104737\n",
      "babel precision:  0.0141342756184\n",
      "babel recall:  0.0996168582375\n",
      "initial_corpus size: 2497\n",
      "length of s3h results:  9294 length of test set 522\n",
      "F1:  0.0350448247759\n",
      "precision s3h:  0.0185065633742 recall s3h:  0.329501915709\n",
      "initial_corpus size: 2497\n",
      "length of results:  3679 length of test set 522\n",
      "F1:  0.0252320875982\n",
      "precision fusion:  0.014406088611 recall fusion:  0.10153256705\n",
      "for length of results:  7358\n",
      "F1:  0.0378172588832\n",
      "precision fusion:  0.0202500679532 recall fusion:  0.285440613027\n",
      " for length of results:  11037\n",
      "F1:  0.0358162470802\n",
      "precision fusion:  0.0187550964936 recall fusion:  0.396551724138\n",
      " for length of results:  14716\n",
      "F1:  0.0312376952356\n",
      "precision fusion:  0.0161728730633 recall fusion:  0.455938697318\n",
      " for length of results:  18395\n",
      "F1:  0.0284400274885\n",
      "precision fusion:  0.0146235390052 recall fusion:  0.515325670498\n"
     ]
    }
   ],
   "source": [
    "bab = \"neuroimaging, Brain imaging, Brain scanning, Brain function map, Brain scan, Brain scans, Functional neurological mapping, Neuro-imaging, Neuroradiography\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Neuroimaging'\n",
    "res_pickle = \"results/Neuroimaging_results.pickle\"\n",
    "Ne_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Ne_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 7941\n",
      "initial_corpus size: 7941\n",
      "babelnet results size of the topic \"Literature\": 7357\n",
      "ground truth size 860\n",
      "intersection with the ground truth: 107\n",
      "F1 0.0260435682122\n",
      "babel precision:  0.0145439717276\n",
      "babel recall:  0.124418604651\n",
      "initial_corpus size: 7941\n",
      "length of s3h results:  8723 length of test set 860\n",
      "F1:  0.00918292810185\n",
      "precision s3h:  0.00504413619168 recall s3h:  0.0511627906977\n",
      "initial_corpus size: 7941\n",
      "length of results:  7357 length of test set 860\n",
      "F1:  0.0136302786905\n",
      "precision fusion:  0.00761179828735 recall fusion:  0.0651162790698\n",
      "for length of results:  14714\n",
      "F1:  0.0137408501348\n",
      "precision fusion:  0.0072719858638 recall fusion:  0.124418604651\n",
      " for length of results:  22071\n",
      "F1:  0.0109894902098\n",
      "precision fusion:  0.00570884871551 recall fusion:  0.146511627907\n",
      " for length of results:  29428\n",
      "F1:  0.00990491283677\n",
      "precision fusion:  0.00509718635313 recall fusion:  0.174418604651\n",
      " for length of results:  36785\n",
      "F1:  0.00940363926152\n",
      "precision fusion:  0.00481174391736 recall fusion:  0.205813953488\n"
     ]
    }
   ],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "res_pickle = \"results/res__Literature\"\n",
    "Lit_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Lit_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2356\n",
      "initial_corpus size: 2356\n",
      "babelnet results size of the topic \"Sociology\": 4718\n",
      "ground truth size 698\n",
      "intersection with the ground truth: 53\n",
      "F1 0.0195716395864\n",
      "babel precision:  0.0112335735481\n",
      "babel recall:  0.0759312320917\n",
      "initial_corpus size: 2356\n",
      "length of s3h results:  13403 length of test set 698\n",
      "F1:  0.00439685128714\n",
      "precision s3h:  0.00231291501903 recall s3h:  0.0444126074499\n",
      "initial_corpus size: 2356\n",
      "length of results:  4718 length of test set 698\n",
      "F1:  0.0162481536189\n",
      "precision fusion:  0.00932598558711 recall fusion:  0.0630372492837\n",
      "for length of results:  9436\n",
      "F1:  0.0100651272943\n",
      "precision fusion:  0.00540483255617 recall fusion:  0.0730659025788\n",
      " for length of results:  14154\n",
      "F1:  0.00794505790466\n",
      "precision fusion:  0.00416843295182 recall fusion:  0.0845272206304\n",
      " for length of results:  18872\n",
      "F1:  0.00674501788452\n",
      "precision fusion:  0.00349724459517 recall fusion:  0.0945558739255\n",
      " for length of results:  23590\n",
      "F1:  0.00601119894598\n",
      "precision fusion:  0.00309453158118 recall fusion:  0.104584527221\n"
     ]
    }
   ],
   "source": [
    "bab = \"sociology, Sociological, Sociologists, Marketing sociologist, Marketing sociology, Scientific sociology, Social physics, Socialogy, Sociol, Sociological inquiry, Sociological term, Sociological terms, sociologist, Sociology versus social theory, Sociology vs. Social Theory, Sosiology, Study of culture\"\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Sociology'\n",
    "res_pickle = \"results/res__Sociology\"\n",
    "Soc_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Soc_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 1231\n",
      "initial_corpus size: 1231\n",
      "babelnet results size of the topic \"Robotics\": 3705\n",
      "ground truth size 747\n",
      "intersection with the ground truth: 242\n",
      "F1 0.108715184187\n",
      "babel precision:  0.0653171390013\n",
      "babel recall:  0.323962516734\n",
      "initial_corpus size: 1231\n",
      "length of s3h results:  3122 length of test set 747\n",
      "F1:  0.0558283794262\n",
      "precision s3h:  0.0345932094811 recall s3h:  0.144578313253\n",
      "initial_corpus size: 1231\n",
      "length of results:  3705 length of test set 747\n",
      "F1:  0.0974842767296\n",
      "precision fusion:  0.0585695006748 recall fusion:  0.290495314592\n",
      "for length of results:  7410\n",
      "F1:  0.0649748682114\n",
      "precision fusion:  0.0357624831309 recall fusion:  0.354752342704\n",
      " for length of results:  11115\n",
      "F1:  0.0512561119541\n",
      "precision fusion:  0.0273504273504 recall fusion:  0.406961178046\n",
      " for length of results:  14820\n",
      "F1:  0.0423973790711\n",
      "precision fusion:  0.0222672064777 recall fusion:  0.441767068273\n",
      " for length of results:  18525\n",
      "F1:  0.0367372353674\n",
      "precision fusion:  0.0191093117409 recall fusion:  0.473895582329\n"
     ]
    }
   ],
   "source": [
    "bab = \"robotics, Autonomous Systems, Flying robots, Robotic, Robotic leg, Robotic legs, Socionics, Under water robotics, Unmanned systems\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Robotics'\n",
    "res_pickle = \"results/res__Robotics\"\n",
    "Ro_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Ro_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2145\n",
      "initial_corpus size: 2145\n",
      "babelnet results size of the topic \"Psychiatry\": 7448\n",
      "ground truth size 1700\n",
      "intersection with the ground truth: 250\n",
      "F1 0.054656755575\n",
      "babel precision:  0.0335660580021\n",
      "babel recall:  0.147058823529\n",
      "initial_corpus size: 2145\n",
      "length of s3h results:  5746 length of test set 1700\n",
      "F1:  0.0252484555466\n",
      "precision s3h:  0.0163592064045 recall s3h:  0.0552941176471\n",
      "initial_corpus size: 2145\n",
      "length of results:  7448 length of test set 1700\n",
      "F1:  0.0476606908614\n",
      "precision fusion:  0.0292696025779 recall fusion:  0.128235294118\n",
      "for length of results:  14896\n",
      "F1:  0.0384429983128\n",
      "precision fusion:  0.0214151450054 recall fusion:  0.187647058824\n",
      " for length of results:  22344\n",
      "F1:  0.0318582598569\n",
      "precision fusion:  0.0171410669531 recall fusion:  0.225294117647\n",
      " for length of results:  29792\n",
      "F1:  0.0286421948431\n",
      "precision fusion:  0.015138292159 recall fusion:  0.265294117647\n",
      " for length of results:  37240\n",
      "F1:  0.0255778120185\n",
      "precision fusion:  0.0133727175081 recall fusion:  0.292941176471\n"
     ]
    }
   ],
   "source": [
    "bab = \"psychiatry, psychopathology, psychological medicine, Adult Psychiatry, Criticism of psychiatry, Ethics of psychiatry, History of psychiatry, Mental Pathology, Modern psychiatry, Personalistic disease theories, Psichiatry, Psychaitry, Psychiatric, Psychiatric medicine, Psychiatric syndrome, Psychiatric treatment, psychiatrist, Psychiatrists, Psychological pathology, Psychopathological, Psychopathologist, Psycopathological, Psycopathology, Pyschiatric care, Pyschiatry\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Psychiatry'\n",
    "res_pickle = \"results/res__Psychiatry\"\n",
    "Psych_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Psych_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 565\n",
      "initial_corpus size: 565\n",
      "babelnet results size of the topic \"Pediatrics\": 8233\n",
      "ground truth size 747\n",
      "intersection with the ground truth: 223\n",
      "F1 0.0496659242762\n",
      "babel precision:  0.0270861168468\n",
      "babel recall:  0.298527443106\n",
      "initial_corpus size: 565\n",
      "length of s3h results:  8303 length of test set 747\n",
      "F1:  0.0117127071823\n",
      "precision s3h:  0.00638323497531 recall s3h:  0.0709504685408\n",
      "initial_corpus size: 565\n",
      "length of results:  8233 length of test set 747\n",
      "F1:  0.0307349665924\n",
      "precision fusion:  0.0167618122191 recall fusion:  0.184738955823\n",
      "for length of results:  16466\n",
      "F1:  0.0197525126358\n",
      "precision fusion:  0.0103243046277 recall fusion:  0.227576974565\n",
      " for length of results:  24699\n",
      "F1:  0.016191149886\n",
      "precision fusion:  0.00834041864043 recall fusion:  0.275769745649\n",
      " for length of results:  32932\n",
      "F1:  0.0141334362659\n",
      "precision fusion:  0.0072270132394 recall fusion:  0.318607764391\n",
      " for length of results:  41165\n",
      "F1:  0.0127409811033\n",
      "precision fusion:  0.00648609255435 recall fusion:  0.357429718876\n"
     ]
    }
   ],
   "source": [
    "bab = \" pediatrics, paediatrics, pediatric medicine, pedology, pediatrician, paediatrician, pædiatrics, Paediatric, Paediatricians, Pediatric, Pediatricians, Pediatrist, Pediatry, Pædiatric, Pædiatrician\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Pediatrics'\n",
    "res_pickle = \"results/res__Pediatrics\"\n",
    "Ped_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Ped_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 3922\n",
      "initial_corpus size: 3922\n",
      "babelnet results size of the topic \"Oncology\": 5705\n",
      "ground truth size 2937\n",
      "intersection with the ground truth: 107\n",
      "F1 0.024762786392\n",
      "babel precision:  0.0187554776512\n",
      "babel recall:  0.0364317330609\n",
      "initial_corpus size: 3922\n",
      "length of s3h results:  11875 length of test set 2937\n",
      "F1:  0.0421280043208\n",
      "precision s3h:  0.0262736842105 recall s3h:  0.106230847804\n",
      "initial_corpus size: 3922\n",
      "length of results:  5705 length of test set 2937\n",
      "F1:  0.0242999305716\n",
      "precision fusion:  0.0184049079755 recall fusion:  0.0357507660878\n",
      "for length of results:  11410\n",
      "F1:  0.0398689621524\n",
      "precision fusion:  0.0250657318142 recall fusion:  0.0973782771536\n",
      " for length of results:  17115\n",
      "F1:  0.0443846000399\n",
      "precision fusion:  0.0260005842828 recall fusion:  0.151515151515\n",
      " for length of results:  22820\n",
      "F1:  0.0459680863455\n",
      "precision fusion:  0.0259421560035 recall fusion:  0.201566224038\n",
      " for length of results:  28525\n",
      "F1:  0.046723030958\n",
      "precision fusion:  0.0257668711656 recall fusion:  0.250255362615\n"
     ]
    }
   ],
   "source": [
    "bab = \"oncology, oncologist, AllergoOncology, Cancer care, Medical oncology, Oncologic, Oncological, Oncologists, Oncology research, Oncology unit, Pediatric cancers\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Oncology'\n",
    "res_pickle = \"results/res__Oncology\"\n",
    "On_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(On_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8210\n",
      "initial_corpus size: 8210\n",
      "babelnet results size of the topic \"Mechanics\": 8222\n",
      "ground truth size 4640\n",
      "intersection with the ground truth: 1\n",
      "F1 0.000155496812315\n",
      "babel precision:  0.000121624908781\n",
      "babel recall:  0.000215517241379\n",
      "initial_corpus size: 8210\n",
      "length of s3h results:  9556 length of test set 4640\n",
      "F1:  0.028740490279\n",
      "precision s3h:  0.0213478442863 recall s3h:  0.0439655172414\n",
      "initial_corpus size: 8210\n",
      "length of results:  8222 length of test set 4640\n",
      "F1:  0.000932980873892\n",
      "precision fusion:  0.000729749452688 recall fusion:  0.00129310344828\n",
      "for length of results:  16444\n",
      "F1:  0.0247581104155\n",
      "precision fusion:  0.015872050596 recall fusion:  0.05625\n",
      " for length of results:  24666\n",
      "F1:  0.0337814781956\n",
      "precision fusion:  0.0200681099489 recall fusion:  0.106681034483\n",
      " for length of results:  32888\n",
      "F1:  0.035813259433\n",
      "precision fusion:  0.0204329846753 recall fusion:  0.144827586207\n",
      " for length of results:  41110\n",
      "F1:  0.0356284153005\n",
      "precision fusion:  0.0198248601314 recall fusion:  0.175646551724\n"
     ]
    }
   ],
   "source": [
    "bab = \" mechanics, History of mechanics, Mechanical processes, Particle mechanics, Theoretical mechanics\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Mechanics'\n",
    "res_pickle = \"results/res__Mechanics\"\n",
    "Mecha_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Mecha_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 275\n",
      "initial_corpus size: 275\n",
      "babelnet results size of the topic \"biophysics\": 3674\n",
      "ground truth size 323\n",
      "intersection with the ground truth: 19\n",
      "F1 0.00950713034776\n",
      "babel precision:  0.00517147523136\n",
      "babel recall:  0.0588235294118\n",
      "initial_corpus size: 275\n",
      "length of s3h results:  9602 length of test set 323\n",
      "F1:  0.00584382871537\n",
      "precision s3h:  0.00302020412414 recall s3h:  0.0897832817337\n",
      "initial_corpus size: 275\n",
      "length of results:  3674 length of test set 323\n",
      "F1:  0.00800600450338\n",
      "precision fusion:  0.00435492651062 recall fusion:  0.0495356037152\n",
      "for length of results:  7348\n",
      "F1:  0.00677877721288\n",
      "precision fusion:  0.00353837778987 recall fusion:  0.0804953560372\n",
      " for length of results:  11022\n",
      "F1:  0.00564125165271\n",
      "precision fusion:  0.00290328434041 recall fusion:  0.0990712074303\n",
      " for length of results:  14696\n",
      "F1:  0.00519342166589\n",
      "precision fusion:  0.00265378334241 recall fusion:  0.120743034056\n",
      " for length of results:  18370\n",
      "F1:  0.00492162841705\n",
      "precision fusion:  0.0025040827436 recall fusion:  0.142414860681\n"
     ]
    }
   ],
   "source": [
    "bab = \"biophysics, Biological physics, Biophysical, Biophysicists, History of biophysics\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'biophysics'\n",
    "res_pickle = \"results/res__biophysics\"\n",
    "biophy_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(biophy_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, pickle, argparse, json, os, urllib2\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_from(q, f):\n",
    "    q = q+'&from='+str(f)\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    subject_ids = np.array(range(len(data['hits'])), dtype=np.object)\n",
    "    for (i, hit) in enumerate(data['hits']):\n",
    "        subject_ids[i] = hit['id']\n",
    "    return subject_ids\n",
    "\n",
    "def query(q):\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    nb_requests = 1 + data['total'] / 1000\n",
    "    if nb_requests > 10: # maximum number of pages due to API pagination restrection\n",
    "        nb_requests = 10\n",
    "    subject_ids = query_from(q, 0)\n",
    "    for i in range(nb_requests)[1:]:\n",
    "        f = i * 1000\n",
    "        next_request = query_from(q, f)\n",
    "        subject_ids = np.hstack((subject_ids, next_request))\n",
    "    return subject_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['al-natsheh', 'hussein']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_intersection(list_a, list_b):\n",
    "    return list(set(list_a) & set(list_b))\n",
    "a = ['hussein', 'tayseer', 'al-natsheh']\n",
    "b = ['loay', 'hussein', 'al-natsheh']\n",
    "find_intersection(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original inversed_index\n",
      "[(u'ISTEX_D89FA3AC3521074D46F4245762153DF497BFFA1F', 2002320), (u'ISTEX_18EAF4D6A126B077EB38667801D1B7292F32FF49', 2483732), (u'ISTEX_5F91044435FCC4FABB9F02E31467DCFE75F4A7BE', 1429049)]\n",
      "processed inversed_index\n",
      "[(u'FCF1393F9B8136AC08FB67E88F94F3CF62C17288', 3517138), (u'482E1102A1114327A744FD2ADB4D9F8FF7E9A70B', 751643), (u'A81022B6295AE66F68A10222C3B94A06B033C1BA', 3983232)]\n"
     ]
    }
   ],
   "source": [
    "inv_index = json.load(open('../RecSys_Exp_files/182_381_vec150_results/output_paragraph_inversed_index.json','rb'))\n",
    "print 'original inversed_index'\n",
    "print inv_index.items()[:3]\n",
    "inversed_index = dict()\n",
    "for (k, v) in inv_index.items():\n",
    "    key = k.split('_')[1]\n",
    "    inversed_index[key] = v\n",
    "print 'processed inversed_index'\n",
    "print inversed_index.items()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "7488 55\n"
     ]
    }
   ],
   "source": [
    "def babelnet_get_input():\n",
    "    results = query('https://api.istex.fr/document/?q=((title:%22Biodiversity%20Conservation%22%20OR%20abstract:%22Biodiversity%20Conservation%22%20OR%20title:%22conservation%20biology%22%20OR%20abstract:%22conservation%20biology%22%20OR%20title:%22Animal%20conservation%22%20OR%20abstract:%22Animal%20conservation%22%20OR%20title:%22Biological%20Conservation%22%20OR%20abstract:%22Biological%20Conservation%22%20ORtitle:%22Conservation%20of%20natural%20resources%22%20OR%20abstract:%22Conservation%20of%20natural%20resources%22%20OR%20title:%22Conservation%20of%20wildlife%22%20OR%20abstract:%22Conservation%20of%20wildlife%22%20ORtitle:%22Wildlife%20Conservation%22%20OR%20abstract:%22Wildlife%20Conservation%22%20OR%20title:%22Ecological%20conservation%22%20OR%20abstract:%22Ecological%20conservation%22%20ORtitle:%22Conservation%20science%22%20OR%20abstract:%22Conservation%20science%22%20OR%20title:%22Conservation%22%20OR%20abstract:%22Conservation%22%20ORtitle:%22Conservation%20biologist%22%20OR%20abstract:%22Conservation%20biologist%22%20OR%20title:%22Conservation%20biologists%22%20OR%20abstract:%22Conservation%20biologists%22%20ORtitle:%22Conservation%20priority%22%20OR%20abstract:%22Conservation%20priority%22%20OR%20title:%22Earth%20biologist%22%20OR%20abstract:%22Earth%20biologist%22%20ORtitle:%22Earth%20biology%22%20OR%20abstract:%22Earth%20biology%22%20OR%20title:%22History%20of%20conservation%20biology%22%20OR%20abstract:%22History%20of%20conservation%20biology%22)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id')\n",
    "    _gs = query('https://api.istex.fr/document/?q=((subject.value:%22Biodiversity%20Conservation%22%20OR%20keywords:%22Biodiversity%20Conservation%22%20)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id')\n",
    "    results = find_intersection(results, inversed_index.keys())\n",
    "    _abst_title = query('https://api.istex.fr/document/?q=((title:%22Biodiversity%20Conservation%22%20OR%20abstract:%22Biodiversity%20Conservation%22)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id')\n",
    "    test_set = _inter = {x for x in _gs if x not in _abst_title}\n",
    "    test_set = find_intersection(test_set, inversed_index.keys())\n",
    "    results = list(results)\n",
    "    test = list(test_set)\n",
    "    print 'initial_corpus size:', len(find_intersection(_abst_title, inversed_index.keys()))\n",
    "    return results, test\n",
    "babelnet_results, test = babelnet_get_input()\n",
    "print len(babelnet_results), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%22conservation%20biology%22'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def term2url(string):\n",
    "    string = string.split(' ')\n",
    "    res = '%22'\n",
    "    for s in string:\n",
    "        res = res + s + '%20'\n",
    "    res = res[:-3]\n",
    "    res = res + '%22'\n",
    "    return res\n",
    "term2url('conservation biology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emergency%20medicine', 'Emergency%20care', 'Emergency%20med', 'Emergency%20treatment', 'Emergent%20condition', 'Emergentology', 'Er%20physician']\n"
     ]
    }
   ],
   "source": [
    "use_synset = \"Emergency medicine, Emergency care, Emergency med, Emergency treatment, Emergent condition, Emergentology, Er physician\"\n",
    "synset_t = use_synset.split(', ')\n",
    "synset = []\n",
    "for syn in synset_t:\n",
    "    x = syn.replace(' ','%20')\n",
    "    synset.append(x)\n",
    "print synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sports science',\n",
       " ' Sport science',\n",
       " ' Sport Sciences',\n",
       " ' Human movement',\n",
       " ' Human movement studies',\n",
       " ' Science and sports',\n",
       " ' Sport and Exercise Science',\n",
       " ' Sport scientist',\n",
       " ' Sports sciences',\n",
       " ' Sports scientist',\n",
       " ' Sports scientists']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_text = 'Sports science, Sport science, Sport Sciences, Human movement, Human movement studies, Science and sports, Sport and Exercise Science, Sport scientist, Sports sciences, Sports scientist, Sports scientists'\n",
    "synset = synset_text.split(',')\n",
    "synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def babel_synset(synset):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    for syn in synset:\n",
    "        syn = term2url(syn)\n",
    "        q = q + 'title:' + syn + '%20OR%20abstract:' + syn + '%20OR%20'\n",
    "    q = q[:-8]\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    "\n",
    "#test babel_synset\n",
    "#synset = ['conservation biology', 'Animal conservation', 'Biological Conservation', 'Conservation', 'Biodiversity conservation', 'Conservation biologist', 'Conservation biologists', 'Conservation of natural resources', 'Conservation of wildlife', 'Conservation priority', 'Conservation science', 'Earth biologist', 'Earth biology', 'Ecological conservation', 'History of conservation biology', 'Wildlife Conservation']\n",
    "#print babel_synset(synset)\n",
    "#print 'https://api.istex.fr/document/?q=((title:%22conservation%20biology%22%20OR%20abstract:%22conservation%20biology%22%20ORtitle:%22Animal%20conservation%22%20OR%20abstract:%22Animal%20conservation%22%20OR%20title:%22Biological%20Conservation%22%20OR%20abstract:%22Biological%20Conservation%22%20ORtitle:%22Conservation%20of%20natural%20resources%22%20OR%20abstract:%22Conservation%20of%20natural%20resources%22%20OR%20title:%22Conservation%20of%20wildlife%22%20OR%20abstract:%22Conservation%20of%20wildlife%22%20ORtitle:%22Wildlife%20Conservation%22%20OR%20abstract:%22Wildlife%20Conservation%22%20OR%20title:%22Ecological%20conservation%22%20OR%20abstract:%22Ecological%20conservation%22%20ORtitle:%22Conservation%20science%22%20OR%20abstract:%22Conservation%20science%22%20OR%20title:%22Conservation%22%20OR%20abstract:%22Conservation%22%20ORtitle:%22Conservation%20biologist%22%20OR%20abstract:%22Conservation%20biologist%22%20OR%20title:%22Conservation%20biologists%22%20OR%20abstract:%22Conservation%20biologists%22%20ORtitle:%22Conservation%20priority%22%20OR%20abstract:%22Conservation%20priority%22%20OR%20title:%22Earth%20biologist%22%20OR%20abstract:%22Earth%20biologist%22%20ORtitle:%22Earth%20biology%22%20OR%20abstract:%22Earth%20biology%22%20OR%20title:%22History%20of%20conservation%20biology%22%20OR%20abstract:%22History%20of%20conservation%20biology%22)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "\n",
    "def babel_subj_keyword(topic):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    topic = term2url(topic)\n",
    "    q = q+ 'subject.value:' + topic + '%20OR%20keywords:' + topic\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    " \n",
    "#test babel_subj_keyword\n",
    "#print babel_subj_keyword('Biodiversity Conservation')\n",
    "#print 'https://api.istex.fr/document/?q=((subject.value:%22Biodiversity%20Conservation%22%20OR%20keywords:%22Biodiversity%20Conservation%22%20)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "\n",
    "def babel_title_abst(topic):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    topic = term2url(topic)\n",
    "    q = q+ 'title:' + topic + '%20OR%20abstract:' + topic\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    " \n",
    "#test babel_title_abst\n",
    "#print babel_title_abst('Biodiversity Conservation')\n",
    "#print 'https://api.istex.fr/document/?q=((title:%22Biodiversity%20Conservation%22%20OR%20abstract:%22Biodiversity%20Conservation%22)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "7481 55\n"
     ]
    }
   ],
   "source": [
    "def babelnet_syn_get_input(topic, synset):\n",
    "    results = query(babel_synset(synset))\n",
    "    _gs = query(babel_subj_keyword(topic))\n",
    "    results = find_intersection(results, inversed_index.keys())\n",
    "    _abst_title = query(babel_title_abst(topic))\n",
    "    test_set = _inter = {x for x in _gs if x not in _abst_title}\n",
    "    test_set = find_intersection(test_set, inversed_index.keys())\n",
    "    results = list(results)\n",
    "    test = list(test_set)\n",
    "    print 'initial_corpus size:', len(find_intersection(_abst_title, inversed_index.keys()))\n",
    "    return results, test\n",
    "#test babelnet_syn_get_input\n",
    "synset = ['conservation biology', 'Animal conservation', 'Biological Conservation', 'Conservation', 'Biodiversity conservation', 'Conservation biologist', 'Conservation biologists', 'Conservation of natural resources', 'Conservation of wildlife', 'Conservation priority', 'Conservation science', 'Earth biologist', 'Earth biology', 'Ecological conservation', 'History of conservation biology', 'Wildlife Conservation']\n",
    "babelnet_results, test = babelnet_syn_get_input('Biodiversity Conservation', synset)\n",
    "print len(babelnet_results), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cond = pickle.load(open('results/res__Condensed_matter','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'269044D0F0C5DD5F37043AA209B72F7DA99003FE', 0.98799999999999999),\n",
       " (u'421D402518F03D805E8A29EF04D4BE93539F1FA7', 0.97999999999999998),\n",
       " (u'658F818CB292894E38B02C72EA9BA602AA47A587', 0.97599999999999998),\n",
       " (u'E189B02A80A7DD0E49D85098A0ABD9D3DEB78F62', 0.97599999999999998),\n",
       " (u'E95A15FB563CC5A94E27EFEA16BB4B9F7C71313B', 0.97599999999999998),\n",
       " (u'2FB037B74E1B24E4BD7EC87F46D04CCCB373D212', 0.97399999999999998),\n",
       " (u'6A3390346932D416990B07583AFD6D44AE91A219', 0.97399999999999998),\n",
       " (u'466EAC8621A99784C5F6EEEF54C6DED85D4D97F9', 0.97199999999999998),\n",
       " (u'D550E2F31532FA9D0D5211CE584998B69BCE3CC6', 0.96999999999999997),\n",
       " (u'776E1BBE47292EF9047FF0671A9DF0D6B693ABCE', 0.96999999999999997)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3h_results = pickle.load(open('results/BC0t_results.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3h_syn_results = pickle.load(open('results/BC0t_1_RFC_syn_results.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "babel_test_intersection_size = len(babel_test_intersection)\n",
    "print babel_test_intersection_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def babelnet_eval(topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    print 'results size of the topic \"' + topic + '\":', len(babelnet_results) \n",
    "    print 'ground truth size', len(test)\n",
    "    babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "    babel_test_intersection_size = len(babel_test_intersection)\n",
    "    print 'intersection with the ground truth:', babel_test_intersection_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 4642\n",
      "results size of the topic \"Remote sensing\": 4685\n",
      "ground truth size 1192\n",
      "intersection with the ground truth: 3\n"
     ]
    }
   ],
   "source": [
    "synset = ['Remote sensing', 'Infrared remote sensing', 'Passive remote sensing', 'Remote-sensing', 'Remote-Sensing Image', 'Remote Sensing Satellites', 'Remote sensor']\n",
    "topic = 'Remote sensing'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 20\n",
      "results size of the topic \"agricultural engineering\": 46\n",
      "ground truth size 5\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "agri = 'agricultural engineering, agrotechnology, Farm Engineering, Agricultural engineer, Agricultural engineers, Agriculture Engineering'\n",
    "synset = agri.split(',')\n",
    "topic = 'agricultural engineering'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 795\n",
      "results size of the topic \"Respiratory system\": 1024\n",
      "ground truth size 189\n",
      "intersection with the ground truth: 3\n"
     ]
    }
   ],
   "source": [
    "synset = ['Respiratory system', 'systema respiratorium', 'Respiration organ', 'Breathing organ', 'Breathing system', 'Development of respiratory system', 'Human Respiration', 'Human respiratory system', 'Pulmonary respiration', 'Pulmonary system', 'Respatory system', 'Respiration of human', 'Respiration system, Respiratory', 'Respiratory organs', 'Respiratory system agents', 'Respiratory system disorders', 'Respiratory systems', 'Respitory System', 'Subglottic airway', 'The respiratory system']\n",
    "topic = 'Respiratory system'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 810\n",
      "results size of the topic \"Emergency medicine\": 1379\n",
      "ground truth size 285\n",
      "intersection with the ground truth: 14\n"
     ]
    }
   ],
   "source": [
    "synset = ['Emergency medicine', 'Emergency care', 'Emergency med', 'Emergency treatment', 'Emergent condition', 'Emergentology', 'Er physician']\n",
    "topic = 'Emergency medicine'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2658\n",
      "results size of the topic \"Substance abuse\": 7893\n",
      "ground truth size 466\n",
      "intersection with the ground truth: 66\n"
     ]
    }
   ],
   "source": [
    "synset = ['Substance abuse', 'drug abuse', 'habit', 'addiction', 'dependency', 'Abuse potential', 'Abusing drugs', 'Abusive drug use', 'Anti-drug', 'Cannabis abuse', 'Drug-abuse', 'Drug misuse', 'Drug prevention', 'Drugs of abuse', 'Illegal drug abuse', 'Illegal drug use', 'Misuse of drugs', 'Narcotic abuse theory', 'Nondependent abuse of drugs', 'Prescription drug abuse', 'Prescription Drug Misuse']\n",
    "topic = 'Substance abuse'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 724\n",
      "results size of the topic \"international relations\": 983\n",
      "ground truth size 60\n",
      "intersection with the ground truth: 5\n"
     ]
    }
   ],
   "source": [
    "synset = ['international relations', 'international affairs', 'world affairs', 'International politics', 'Intrel', 'Diplomacy and Statecraft', 'Diplomatic relationship', 'Global relations', 'International relation', 'International Relation Studies', 'International trade relations', 'Interstate relations', 'Levels of Analysis in international relations', 'Study of International Relations']\n",
    "topic = 'international relations'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 3\n",
      "results size of the topic \"Digital forensics\": 34\n",
      "ground truth size 0\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "synset = ['forensic computing', 'Digital forensics', 'Digital forensic science', 'Evidence collection', 'Forensic computing', 'Intrusion investigation']\n",
    "topic = 'Digital forensics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_all = pickle.load(open('results/Substance_results.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_all_np = np.array(ranked_all.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top50 = ranked_all_np[:50]\n",
    "#top50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>istex_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6E940E7B1FEE274AE69561BD9B2C6FE55BF45951</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>852F700B181D092A2FAC5D405B51824FC49EA929</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   istex_id  score\n",
       "0  6E940E7B1FEE274AE69561BD9B2C6FE55BF45951   0.99\n",
       "1  852F700B181D092A2FAC5D405B51824FC49EA929   0.99"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top50_df = pd.DataFrame(data=top50, index=None, columns=['istex_id', 'score'], copy=False)\n",
    "top50_df['score'] = top50_df[['score']].astype(float)\n",
    "top50_df[top50_df['score'] > 0.988]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "def top_thresh(ordered_dict_pickle, thresh):\n",
    "    ranked_all = pickle.load(open(ordered_dict_pickle, 'rb'))\n",
    "    ranked_all_np = np.array(ranked_all.items())\n",
    "    ranked_all_df = pd.DataFrame(data=ranked_all_np, index=None, columns=['istex_id', 'score'])\n",
    "    ranked_all_df['score'] = ranked_all_df[['score']].astype(float)\n",
    "    return ranked_all_df[ranked_all_df['score'] > thresh]\n",
    "top_substance_95 = top_thresh('results/Substance_results.pickle', 0.95)\n",
    "print len(top_substance_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9988\n"
     ]
    }
   ],
   "source": [
    "def top_thresh_lst(res_lst_pickle, thresh):\n",
    "    ranked_all = pickle.load(open(res_lst_pickle, 'rb'))\n",
    "    ranked_all_np = np.array(ranked_all)\n",
    "    ranked_all_df = pd.DataFrame(data=ranked_all_np, index=None, columns=['istex_id', 'score'])\n",
    "    ranked_all_df['score'] = ranked_all_df[['score']].astype(float)\n",
    "    return ranked_all_df[ranked_all_df['score'] > thresh]\n",
    "top_cond_75 = top_thresh_lst('results/res__Condensed_matter', 0.75)\n",
    "print len(top_cond_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_substance_85 = top_thresh('results/Substance_results.pickle', 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368\n"
     ]
    }
   ],
   "source": [
    "print len(top_substance_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "top_respiratory_85 = top_thresh('results/Respiratory_system_results.pickle', 0.85)\n",
    "print len(top_respiratory_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate 3SH results at treshold\n",
    "def eval_at_thresh(ordered_dict_pickle, topic, synset, thresh):\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 795\n",
      "3sh tresh results ofRespiratory system\n",
      "precision: 0.00421274354924 recall: 0.042328042328\n",
      "number of selected items \"3sh results lenght\": 1899 lenght of correct answers: 189\n"
     ]
    }
   ],
   "source": [
    "topic = 'Respiratory system'\n",
    "synset = ['Respiratory system', 'systema respiratorium', 'Respiration organ', 'Breathing organ', 'Breathing system', 'Development of respiratory system', 'Human Respiration', 'Human respiratory system', 'Pulmonary respiration', 'Pulmonary system', 'Respatory system', 'Respiration of human', 'Respiration system, Respiratory', 'Respiratory organs', 'Respiratory system agents', 'Respiratory system disorders', 'Respiratory systems', 'Respitory System', 'Subglottic airway', 'The respiratory system']\n",
    "p , r, n, t = eval_at_thresh('results/Respiratory_system_results.pickle', topic , synset, 0.7)\n",
    "print '3sh tresh results of'+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2658\n",
      "3sh tresh results of Substance abuse\n",
      "precision: 0.00862325304787 recall: 0.124463519313\n",
      "number of selected items \"3sh results lenght\": 6726 lenght of correct answers: 466\n"
     ]
    }
   ],
   "source": [
    "topic = 'Substance abuse'\n",
    "synset = ['Substance abuse', 'drug abuse', 'habit', 'addiction', 'dependency', 'Abuse potential', 'Abusing drugs', 'Abusive drug use', 'Anti-drug', 'Cannabis abuse', 'Drug-abuse', 'Drug misuse', 'Drug prevention', 'Drugs of abuse', 'Illegal drug abuse', 'Illegal drug use', 'Misuse of drugs', 'Narcotic abuse theory', 'Nondependent abuse of drugs', 'Prescription drug abuse', 'Prescription Drug Misuse']\n",
    "p , r, n, t = eval_at_thresh('results/Substance_results.pickle', topic , synset, 0.7)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 4642\n",
      "3sh tresh results of Remote sensing\n",
      "precision: 0.0226702111948 recall: 0.515100671141\n",
      "number of selected items \"3sh results lenght\": 27084 lenght of correct answers: 1192\n"
     ]
    }
   ],
   "source": [
    "topic = 'Remote sensing'\n",
    "synset = ['Remote sensing', 'Infrared remote sensing', 'Passive remote sensing', 'Remote-sensing', 'Remote-Sensing Image', 'Remote Sensing Satellites', 'Remote sensor']\n",
    "p , r, n, t = eval_at_thresh('results/Remote_sensing_results.pickle', topic , synset, 0.7)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 724\n",
      "3sh tresh results of international relations\n",
      "precision: 0.00126830393174 recall: 0.183333333333\n",
      "number of selected items \"3sh results lenght\": 8673 lenght of correct answers: 60\n"
     ]
    }
   ],
   "source": [
    "synset = ['international relations', 'international affairs', 'world affairs', 'International politics', 'Intrel', 'Diplomacy and Statecraft', 'Diplomatic relationship', 'Global relations', 'International relation', 'International Relation Studies', 'International trade relations', 'Interstate relations', 'Levels of Analysis in international relations', 'Study of International Relations']\n",
    "topic = 'international relations'\n",
    "p , r, n, t = eval_at_thresh('results/international_relations_results.pickle', topic , synset, 0.7)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 810\n",
      "3sh tresh results of Emergency medicine\n",
      "precision: 0.00237529691211 recall: 0.0105263157895\n",
      "number of selected items \"3sh results lenght\": 1263 lenght of correct answers: 285\n"
     ]
    }
   ],
   "source": [
    "synset = ['Emergency medicine', 'Emergency care', 'Emergency med', 'Emergency treatment', 'Emergent condition', 'Emergentology', 'Er physician']\n",
    "topic = 'Emergency medicine'\n",
    "p , r, n, t = eval_at_thresh('results/Emergence_results.pickle', topic , synset, 0.7)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "3sh tresh results of Biodiversity Conservation\n",
      "precision: 0.00338218714769 recall: 0.0545454545455\n",
      "number of selected items \"3sh results lenght\": 887 lenght of correct answers: 55\n"
     ]
    }
   ],
   "source": [
    "synset = ['conservation biology', 'Animal conservation', 'Biological Conservation', 'Conservation', 'Biodiversity conservation', 'Conservation biologist', 'Conservation biologists', 'Conservation of natural resources', 'Conservation of wildlife', 'Conservation priority', 'Conservation science', 'Earth biologist', 'Earth biology', 'Ecological conservation', 'History of conservation biology', 'Wildlife Conservation']\n",
    "topic = 'Biodiversity Conservation'\n",
    "p , r, n, t = eval_at_thresh('results/Biodiversity_Conservation_results.pickle', topic , synset, 0.7)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synset = ['conservation biology', 'Animal conservation', 'Biological Conservation', 'Conservation', 'Biodiversity conservation', 'Conservation biologist', 'Conservation biologists', 'Conservation of natural resources', 'Conservation of wildlife', 'Conservation priority', 'Conservation science', 'Earth biologist', 'Earth biology', 'Ecological conservation', 'History of conservation biology', 'Wildlife Conservation']\n",
    "topic = 'Biodiversity Conservation'\n",
    "p , r, n, t = eval_at_thresh('results/Biodiversity_Conservation_results.pickle', topic , synset, 0.7)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646\n",
      "3207\n",
      "6038\n"
     ]
    }
   ],
   "source": [
    "top_bio_65 = top_thresh('results/Biodiversity_Conservation_results.pickle', 0.65)\n",
    "print len(top_bio_65)\n",
    "top_bio_60 = top_thresh('results/Biodiversity_Conservation_results.pickle', 0.60)\n",
    "print len(top_bio_60)\n",
    "top_bio_55 = top_thresh('results/Biodiversity_Conservation_results.pickle', 0.55)\n",
    "print len(top_bio_55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "3sh tresh results of Biodiversity Conservation\n",
      "precision: 0.00311817898347 recall: 0.181818181818\n",
      "number of selected items \"3sh results lenght\": 3207 lenght of correct answers: 55\n"
     ]
    }
   ],
   "source": [
    "synset = ['conservation biology', 'Animal conservation', 'Biological Conservation', 'Conservation', 'Biodiversity conservation', 'Conservation biologist', 'Conservation biologists', 'Conservation of natural resources', 'Conservation of wildlife', 'Conservation priority', 'Conservation science', 'Earth biologist', 'Earth biology', 'Ecological conservation', 'History of conservation biology', 'Wildlife Conservation']\n",
    "topic = 'Biodiversity Conservation'\n",
    "p , r, n, t = eval_at_thresh('results/Biodiversity_Conservation_results.pickle', topic , synset, 0.6)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "3sh tresh results of Biodiversity Conservation\n",
      "precision: 0.00231864855913 recall: 0.254545454545\n",
      "number of selected items \"3sh results lenght\": 6038 lenght of correct answers: 55\n"
     ]
    }
   ],
   "source": [
    "synset = ['conservation biology', 'Animal conservation', 'Biological Conservation', 'Conservation', 'Biodiversity conservation', 'Conservation biologist', 'Conservation biologists', 'Conservation of natural resources', 'Conservation of wildlife', 'Conservation priority', 'Conservation science', 'Earth biologist', 'Earth biology', 'Ecological conservation', 'History of conservation biology', 'Wildlife Conservation']\n",
    "topic = 'Biodiversity Conservation'\n",
    "p , r, n, t = eval_at_thresh('results/Biodiversity_Conservation_results.pickle', topic , synset, 0.55)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7655\n",
      "12323\n",
      "18560\n"
     ]
    }
   ],
   "source": [
    "top_remote_85 = top_thresh('results/Remote_sensing_results.pickle', 0.85)\n",
    "print len(top_remote_85)\n",
    "top_remote_80 = top_thresh('results/Remote_sensing_results.pickle', 0.80)\n",
    "print len(top_remote_80)\n",
    "top_remote_75 = top_thresh('results/Remote_sensing_results.pickle', 0.75)\n",
    "print len(top_remote_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 4642\n",
      "3sh tresh results of Remote sensing\n",
      "precision: 0.0335728282169 recall: 0.215604026846\n",
      "number of selected items \"3sh results lenght\": 7655 lenght of correct answers: 1192\n"
     ]
    }
   ],
   "source": [
    "topic = 'Remote sensing'\n",
    "synset = ['Remote sensing', 'Infrared remote sensing', 'Passive remote sensing', 'Remote-sensing', 'Remote-Sensing Image', 'Remote Sensing Satellites', 'Remote sensor']\n",
    "p , r, n, t = eval_at_thresh('results/Remote_sensing_results.pickle', topic , synset, 0.85)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 810\n",
      "3sh tresh results of Emergency medicine\n",
      "precision: 0.00451671183379 recall: 0.0701754385965\n",
      "number of selected items \"3sh results lenght\": 4428 lenght of correct answers: 285\n"
     ]
    }
   ],
   "source": [
    "synset = ['Emergency medicine', 'Emergency care', 'Emergency med', 'Emergency treatment', 'Emergent condition', 'Emergentology', 'Er physician']\n",
    "topic = 'Emergency medicine'\n",
    "p , r, n, t = eval_at_thresh('results/Emergence_results.pickle', topic , synset, 0.55)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 795\n",
      "3sh tresh results ofRespiratory system\n",
      "precision: 0.00223435948361 recall: 0.0952380952381\n",
      "number of selected items \"3sh results lenght\": 8056 lenght of correct answers: 189\n"
     ]
    }
   ],
   "source": [
    "topic = 'Respiratory system'\n",
    "synset = ['Respiratory system', 'systema respiratorium', 'Respiration organ', 'Breathing organ', 'Breathing system', 'Development of respiratory system', 'Human Respiration', 'Human respiratory system', 'Pulmonary respiration', 'Pulmonary system', 'Respatory system', 'Respiration of human', 'Respiration system, Respiratory', 'Respiratory organs', 'Respiratory system agents', 'Respiratory system disorders', 'Respiratory systems', 'Respitory System', 'Subglottic airway', 'The respiratory system']\n",
    "p , r, n, t = eval_at_thresh('results/Respiratory_system_results.pickle', topic , synset, 0.55)\n",
    "print '3sh tresh results of'+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2091\n",
      "3sh tresh results ofartificial intelligence\n",
      "precision: 0.00484888073763 recall: 0.293759512938\n",
      "number of selected items \"3sh results lenght\": 39803 lenght of correct answers: 657\n"
     ]
    }
   ],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "p , r, n, t = eval_at_thresh('results/AI_results.pickle', topic , synset, 0.75)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 5033\n",
      "3sh tresh results ofInformation Systems\n",
      "precision: 0.0116214720531 recall: 0.0799695354151\n",
      "number of selected items \"3sh results lenght\": 9035 lenght of correct answers: 1313\n"
     ]
    }
   ],
   "source": [
    "bab = 'information system, data system, information systems, system info, Business computing, Computer information system, Computer information systems, Elements of Information System, Information in Computer Science, Information systems and technology, Information systems discipline, Information systems theory, Informationssystem'\n",
    "synset = bab.split(',')\n",
    "topic = 'Information Systems'\n",
    "p , r, n, t = eval_at_thresh('results/infosys_results.pickle', topic , synset, 0.75)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def babelnet_eval_PR(topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    print 'babelnet results size of the topic \"' + topic + '\":', len(babelnet_results) \n",
    "    print 'ground truth size', len(test)\n",
    "    babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "    babel_test_intersection_size = len(babel_test_intersection)\n",
    "    print 'intersection with the ground truth:', babel_test_intersection_size\n",
    "    print 'babel precision: ', babel_test_intersection_size / float(len(babelnet_results))\n",
    "    print 'babel recall: ', babel_test_intersection_size / float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate 3SH results at treshold\n",
    "def eval_all_at_thresh(ordered_dict_pickle, topic, synset, thresh):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    if n > 10000:\n",
    "        thresh = thresh + 0.1\n",
    "        top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    elif n < 1000:\n",
    "        thresh = thresh - 0.1\n",
    "        top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate 3SH results list at treshold\n",
    "def eval_all_at_thresh_lst(res_pickle, topic, synset, thresh=0.75):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    if n > 10000:\n",
    "        thresh = thresh + 0.1\n",
    "        top_res = top_thresh_lst(res_pickle, thresh)\n",
    "        if len(top_res) > 10000:\n",
    "            thresh = thresh + 0.05\n",
    "            top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    elif n < 1000:\n",
    "        thresh = thresh - 0.1\n",
    "        top_res = top_thresh_lst(res_pickle, thresh)\n",
    "        if len(top_res) > 1000:\n",
    "            thresh = thresh - 0.05\n",
    "            top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2091\n",
      "babelnet results size of the topic \"artificial intelligence\": 7903\n",
      "ground truth size 657\n",
      "intersection with the ground truth: 43\n",
      "babel precision:  0.00544097178287\n",
      "babel recall:  0.0654490106545\n",
      "initial_corpus size: 2091\n",
      "3sh tresh results of artificial intelligence\n",
      "precision: 0.00933924819052 recall: 0.121765601218\n",
      "number of selected items \"3sh results lenght\": 8566 lenght of correct answers: 657\n",
      "threshold =  0.85\n"
     ]
    }
   ],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "p , r, n, t, thresh = eval_all_at_thresh('results/AI_results.pickle', topic , synset, thresh = 0.75)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t\n",
    "print 'threshold = ', thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 5033\n",
      "babelnet results size of the topic \"Information Systems\": 8440\n",
      "ground truth size 1313\n",
      "intersection with the ground truth: 298\n",
      "babel precision:  0.035308056872\n",
      "babel recall:  0.226961157654\n",
      "initial_corpus size: 5033\n",
      "3sh tresh results of Information Systems\n",
      "precision: 0.0104105571848 recall: 0.0540746382331\n",
      "number of selected items \"3sh results lenght\": 6820 lenght of correct answers: 1313\n",
      "threshold =  0.75\n"
     ]
    }
   ],
   "source": [
    "bab = 'information system, data system, information systems, system info, Business computing, Computer information system, Computer information systems, Elements of Information System, Information in Computer Science, Information systems and technology, Information systems discipline, Information systems theory, Informationssystem'\n",
    "synset = bab.split(',')\n",
    "topic = 'Information Systems'\n",
    "p , r, n, t, thresh = eval_all_at_thresh('results/infosys_results.pickle', topic , synset, thresh = 0.75)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t\n",
    "print 'threshold = ', thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2497\n",
      "babelnet results size of the topic \"Neuroimaging\": 3679\n",
      "ground truth size 522\n",
      "intersection with the ground truth: 52\n",
      "babel precision:  0.0141342756184\n",
      "babel recall:  0.0996168582375\n",
      "initial_corpus size: 2497\n",
      "3sh tresh results of Neuroimaging\n",
      "precision: 0.0153054221002 recall: 0.427203065134\n",
      "number of selected items \"3sh results lenght\": 14570 lenght of correct answers: 522\n",
      "threshold =  0.85\n"
     ]
    }
   ],
   "source": [
    "bab = 'neuroimaging, Brain imaging, Brain scanning, Brain function map, Brain scan, Brain scans, Functional neurological mapping, Neuro-imaging, Neuroradiography'\n",
    "synset = bab.split(',')\n",
    "topic = 'Neuroimaging'\n",
    "p , r, n, t, thresh = eval_all_at_thresh('results/Neuroimaging_results.pickle', topic , synset, thresh = 0.75)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t\n",
    "print 'threshold = ', thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2588\n",
      "babelnet results size of the topic \"Infectious Diseases\": 8343\n",
      "ground truth size 1375\n",
      "intersection with the ground truth: 50\n",
      "babel precision:  0.00599304806425\n",
      "babel recall:  0.0363636363636\n",
      "initial_corpus size: 2588\n",
      "3sh tresh results of Infectious Diseases\n",
      "precision: 0.00153295861012 recall: 0.00218181818182\n",
      "number of selected items \"3sh results lenght\": 1957 lenght of correct answers: 1375\n",
      "threshold =  0.75\n"
     ]
    }
   ],
   "source": [
    "bab = 'infection, infectious disease, communicable diseases, contagion, Infectious diseases, Acute infection, AIDS-related bacterial infections, AIDS-related viral infections, Anti-infective, Anti-infectives, Antiinfective, Bacterial Infections, Communicable disease, Contagious diseases, Definition to contagious, Infect, Infecting, Infectiology, Infections, Infectious, Infectious disease epidemiology, Infectious disease medicine, Infectology, Local infection, Primary infection, Rochalimea infections, Secondary infection, Tropical bacterial infections, Tropical infections, Viral Infections, Wound colonization, Wound infection'\n",
    "synset = bab.split(',')\n",
    "topic = 'Infectious Diseases'\n",
    "p , r, n, t, thresh = eval_all_at_thresh('results/Infectious_results.pickle', topic , synset, thresh = 0.75)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t\n",
    "print 'threshold = ', thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "babelnet results size of the topic \"Biodiversity Conservation\": 7481\n",
      "ground truth size 55\n",
      "intersection with the ground truth: 12\n",
      "babel precision:  0.00160406362786\n",
      "babel recall:  0.218181818182\n",
      "initial_corpus size: 344\n",
      "3sh tresh results of Biodiversity Conservation\n",
      "precision: 0.00243013365735 recall: 0.0727272727273\n",
      "number of selected items \"3sh results lenght\": 1646 lenght of correct answers: 55\n",
      "threshold =  0.65\n"
     ]
    }
   ],
   "source": [
    "synset = ['conservation biology', 'Animal conservation', 'Biological Conservation', 'Conservation', 'Biodiversity conservation', 'Conservation biologist', 'Conservation biologists', 'Conservation of natural resources', 'Conservation of wildlife', 'Conservation priority', 'Conservation science', 'Earth biologist', 'Earth biology', 'Ecological conservation', 'History of conservation biology', 'Wildlife Conservation']\n",
    "topic = 'Biodiversity Conservation'\n",
    "p , r, n, t, thresh = eval_all_at_thresh('results/Biodiversity_Conservation_results.pickle', topic , synset, thresh = 0.75)\n",
    "print '3sh tresh results of '+topic\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"3sh results lenght\":', n, 'lenght of correct answers:', t\n",
    "print 'threshold = ', thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate 3SH results at topN\n",
    "def eval_at_n(results_file, topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_results) \n",
    "    results = pickle.load(open(results_file, 'rb'))\n",
    "    topn = pd.DataFrame(results.items()[:n])\n",
    "    t = len(test)\n",
    "    recall = len(find_intersection(test,topn[0]))/float(t)\n",
    "    precision = len(find_intersection(test,topn[0]))/float(n)\n",
    "    return precision, recall, n, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2658\n",
      "precision: 0.00810845052578 recall: 0.137339055794\n",
      "number of selected items \"babelnet results lenght\": 7893 lenght of correct answers: 466\n"
     ]
    }
   ],
   "source": [
    "synset = ['Substance abuse', 'drug abuse', 'habit', 'addiction', 'dependency', 'Abuse potential', 'Abusing drugs', 'Abusive drug use', 'Anti-drug', 'Cannabis abuse', 'Drug-abuse', 'Drug misuse', 'Drug prevention', 'Drugs of abuse', 'Illegal drug abuse', 'Illegal drug use', 'Misuse of drugs', 'Narcotic abuse theory', 'Nondependent abuse of drugs', 'Prescription drug abuse', 'Prescription Drug Misuse']\n",
    "p , r, n, t = eval_at_n('results/Substance_results.pickle', 'Substance abuse', synset)\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"babelnet results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 795\n",
      "precision: 0.00390625 recall: 0.021164021164\n",
      "number of selected items \"babelnet results lenght\": 1024 lenght of correct answers: 189\n"
     ]
    }
   ],
   "source": [
    "synset = ['Respiratory system', 'systema respiratorium', 'Respiration organ', 'Breathing organ', 'Breathing system', 'Development of respiratory system', 'Human Respiration', 'Human respiratory system', 'Pulmonary respiration', 'Pulmonary system', 'Respatory system', 'Respiration of human', 'Respiration system, Respiratory', 'Respiratory organs', 'Respiratory system agents', 'Respiratory system disorders', 'Respiratory systems', 'Respitory System', 'Subglottic airway', 'The respiratory system']\n",
    "p , r, n, t = eval_at_n('results/Respiratory_system_results.pickle', 'Respiratory system', synset)\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"babelnet results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 4642\n",
      "precision: 0.0345784418356 recall: 0.135906040268\n",
      "number of selected items \"babelnet results lenght\": 4685 lenght of correct answers: 1192\n"
     ]
    }
   ],
   "source": [
    "synset = ['Remote sensing', 'Infrared remote sensing', 'Passive remote sensing', 'Remote-sensing', 'Remote-Sensing Image', 'Remote Sensing Satellites', 'Remote sensor']\n",
    "topic = 'Remote sensing'\n",
    "p , r, n, t = eval_at_n('results/Remote_sensing_results.pickle', topic, synset)\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"babelnet results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 724\n",
      "precision: 0.0030518819939 recall: 0.05\n",
      "number of selected items \"babelnet results lenght\": 983 lenght of correct answers: 60\n"
     ]
    }
   ],
   "source": [
    "synset = ['international relations', 'international affairs', 'world affairs', 'International politics', 'Intrel', 'Diplomacy and Statecraft', 'Diplomatic relationship', 'Global relations', 'International relation', 'International Relation Studies', 'International trade relations', 'Interstate relations', 'Levels of Analysis in international relations', 'Study of International Relations']\n",
    "topic = 'international relations'\n",
    "p , r, n, t = eval_at_n('results/international_relations_results.pickle', topic, synset)\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"babelnet results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 810\n",
      "precision: 0.00290065264685 recall: 0.0140350877193\n",
      "number of selected items \"babelnet results lenght\": 1379 lenght of correct answers: 285\n"
     ]
    }
   ],
   "source": [
    "synset = ['Emergency medicine', 'Emergency care', 'Emergency med', 'Emergency treatment', 'Emergent condition', 'Emergentology', 'Er physician']\n",
    "topic = 'Emergency medicine'\n",
    "p , r, n, t = eval_at_n('results/Emergence_results.pickle', topic, synset)\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"babelnet results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 344\n",
      "precision: 0.0022724234728 recall: 0.309090909091\n",
      "number of selected items \"babelnet results lenght\": 7481 lenght of correct answers: 55\n"
     ]
    }
   ],
   "source": [
    "synset = ['conservation biology', 'Animal conservation', 'Biological Conservation', 'Conservation', 'Biodiversity conservation', 'Conservation biologist', 'Conservation biologists', 'Conservation of natural resources', 'Conservation of wildlife', 'Conservation priority', 'Conservation science', 'Earth biologist', 'Earth biology', 'Ecological conservation', 'History of conservation biology', 'Wildlife Conservation']\n",
    "topic = 'Biodiversity Conservation'\n",
    "p , r, n, t = eval_at_n('results/Biodiversity_Conservation_results.pickle', topic, synset)\n",
    "print 'precision:', p, 'recall:', r\n",
    "print 'number of selected items \"babelnet results lenght\":', n, 'lenght of correct answers:', t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "213\n",
      "169\n",
      "163\n",
      "137\n",
      "96\n",
      "98\n",
      "103\n",
      "103\n",
      "59\n",
      "531\n",
      "321\n",
      "220\n",
      "190\n",
      "136\n",
      "133\n",
      "130\n",
      "102\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:1000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[1000:2000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[2000:3000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[3000:4000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[4000:5000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[5000:6000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[6000:7000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[7000:8000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[8000:9000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[9000:10000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[10000:20000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[20000:30000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[30000:40000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[40000:50000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[50000:60000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[60000:70000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[70000:80000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[80000:90000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[90000:100000])[0], babelnet_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649\n",
      "272\n",
      "221\n",
      "173\n",
      "179\n",
      "182\n",
      "181\n",
      "151\n",
      "177\n",
      "1356\n",
      "852\n",
      "506\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[100000:200000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[200000:300000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[300000:400000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[400000:500000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[500000:600000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[600000:700000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[700000:800000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[800000:900000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[900000:1000000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[1000000:2000000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[2000000:3000000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[3000000:4000000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[4000000:])[0], babelnet_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "517\n",
      "686\n",
      "849\n",
      "986\n",
      "1082\n",
      "1180\n",
      "1283\n",
      "1386\n",
      "1445\n",
      "1976\n",
      "2297\n",
      "2517\n",
      "2707\n",
      "2843\n",
      "2976\n",
      "3106\n",
      "3208\n",
      "3308\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:1000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:2000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:3000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:4000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:5000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:6000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:7000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:8000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:9000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:10000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:20000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:30000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:40000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:50000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:60000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:70000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:80000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:90000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:100000])[0], babelnet_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:1000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[1000:2000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[2000:3000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[3000:4000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[4000:5000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[5000:6000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[6000:7000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[7000:8000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[8000:9000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[9000:10000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[10000:20000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[20000:30000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[30000:40000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[40000:50000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[50000:60000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[60000:70000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[70000:80000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[80000:90000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[90000:100000])[0], babel_test_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[100000:200000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[200000:300000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[300000:400000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[400000:500000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[500000:600000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[600000:700000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[700000:800000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[800000:900000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[900000:1000000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[1000000:2000000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[2000000:3000000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[3000000:4000000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[4000000:])[0], babel_test_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:1000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:2000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:3000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:4000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:5000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:6000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:7000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:8000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:9000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:10000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:20000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:30000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:40000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:50000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:60000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:70000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:80000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:90000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:100000])[0], babel_test_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0A8071F67FEE1A6BF14E5069288C9E67B8A3F2AA</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2946E8F7A320DEF1BEFBB26A21CB95E0492FE580</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4B1F119CA2211850D0FA755EB5D43D0C26A0358B</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>396205226F7DDC8B48E8FBEF16684C7703D905D1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FA9D1081F58F45F8168813C84C6D9223BBB1873E</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7E4EF8002D4FEA0DE518902EC3D79AE81B29D79A</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>277C9AC0AF7CB753DBF8609EE57C946AAEF0A172</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FAD58615F61C6D0F70C3F054B27D8D7B75AC7C34</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D037DE25D3FAE7E3D1F7EC618602687C735224C8</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71ADE7014F39A00FDE2D4530BCDD5DF7E509B179</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>E89D1B5A74E9E95A422B7FAEB5415FFABB96BB24</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>82D1903BD41762184DD13673F1C2EE81492146BA</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4823D2BB6AEC5BFC2714F37562BD4879B24D813B</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>978161F59C993E54C21EB389B9740DD2823F9B56</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A46E2E793CCE401D1904EE6304631705283723A0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>73E187A48A0D7E5A60113A5F383CF353C5EFD77B</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>E4B8155B65A0EE4AAD33F5DBEEF565F31ED172DE</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>609C84191239074E21B3B408C6E8AA2BA3FDD3E7</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C0796575BA62A06426215C74C9F15B8664ABA59C</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>941079769D5D18B62443DF6547176AF2A7E09E15</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2D7E81BAF55D1C892830A3336E81862749469B3D</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>882F423AB19B984B34432EAC394F30FDEC1D9241</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>724B816DB338A9C1EDFBD11BC2853868876E3D75</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>05A795293716CEB99D83F7E811AD0695C58E5EF3</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0EA19C5868F0280525CFF1EBC2BEA234F1115CE7</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>630BE4C3887ECF7ACBA1B8BBE1D134F2B5D703CC</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>80B73E520426E3ADE858FC94F82EDD4F44B8D892</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>FC492A54848826AD03E4BA419E4E72114C8B5706</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A47E372634664B30F15AEB6E21327276FB83C0C8</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>039D68A9EF11CA73D637AF4E337E7A6932B7CA37</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>BC2FAD48CC3BDD96A5CF6AA696A82336213F055B</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>ACA116610910FAC9F1ADE2AD98BB15590B98D9C1</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>9F6CE09AFA4422FBFDAF710AEFDE57DEBF4E55C6</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>743C338E7C4EEDA43E4454F754B54E1F2FE0BA2E</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>5BED038D76384C4174CEC2DA5317B4252FA93088</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>411776A76EDD2891CF1B16344E52E9CA0D5AECC4</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>84A89DB335AE21CDA850C8586B9E9DF3DADB3FC9</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>8606C23B15811A9F250D3FECE3A375795411B0D9</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>A915A7293C81CC4EB08FBCFCD3D0CF745EE2CA7B</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>1E07B4853FAE1077514C63BC670C6A0D368FD524</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0D23A60FB7BAA8D4036D80D6BC6D6FA068DC1A1D</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>4477463EE8BD6BE1E64CCADEFC0E3A5B80C3A176</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>4484E046DF3F1DC85F799031334D77503665C372</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>6C395372036972FEF2426C76A58E85B4CFF06D88</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>B66B7827F6ED7A519A0C29B60AAA3A217E2F187D</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>08EC4113EC9DF743EB4A4C782B99062E902FDEF8</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>62E702370DA675EBD0FEF256E8D8798D6E4325DA</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>5F5917074C6C5D499F9D3BAE5913C2D8497218CC</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>ABEA4C3756BEF631D831474E8AE3B4F285434CE9</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>F6F233A944469CB9337A9EE49B2166A7B5A11A86</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>576A8BB5A9D983A9A1F38951997573348B74A53C</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>8BF001293B2879298D858778AE49BC83E2C58A51</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>F2BA5D0BC2EB269FA3ACA2EF1C4A531E46F50BCC</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>8E6BA8026CF4A4697C8E2DD55564E614538E3D79</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>20DD2FB52AAE6A75A9E37185D323CB172F5EE1F3</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>A61B5B3D43D4069FB0A47387CB1A2474E27CBE15</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>BDC7E9A899C239C57861783999F459BB56AC652A</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>57C2F168EA01528A006460E95BA4FE3D654A5776</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>CC81B839CD5A59EB1C34CBB2342C7FB6C2BDC62C</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>51968CA1D613439E800031A09FBED20B7D3F793F</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            0      1\n",
       "0    0A8071F67FEE1A6BF14E5069288C9E67B8A3F2AA  1.000\n",
       "1    2946E8F7A320DEF1BEFBB26A21CB95E0492FE580  1.000\n",
       "2    4B1F119CA2211850D0FA755EB5D43D0C26A0358B  1.000\n",
       "3    396205226F7DDC8B48E8FBEF16684C7703D905D1  1.000\n",
       "4    FA9D1081F58F45F8168813C84C6D9223BBB1873E  1.000\n",
       "5    7E4EF8002D4FEA0DE518902EC3D79AE81B29D79A  1.000\n",
       "6    277C9AC0AF7CB753DBF8609EE57C946AAEF0A172  1.000\n",
       "7    FAD58615F61C6D0F70C3F054B27D8D7B75AC7C34  1.000\n",
       "8    D037DE25D3FAE7E3D1F7EC618602687C735224C8  1.000\n",
       "9    71ADE7014F39A00FDE2D4530BCDD5DF7E509B179  1.000\n",
       "10   E89D1B5A74E9E95A422B7FAEB5415FFABB96BB24  1.000\n",
       "11   82D1903BD41762184DD13673F1C2EE81492146BA  1.000\n",
       "12   4823D2BB6AEC5BFC2714F37562BD4879B24D813B  1.000\n",
       "13   978161F59C993E54C21EB389B9740DD2823F9B56  1.000\n",
       "14   A46E2E793CCE401D1904EE6304631705283723A0  1.000\n",
       "15   73E187A48A0D7E5A60113A5F383CF353C5EFD77B  1.000\n",
       "16   E4B8155B65A0EE4AAD33F5DBEEF565F31ED172DE  1.000\n",
       "17   609C84191239074E21B3B408C6E8AA2BA3FDD3E7  1.000\n",
       "18   C0796575BA62A06426215C74C9F15B8664ABA59C  1.000\n",
       "19   941079769D5D18B62443DF6547176AF2A7E09E15  1.000\n",
       "20   2D7E81BAF55D1C892830A3336E81862749469B3D  1.000\n",
       "21   882F423AB19B984B34432EAC394F30FDEC1D9241  1.000\n",
       "22   724B816DB338A9C1EDFBD11BC2853868876E3D75  1.000\n",
       "23   05A795293716CEB99D83F7E811AD0695C58E5EF3  1.000\n",
       "24   0EA19C5868F0280525CFF1EBC2BEA234F1115CE7  1.000\n",
       "25   630BE4C3887ECF7ACBA1B8BBE1D134F2B5D703CC  1.000\n",
       "26   80B73E520426E3ADE858FC94F82EDD4F44B8D892  1.000\n",
       "27   FC492A54848826AD03E4BA419E4E72114C8B5706  1.000\n",
       "28   A47E372634664B30F15AEB6E21327276FB83C0C8  1.000\n",
       "29   039D68A9EF11CA73D637AF4E337E7A6932B7CA37  1.000\n",
       "..                                        ...    ...\n",
       "970  BC2FAD48CC3BDD96A5CF6AA696A82336213F055B  0.986\n",
       "971  ACA116610910FAC9F1ADE2AD98BB15590B98D9C1  0.986\n",
       "972  9F6CE09AFA4422FBFDAF710AEFDE57DEBF4E55C6  0.986\n",
       "973  743C338E7C4EEDA43E4454F754B54E1F2FE0BA2E  0.986\n",
       "974  5BED038D76384C4174CEC2DA5317B4252FA93088  0.986\n",
       "975  411776A76EDD2891CF1B16344E52E9CA0D5AECC4  0.986\n",
       "976  84A89DB335AE21CDA850C8586B9E9DF3DADB3FC9  0.986\n",
       "977  8606C23B15811A9F250D3FECE3A375795411B0D9  0.986\n",
       "978  A915A7293C81CC4EB08FBCFCD3D0CF745EE2CA7B  0.986\n",
       "979  1E07B4853FAE1077514C63BC670C6A0D368FD524  0.986\n",
       "980  0D23A60FB7BAA8D4036D80D6BC6D6FA068DC1A1D  0.986\n",
       "981  4477463EE8BD6BE1E64CCADEFC0E3A5B80C3A176  0.986\n",
       "982  4484E046DF3F1DC85F799031334D77503665C372  0.986\n",
       "983  6C395372036972FEF2426C76A58E85B4CFF06D88  0.986\n",
       "984  B66B7827F6ED7A519A0C29B60AAA3A217E2F187D  0.986\n",
       "985  08EC4113EC9DF743EB4A4C782B99062E902FDEF8  0.986\n",
       "986  62E702370DA675EBD0FEF256E8D8798D6E4325DA  0.986\n",
       "987  5F5917074C6C5D499F9D3BAE5913C2D8497218CC  0.986\n",
       "988  ABEA4C3756BEF631D831474E8AE3B4F285434CE9  0.986\n",
       "989  F6F233A944469CB9337A9EE49B2166A7B5A11A86  0.986\n",
       "990  576A8BB5A9D983A9A1F38951997573348B74A53C  0.986\n",
       "991  8BF001293B2879298D858778AE49BC83E2C58A51  0.986\n",
       "992  F2BA5D0BC2EB269FA3ACA2EF1C4A531E46F50BCC  0.986\n",
       "993  8E6BA8026CF4A4697C8E2DD55564E614538E3D79  0.986\n",
       "994  20DD2FB52AAE6A75A9E37185D323CB172F5EE1F3  0.986\n",
       "995  A61B5B3D43D4069FB0A47387CB1A2474E27CBE15  0.986\n",
       "996  BDC7E9A899C239C57861783999F459BB56AC652A  0.986\n",
       "997  57C2F168EA01528A006460E95BA4FE3D654A5776  0.986\n",
       "998  CC81B839CD5A59EB1C34CBB2342C7FB6C2BDC62C  0.986\n",
       "999  51968CA1D613439E800031A09FBED20B7D3F793F  0.986\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(s3h_results.items()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_babelnet(results, top_n, test):\n",
    "\ttop = results[:top_n]\n",
    "\tprint 'matched rate at '+str(top_n)+':' , len(find_intersection(test,top))/float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BabelNet Method Evaluation:\n",
      "matched rate at 1000: 0.0181818181818\n",
      "matched rate at 2000: 0.0181818181818\n",
      "matched rate at 3000: 0.0363636363636\n",
      "matched rate at 4000: 0.0727272727273\n",
      "matched rate at 5000: 0.109090909091\n",
      "matched rate at 6000: 0.109090909091\n",
      "matched rate at 7000: 0.145454545455\n",
      "matched rate at 8000: 0.218181818182\n",
      "matched rate at 9000: 0.218181818182\n",
      "matched rate at 10000: 0.218181818182\n"
     ]
    }
   ],
   "source": [
    "print 'BabelNet Method Evaluation:'\n",
    "for i in range(1000,11000,1000):\n",
    "    eval_babelnet(babelnet_results, i, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_use_case(sorted_results, top_n, test):\n",
    "\ttop = pd.DataFrame(sorted_results.items()[:top_n])\n",
    "\tprint 'matched rate at '+str(top_n)+':' , len(find_intersection(test,top[0]))/float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "matched rate at 1000: 0.127272727273\n",
      "matched rate at 2000: 0.236363636364\n",
      "matched rate at 3000: 0.290909090909\n",
      "matched rate at 4000: 0.309090909091\n",
      "matched rate at 5000: 0.327272727273\n",
      "matched rate at 6000: 0.345454545455\n",
      "matched rate at 7000: 0.363636363636\n",
      "matched rate at 8000: 0.436363636364\n",
      "matched rate at 9000: 0.436363636364\n",
      "matched rate at 10000: 0.454545454545\n"
     ]
    }
   ],
   "source": [
    "print 'Evaluation:'\n",
    "for i in range(1000,11000,1000):\n",
    "    eval_use_case(s3h_results, i, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Hyprid:\n",
      "matched rate at 1000: 0.109090909091\n",
      "matched rate at 2000: 0.218181818182\n",
      "matched rate at 3000: 0.236363636364\n",
      "matched rate at 4000: 0.327272727273\n",
      "matched rate at 5000: 0.363636363636\n",
      "matched rate at 6000: 0.381818181818\n",
      "matched rate at 7000: 0.381818181818\n",
      "matched rate at 8000: 0.418181818182\n",
      "matched rate at 9000: 0.454545454545\n",
      "matched rate at 10000: 0.472727272727\n"
     ]
    }
   ],
   "source": [
    "print 'Evaluation Hyprid:'\n",
    "for i in range(1000,11000,1000):\n",
    "    eval_use_case(s3h_syn_results, i, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlt_results = pickle.load(open('elasticsearch/mlt_initial_corpus_results.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLT Method Evaluation:\n",
      "matched rate at 1000: 0.0\n",
      "matched rate at 2000: 0.0\n",
      "matched rate at 3000: 0.0\n",
      "matched rate at 4000: 0.0\n",
      "matched rate at 5000: 0.0\n",
      "matched rate at 6000: 0.0\n",
      "matched rate at 7000: 0.0\n",
      "matched rate at 8000: 0.0\n",
      "matched rate at 9000: 0.0\n",
      "matched rate at 10000: 0.0\n"
     ]
    }
   ],
   "source": [
    "print 'MLT Method Evaluation:'\n",
    "for i in range(1000,11000,1000):\n",
    "    eval_babelnet(mlt_results, i, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_intersection(mlt_results, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'205E7834346C3A1A1192C11E52B56AA5ACD08D00',\n",
       " u'C57EE34326E4C78F8A45F036F2D29F6F9E0A40B4',\n",
       " u'50DCDEA86FBA19D63FAC7CED48FFE844FEE3D5DA',\n",
       " u'6F1C1FC32134D9EB42F204EA54EFF076B0B84276',\n",
       " u'FF0EBD3588C293F6D2423BE2F1C6A33826919DE2']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 83\n",
      "results size of the topic \"Area Studies\": 83\n",
      "ground truth size 2\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "area = 'Area studies'\n",
    "synset = area.split(',')\n",
    "topic = 'Area Studies'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 121\n",
      "results size of the topic \"Behavioral Sciences\": 357\n",
      "ground truth size 21\n",
      "intersection with the ground truth: 4\n"
     ]
    }
   ],
   "source": [
    "bab = 'behavioural sciences, Behavioral science, Behavioral sciences, Behavioral disciplines, Behavioral scientist, Behavioral scientists, Behavioural disciplines, Behavioural science, Behavioural scientist'\n",
    "synset = bab.split(',')\n",
    "topic = 'Behavioral Sciences'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 275\n",
      "results size of the topic \"biophysics\": 3674\n",
      "ground truth size 323\n",
      "intersection with the ground truth: 19\n"
     ]
    }
   ],
   "source": [
    "bab = 'biophysics, Biological physics, Biophysical, Biophysicists, History of biophysics'\n",
    "synset = bab.split(',')\n",
    "topic = 'biophysics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2091\n",
      "results size of the topic \"artificial intelligence\": 7903\n",
      "ground truth size 657\n",
      "intersection with the ground truth: 43\n"
     ]
    }
   ],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 291\n",
      "results size of the topic \"Cybernetics\": 511\n",
      "ground truth size 596\n",
      "intersection with the ground truth: 47\n"
     ]
    }
   ],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 5033\n",
      "results size of the topic \"Information Systems\": 8440\n",
      "ground truth size 1313\n",
      "intersection with the ground truth: 298\n"
     ]
    }
   ],
   "source": [
    "bab = 'information system, data system, information systems, system info, Business computing, Computer information system, Computer information systems, Elements of Information System, Information in Computer Science, Information systems and technology, Information systems discipline, Information systems theory, Informationssystem'\n",
    "synset = bab.split(',')\n",
    "topic = 'Information Systems'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 928\n",
      "results size of the topic \"Software Engineering\": 1058\n",
      "ground truth size 241\n",
      "intersection with the ground truth: 5\n"
     ]
    }
   ],
   "source": [
    "bab = 'software engineering, Software Hut, Characteristics of software engineering, Computer Software Engineering, In-House Design, Software eng, software engineer, Software engineers, Software management, Software Specialist'\n",
    "synset = bab.split(',')\n",
    "topic = 'Software Engineering'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 1560\n",
      "results size of the topic \"Aerospace\": 1560\n",
      "ground truth size 122\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = 'aerospace, Aeronautical-Astronautical, Aerospace projects, Aerospace Revolution, Ærospace'\n",
    "synset = bab.split(',')\n",
    "topic = 'Aerospace'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 1514\n",
      "results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n"
     ]
    }
   ],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2588\n",
      "results size of the topic \"Infectious Diseases\": 8343\n",
      "ground truth size 1375\n",
      "intersection with the ground truth: 50\n"
     ]
    }
   ],
   "source": [
    "bab = 'infection, infectious disease, communicable diseases, contagion, Infectious diseases, Acute infection, AIDS-related bacterial infections, AIDS-related viral infections, Anti-infective, Anti-infectives, Antiinfective, Bacterial Infections, Communicable disease, Contagious diseases, Definition to contagious, Infect, Infecting, Infectiology, Infections, Infectious, Infectious disease epidemiology, Infectious disease medicine, Infectology, Local infection, Primary infection, Rochalimea infections, Secondary infection, Tropical bacterial infections, Tropical infections, Viral Infections, Wound colonization, Wound infection'\n",
    "synset = bab.split(',')\n",
    "topic = 'Infectious Diseases'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 108\n",
      "results size of the topic \"Limnology\": 394\n",
      "ground truth size 73\n",
      "intersection with the ground truth: 16\n"
     ]
    }
   ],
   "source": [
    "bab = 'limnology, lakelore, Freshwater science, History of limnology, Limnographer, Limnological'\n",
    "synset = bab.split(',')\n",
    "topic = 'Limnology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 1050\n",
      "results size of the topic \"Linguistics\": 6898\n",
      "ground truth size 214\n",
      "intersection with the ground truth: 61\n"
     ]
    }
   ],
   "source": [
    "bab = 'linguistics, Linguists, Language specialist, Language studies, Lingistics, Linguism, linguist, Linguistic, Linguistic layer, Linguistic layers, Linguistic science, Linguistic scientist, Linguistical, Linguistically, Linguistics and Languages, Linguistics layer, Linguistics layers, Linguistsics, Linguithtics, Lingustics, Speech scientist, Speech scientists, Statistical linguistics, Study of language, Verbal communication'\n",
    "synset = bab.split(',')\n",
    "topic = 'Linguistics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 7941\n",
      "results size of the topic \"Literature\": 7357\n",
      "ground truth size 860\n",
      "intersection with the ground truth: 107\n"
     ]
    }
   ],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2442\n",
      "results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n"
     ]
    }
   ],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8256\n",
      "results size of the topic \"Ceramics\": 8249\n",
      "ground truth size 1276\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = 'ceramics, Ceramic art, Art pottery, Art ware, Ceramic artist, Ceramic paint, Ceramics art, Fine art pot, Vase painting'\n",
    "synset = bab.split(',')\n",
    "topic = 'Ceramics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 765\n",
      "results size of the topic \"Textiles\": 7217\n",
      "ground truth size 158\n",
      "intersection with the ground truth: 97\n"
     ]
    }
   ],
   "source": [
    "bab = 'textile, cloth, fabric, material, Cloths, Fabrical, Fabricalities, Fabricality, Fabrically, Fabrics, Interlock cloth, Textile fibre, Textiled, Textiler, Textilers, Textiles, Textiling, Tray-cloth'\n",
    "synset = bab.split(',')\n",
    "topic = 'Textiles'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8210\n",
      "results size of the topic \"Mechanics\": 8222\n",
      "ground truth size 4640\n",
      "intersection with the ground truth: 1\n"
     ]
    }
   ],
   "source": [
    "bab = ' mechanics, History of mechanics, Mechanical processes, Particle mechanics, Theoretical mechanics'\n",
    "synset = bab.split(',')\n",
    "topic = 'Mechanics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 382\n",
      "results size of the topic \"Medical Ethics\": 413\n",
      "ground truth size 174\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = 'medical ethics, Health ethics, Ethics in medicine, Medical ethicist'\n",
    "synset = bab.split(',')\n",
    "topic = 'Medical Ethics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 139\n",
      "results size of the topic \"Medical Informatics\": 7437\n",
      "ground truth size 94\n",
      "intersection with the ground truth: 20\n"
     ]
    }
   ],
   "source": [
    "bab = 'Health informatics, Biomedical informatics, Clinical Informatics, Clinical Research Informatics, Gero-Informatics, Health Informatics Law, Medical informatics, Nursing informatics, Clinical informatist, Consumer Health Informatics, Dental informatics, Geroinformatics, Health care informatics, Health information, Health information system, Health information systems, Health management information system, Healthcare, Information Technology, Healthcare informatics, Healthcare information system, Healthcare Information Technology, Heath informatics, Medical computer science, Pharmacy informatics, Veterinary informatics'\n",
    "synset = bab.split(',')\n",
    "topic = 'Medical Informatics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 1603\n",
      "results size of the topic \"Microbiology\": 5906\n",
      "ground truth size 1320\n",
      "intersection with the ground truth: 147\n"
     ]
    }
   ],
   "source": [
    "bab = 'microbiology, Rapid microbiology, History of microbiology, Microbiological, Microbiological techniques'\n",
    "synset = bab.split(',')\n",
    "topic = 'Microbiology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8418\n",
      "results size of the topic \"Microscopy\": 8547\n",
      "ground truth size 6819\n",
      "intersection with the ground truth: 253\n"
     ]
    }
   ],
   "source": [
    "bab = 'microscopy, 3D-SIM-microscopy, Amateur microscopy, Bioimaging, Infrared microscopy, IR microscopy, Laser microscopy, Light microscopy, Microscopic examination, Microscopically, Microscopist, Oblique illumination, Polarized light microscope'\n",
    "synset = bab.split(',')\n",
    "topic = 'Microscopy'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2407\n",
      "results size of the topic \"Mineralogy\": 4324\n",
      "ground truth size 196\n",
      "intersection with the ground truth: 48\n"
     ]
    }
   ],
   "source": [
    "bab = 'mineralogy, Mineralogical, Mineralology, Minerology, Quantum mineralogy'\n",
    "synset = bab.split(',')\n",
    "topic = 'Mineralogy'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2449\n",
      "results size of the topic \"Music\": 4324\n",
      "ground truth size 150\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = \"music, music lover, Musical interpretation, 1st Art, Auditory art, Chord, Chromatic, chromatic scale, Chromaticism, composer, consonance, Consonance and dissonance, Dissonance, Flashygoodness, Interpretation, Meaning, Mucic, Music's, Music And Musical Instruments, Musica instrumentalis, Musical, musician, Musicians, Musicologist, Muzic, pianist, Romantic music, Singer-songwriter, songwriter, symphony\"\n",
    "topic = 'Music'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 179\n",
      "results size of the topic \"Mycology\": 542\n",
      "ground truth size 530\n",
      "intersection with the ground truth: 3\n"
     ]
    }
   ],
   "source": [
    "bab = 'mycology, fungology, History of mycology, Micology, Mycological, Mycologists, Study of fungi'\n",
    "synset = bab.split(',')\n",
    "topic = 'Mycology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2497\n",
      "results size of the topic \"Neuroimaging\": 3679\n",
      "ground truth size 522\n",
      "intersection with the ground truth: 52\n"
     ]
    }
   ],
   "source": [
    "bab = 'neuroimaging, Brain imaging, Brain scanning, Brain function map, Brain scan, Brain scans, Functional neurological mapping, Neuro-imaging, Neuroradiography'\n",
    "synset = bab.split(',')\n",
    "topic = 'Neuroimaging'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 202\n",
      "results size of the topic \"Neurosciences\": 4260\n",
      "ground truth size 95\n",
      "intersection with the ground truth: 3\n"
     ]
    }
   ],
   "source": [
    "bab = 'neuroscience, Neuroscience studies, History of behavioral neuroscience, Interface of neuroscience with all liberal arts disciplines, Neural science, Neurobiological, Neuroscience and popular culture, Neuroscience and society, Neuroscience and the media, Neuroscience education: undergraduate models, best practices, Neurosciences, Neuroscientific, neuroscientist, Neuroscince'\n",
    "synset = bab.split(',')\n",
    "topic = 'Neurosciences'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8160\n",
      "results size of the topic \"Nursing\": 8252\n",
      "ground truth size 3282\n",
      "intersection with the ground truth: 276\n"
     ]
    }
   ],
   "source": [
    "bab = 'nursing, Nursing Science, Staff nurse, Adult nursing, Flightnurse, nurse, Nursing History, Nursing Officer, Nursing practice, Nursing skills, Nursing staff, Nursing Student, Nursing unit, Nurxing, Practice of nursing'\n",
    "synset = bab.split(',')\n",
    "topic = 'Nursing'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 523\n",
      "results size of the topic \"Oceanography\": 3231\n",
      "ground truth size 210\n",
      "intersection with the ground truth: 52\n"
     ]
    }
   ],
   "source": [
    "bab = 'oceanography, oceanology, Marine science, Marine Sciences, History of oceanography, Maps of the Oceans, Marine research, Marine scientist, Ocean geography, Ocean science, Ocean sciences, Oceanographic, Oceanologia, Oceanologist, Oceans and Oceanography, Undersea Exploration, Underwater exploration'\n",
    "synset = bab.split(',')\n",
    "topic = 'Oceanography'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 3922\n",
      "results size of the topic \"Oncology\": 5705\n",
      "ground truth size 2937\n",
      "intersection with the ground truth: 107\n"
     ]
    }
   ],
   "source": [
    "bab = 'oncology, oncologist, AllergoOncology, Cancer care, Medical oncology, Oncologic, Oncological, Oncologists, Oncology research, Oncology unit, Pediatric cancers'\n",
    "synset = bab.split(',')\n",
    "topic = 'Oncology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 958\n",
      "results size of the topic \"Ophthalmology\": 2096\n",
      "ground truth size 459\n",
      "intersection with the ground truth: 25\n"
     ]
    }
   ],
   "source": [
    "bab = 'ophthalmology, All India Ophthalmological Conference, Clincial ophthalmology, Clincial opthalmology, Clinical ophtalmology, Clinical Ophthalmology, Clinical Opthalmology, General ophthalmic services, Oculists, Oftamology, Ofthamology, Ophthalmic surgeon, Ophthalmologic, Ophthalmological, Ophthalmologicals, Ophthalmologists, Ophthamologist, Ophthamology, Optamology, Opthalmological, Opthalmologist, Opthalmology, Opthamologist, Opthamology, Opthomologist, Society for clincial ophthalmology, Society for clincial opthalmology, Society for clinical ophthalmology, Society for clinical opthalmology, Vision care'\n",
    "synset = bab.split(',')\n",
    "topic = 'Ophthalmology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 5349\n",
      "results size of the topic \"Optics\": 8730\n",
      "ground truth size 3286\n",
      "intersection with the ground truth: 271\n"
     ]
    }
   ],
   "source": [
    "bab = 'optics, Classical optics, Light physics, Optical, Optical system'\n",
    "synset = bab.split(',')\n",
    "topic = 'Optics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 28\n",
      "results size of the topic \"Ornithology\": 69\n",
      "ground truth size 6\n",
      "intersection with the ground truth: 3\n"
     ]
    }
   ],
   "source": [
    "bab = 'ornithology, birdlore, Bird watching towers, History of ornithology, Ornathology, Ornithologic, Ornithological, Ornithologically, Ornothology, Orthinology, Science of birds, Study of Birds'\n",
    "synset = bab.split(',')\n",
    "topic = 'Ornithology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 178\n",
      "results size of the topic \"Orthopedics\": 5949\n",
      "ground truth size 51\n",
      "intersection with the ground truth: 19\n"
     ]
    }
   ],
   "source": [
    "bab = 'orthopedics, orthopaedics, Orthopedic surgery, Orthopaedic surgery, Orthopedic surgeon, Bone doctor, Bone surgeon, Bone surgery, Bonemender, Orthepedic surgeon, Orthopaedic, Orthopaedic procedure, Orthopaedic surgeon, Orthopaedists, Orthopaedy, Orthopedic, Orthopedic medicine, Orthopedic procedure, Orthopedic procedures, Orthopedic surgeons, Orthopedy, Orthopod, Orthopods, Orthopædic, Orthopædics'\n",
    "synset = bab.split(',')\n",
    "topic = 'Orthopedics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 59\n",
      "results size of the topic \"Otorhinolaryngology\": 8382\n",
      "ground truth size 6\n",
      "intersection with the ground truth: 3\n"
     ]
    }
   ],
   "source": [
    "bab = 'otorhinolaryngology, otolaryngology, rhinolaryngology, ear, nose and throat, ENT doctor, Aural surgeon, E.N.T., Ear, nose, and throat, Ear, nose, and throat specialist, Ear, nose and throat specialist, Ear nose and throat, Ear surgeon, Ear surgery, Ears, nose, mouth, and throat, Ears, Nose and Throat, ENT medicine, ENT specialist, ENT surgery, ENTs, Oto-rhino-laryngology, Oto rhino laryngology, Otolaryngologists, Otolaryngology- head and neck surgery, Otolaryngology - Head  Neck Surgery, Otolaryngology - head and neck surgery, Otolarynology, Otoretolaryngologyst, Otorhinolaryngologic, Otorhinolaryngologic diseases, Otorhinolaryngologic neoplasms, Otorhinolaryngologic surgical procedures, Otorhinolaryngological, Otorhinolaryngologists, Otorhinolaryngology, head and neck surgery, Otorhynolaryngology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Otorhinolaryngology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 99\n",
      "results size of the topic \"Paleontology\": 824\n",
      "ground truth size 63\n",
      "intersection with the ground truth: 8\n"
     ]
    }
   ],
   "source": [
    "bab = 'paleontology, fossilology, palaeontology, paleontologist, Dinosaur palaeontology, Doctor of Paleontology, Palaentologist, Palaentology, Palaeontological, Palaeontologies, Palaeontologist, Palaeontologists, Palaeoology, Palaeoooelogy, Palaeooology, Palaeooölogy, Paleantologist, Paleantology, Palentoligsts, Palentologists, Palentology, Paleonthologist, Paleontological, Paleontologists, Paleoology, Palæontologist, Palæontology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Paleontology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 199\n",
      "results size of the topic \"Parasitology\": 326\n",
      "ground truth size 365\n",
      "intersection with the ground truth: 1\n"
     ]
    }
   ],
   "source": [
    "# left becasue of small testset\n",
    "bab = 'parasitology, parasitologist, Parasite Ecology, History of parasitology, Medical parasitology, Parasitologically'\n",
    "synset = bab.split(',')\n",
    "topic = 'Parasitology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8384\n",
      "results size of the topic \"Pathology\": 8544\n",
      "ground truth size 2726\n",
      "intersection with the ground truth: 174\n"
     ]
    }
   ],
   "source": [
    "bab = 'pathology, Pathology as a medical specialty, General pathology, Autopsy Surgeon, Pathoanatomy, Pathobiology, Pathologic processes, Pathological, Pathological case, Pathologically, Pathologies, Pathologism, Pathologisms, Pathology as a science, Study of disease'\n",
    "synset = bab.split(',')\n",
    "topic = 'Pathology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 565\n",
      "results size of the topic \"Pediatrics\": 8233\n",
      "ground truth size 747\n",
      "intersection with the ground truth: 223\n"
     ]
    }
   ],
   "source": [
    "bab = 'pediatrics, paediatrics, pediatric medicine, pedology, pediatrician, paediatrician, pædiatrics, Paediatric, Paediatricians, Pediatric, Pediatricians, Pediatrist, Pediatry, Pædiatric, Pædiatrician'\n",
    "synset = bab.split(',')\n",
    "topic = 'Pediatrics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 863\n",
      "results size of the topic \"Peripheral Vascular Disease\": 1540\n",
      "ground truth size 164\n",
      "intersection with the ground truth: 23\n"
     ]
    }
   ],
   "source": [
    "bab = 'Peripheral vascular disease, Arterial occlusive disease, Fontaine classification, Fontaine stage, PAOD, Peripheral arterial disease, Peripheral arterial occlusive disease, Peripheral artery disease, Peripheral artery occlusive disease, Peripheral vascular diseases'\n",
    "synset = bab.split(',')\n",
    "topic = 'Peripheral Vascular Disease'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6692\n",
      "results size of the topic \"Philosophy\": 7116\n",
      "ground truth size 677\n",
      "intersection with the ground truth: 110\n"
     ]
    }
   ],
   "source": [
    "bab = 'philosophy, Philosophic, Philosophical, Philosophical Subdisciplines, Philosophically, Philosophized, Applied philosophy, Branch of philosophy, Branches of philosophy, DefinitionOfPhilosophy, Definitions of philosophy, Filosofy, Philisophical, Philisophy, Philo-sophy, Philosophae, philosopher, Philosophers, Philosophhy, Philosophiae, PhilosophicalSubdisciplines, Philosophicians, Philosophies, Philosophise, Philosophised, Philosophiser, Philosophisers, Philosophises, Philosophising, Philosophize, Philosophizer, Philosophizers, Philosophizes, Philosophizing, PhilosophyAndLogic, Philosphical, Philosphy, Phylosophy, Roman ideals, Sage'\n",
    "synset = bab.split(',')\n",
    "topic = 'Philosophy'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 581\n",
      "results size of the topic \"Condensed Matter\": 1523\n",
      "ground truth size 1514\n",
      "intersection with the ground truth: 1\n"
     ]
    }
   ],
   "source": [
    "bab = 'Condensed matter physics, Condensed-matter physics, Condensed matter, Bulk matter, Condenced matter, Condensed matter physicist, Condensed matter system, Condensed matter theory, Condensed phase, History of condensed matter physics, Physics of condensed matter, Theoretical condensed matter physics'\n",
    "synset = bab.split(',')\n",
    "topic = 'Condensed Matter'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 581\n",
      "babelnet results size of the topic \"Condensed Matter\": 1523\n",
      "ground truth size 1514\n",
      "intersection with the ground truth: 1\n",
      "babel precision:  0.000656598818122\n",
      "babel recall:  0.000660501981506\n",
      "initial_corpus size: 581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00881057268722467, 0.05812417437252312, 9988, 1514, 0.75)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'Condensed matter physics, Condensed-matter physics, Condensed matter, Bulk matter, Condenced matter, Condensed matter physicist, Condensed matter system, Condensed matter theory, Condensed phase, History of condensed matter physics, Physics of condensed matter, Theoretical condensed matter physics'\n",
    "synset = bab.split(',')\n",
    "topic = 'Condensed Matter'\n",
    "res_pickle = \"results/res__Condensed_matter\"\n",
    "eval_all_at_thresh_lst(res_pickle, topic, synset, thresh=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8525\n",
      "results size of the topic \"Physiology\": 8494\n",
      "ground truth size 2761\n",
      "intersection with the ground truth: 123\n"
     ]
    }
   ],
   "source": [
    "bab = 'physiology, animal physiology, History of physiology, Institutes of Medicine, Phisiology, Physiologic, Physiological, Physiologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Physiology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2145\n",
      "results size of the topic \"Psychiatry\": 7448\n",
      "ground truth size 1700\n",
      "intersection with the ground truth: 250\n"
     ]
    }
   ],
   "source": [
    "bab = 'psychiatry, psychopathology, psychological medicine, Adult Psychiatry, Criticism of psychiatry, Ethics of psychiatry, History of psychiatry, Mental Pathology, Modern psychiatry, Personalistic disease theories, Psichiatry, Psychaitry, Psychiatric, Psychiatric medicine, Psychiatric syndrome, Psychiatric treatment, psychiatrist, Psychiatrists, Psychological pathology, Psychopathological, Psychopathologist, Psycopathological, Psycopathology, Pyschiatric care, Pyschiatry'\n",
    "synset = bab.split(',')\n",
    "topic = 'Psychiatry'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6857\n",
      "results size of the topic \"Psychology\": 7187\n",
      "ground truth size 1871\n",
      "intersection with the ground truth: 134\n"
     ]
    }
   ],
   "source": [
    "bab = 'psychology, psychological science, Human psychology, Psychological, Psychologically, Criticism of psychology, Human trait, Hyde event, Phsycology, Physcology, Professional psychology, Psychogenics, Psychologic, Psychological sciences, Psychological terms, Psychological theories, Psychological theory, psychologist, Psychologists, Psychology/rewrite, Psycologic, Psycological, Psycology, Pyhscology, Self-report study, WEIRD'\n",
    "synset = bab.split(',')\n",
    "topic = 'Psychology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 733\n",
      "results size of the topic \"Psychoanalysis\": 7176\n",
      "ground truth size 129\n",
      "intersection with the ground truth: 59\n"
     ]
    }
   ],
   "source": [
    "bab = 'psychoanalysis, depth psychology, analysis, Psychoanalyst, Psychoanalysts, Analysand, Criticism of Freud, Criticism of Sigmund Freud, Depth psychologist, Depth therapy, Freudian analysis, Freudian analyst, Freudian psychology, History of psychoanalysis, Jungian interpreretation of religion, Psychanalysis, Psycho-analysis, Psycho Analysis, Psychoanalitic theory, Psychoanalysis/Archives/2003-2005, Psychoanalytic, Psychoanalytic perspective, Psychoanalytic Psychiatry, Psychoanalytic school, Psychoanalytic therapy, PSYCHOANALYTIC TREATMENT, Psychoanalytical, Psychoanalytical feminism, Psychoanalytical theory, Psychoanalyze, Psychoanaylsis, Psychoannalysis, Psychoanylists, Psychological analysis, Psyco-analysis, Psycoanalysis'\n",
    "synset = bab.split(',')\n",
    "topic = 'Psychoanalysis'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 7458\n",
      "results size of the topic \"Rehabilitation\": 7449\n",
      "ground truth size 773\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = 'rehabilitation, Neurocognitive Rehabilitation, Neurological rehabilitation, Neuropsychological rehabilitation, Rehabilitation Neuropsychology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Rehabilitation'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 4073\n",
      "results size of the topic \"Religion\": 7116\n",
      "ground truth size 587\n",
      "intersection with the ground truth: 84\n"
     ]
    }
   ],
   "source": [
    "bab = 'aith, religion, religious belief, belief, creed, Faiths, Religions, Religious beliefs, Allegory of faith, Co-religionism, Co-religionist, Co-religionists, Coreligionism, Coreligionist, Coreligionists, Creating Stories, Dereligionization, Faithful, Faithfully, Fictitous, Magical thinking/Revised, Magickal thinking, Relegious, Relgion, Relig, Relig., Religionistic, Religionistical, Religionistically, Religionists, Religious concepts, Religious faith, Religious issues, Religious tradition, Religious traditions, Religiousity, Religon, Relligion, Totalitarian religious group'\n",
    "synset = bab.split(',')\n",
    "topic = 'Religion'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 837\n",
      "results size of the topic \"Rheumatology\": 2780\n",
      "ground truth size 180\n",
      "intersection with the ground truth: 27\n"
     ]
    }
   ],
   "source": [
    "bab = 'rheumatology, Rheumatic, Rheumatologic, Rheumatologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Rheumatology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 1231\n",
      "results size of the topic \"Robotics\": 3705\n",
      "ground truth size 747\n",
      "intersection with the ground truth: 242\n"
     ]
    }
   ],
   "source": [
    "bab = 'robotics, Autonomous Systems, Flying robots, Robotic, Robotic leg, Robotic legs, Socionics, Under water robotics, Unmanned systems'\n",
    "synset = bab.split(',')\n",
    "topic = 'Robotics'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2356\n",
      "results size of the topic \"Sociology\": 4718\n",
      "ground truth size 698\n",
      "intersection with the ground truth: 53\n"
     ]
    }
   ],
   "source": [
    "bab = 'sociology, Sociological, Sociologists, Marketing sociologist, Marketing sociology, Scientific sociology, Social physics, Socialogy, Sociol, Sociological inquiry, Sociological term, Sociological terms, sociologist, Sociology versus social theory, Sociology vs. Social Theory, Sosiology, Study of culture'\n",
    "synset = bab.split(',')\n",
    "topic = 'Sociology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 136\n",
      "results size of the topic \"soil science\": 145\n",
      "ground truth size 10\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = 'soil science, Soil sciences, Soil scientist'\n",
    "synset = bab.split(',')\n",
    "topic = 'soil science'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8607\n",
      "results size of the topic \"Spectroscopy\": 8513\n",
      "ground truth size 7294\n",
      "intersection with the ground truth: 287\n"
     ]
    }
   ],
   "source": [
    "bab = 'spectroscopy, spectrographic analysis, spectrometry, spectroscopic analysis, spectrum analysis, Fingerprint region, Laser spectroscopy, Optical spectroscopy, Atomic line, Atomic line spectra, Atomic spectra, Atomic spectral line, Electromagnetic spectroscopy, Emission spectrochemical analysis, Spectrochemical Analysis, Spectrography, Spectrology, Spectroscopic, Spectroscopist, Spectroscopists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Spectroscopy'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8455\n",
      "results size of the topic \"Surgery\": 8271\n",
      "ground truth size 6412\n",
      "intersection with the ground truth: 565\n"
     ]
    }
   ],
   "source": [
    "bab = ' surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2755\n",
      "results size of the topic \"Telecommunications\": 5693\n",
      "ground truth size 259\n",
      "intersection with the ground truth: 69\n"
     ]
    }
   ],
   "source": [
    "bab = 'telecommunication, telecom, telecommunications, Communication systems, Communications in the Caribbean, Datacommunications, Electronic and Communication, Electronic communication, Electronic communications, Electronics and Communication, Electronics and Communications, General Telecommunications, Global telecommunications infrastructure, Mobile access, Telecomm, Telecomms, Telecommunication industry, Telecommunication Systems, Telecommunication technology, Telecommunications in the Caribbean, Telecommunications industry, Telecommunications systems, Telecommunications Technology, Telecommunnication, Telecoms, Telecomunication, Telecomunications, Telecomunnication, Transportation of information, Web communication'\n",
    "synset = bab.split(',')\n",
    "topic = 'Telecommunications'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 321\n",
      "results size of the topic \"Theater\": 1312\n",
      "ground truth size 12\n",
      "intersection with the ground truth: 2\n"
     ]
    }
   ],
   "source": [
    "bab = \"dramaturgy, theater, theatre, dramatic art, dramatics, Children's theater, Theater art, Theater Arts, Theatre art, Theatre arts, A Theater, Children's theatre, Dramaturgic, Entertainment venue, Live theater, Live theatre, Stage productions, Theaters, Theatre companies, Theatre company, Theatre Studies, Theatrer, Theatres, Theatrical company, Theatrical scene, Theatrics\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Theater'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6351\n",
      "results size of the topic \"Thermodynamics\": 8375\n",
      "ground truth size 2727\n",
      "intersection with the ground truth: 392\n"
     ]
    }
   ],
   "source": [
    "bab = 'thermodynamics, Thermo-dynamics, Thermodynamic functions, Applied thermodynamics, Classical thermodynamics, entropy, Macroscopic thermodynamics, Phenomenological thermodynamics, second law of thermodynamics, Termodynamics, Thermal behavior, Thermics, Thermodymanics, Thermodynamic, Thermodynamic function, Thermodynamic law, Thermodynamic Laws'\n",
    "synset = bab.split(',')\n",
    "topic = 'Thermodynamics'\n",
    "babelnet_eval(topic, synset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6351\n",
      "babelnet results size of the topic \"Thermodynamics\": 8375\n",
      "ground truth size 2727\n",
      "intersection with the ground truth: 392\n",
      "babel precision:  0.0468059701493\n",
      "babel recall:  0.143747708104\n",
      "initial_corpus size: 6351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 8547, 2727, 0.75)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'thermodynamics, Thermo-dynamics, Thermodynamic functions, Applied thermodynamics, Classical thermodynamics, entropy, Macroscopic thermodynamics, Phenomenological thermodynamics, second law of thermodynamics, Termodynamics, Thermal behavior, Thermics, Thermodymanics, Thermodynamic, Thermodynamic function, Thermodynamic law, Thermodynamic Laws'\n",
    "synset = bab.split(',')\n",
    "topic = 'Thermodynamics'\n",
    "res_pickleckleickle = \"results/res__Thermodynamics\"\n",
    "eval_all_at_thresh_lst(res_pickle, topic, synset, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2550\n",
      "results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n"
     ]
    }
   ],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2550\n",
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n",
      "initial_corpus size: 2550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.014391014391014392, 0.07925257731958762, 8547, 1552, 0.75)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "res_pickleckleickle = \"results/res__Toxicology\"\n",
    "eval_all_at_thresh_lst(res_pickle, topic, synset, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8999\n",
      "results size of the topic \"Transplantation\": 8975\n",
      "ground truth size 4997\n",
      "intersection with the ground truth: 910\n"
     ]
    }
   ],
   "source": [
    "bab = 'organ transplant, transplant, transplantation, Organ transplantation, Medical Transplantation, Transplant Surgery, Black market organs, First transplant, First transplantation, Intestinal transplant, Live organ transplants, Mixed chimerism, Organ and Tissue Donor, Organ doner, Organ farming, Organ transplantation in different countries, Organ transplantation therapy, Organ transplants, Skin transplant, Tissue transplant, Transplant Tourism and Organ Trafficking, Transplantation medicine, Transplantation surgery, Transplantation therapy, Transplanted organs, Transplantology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Transplantation'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8999\n",
      "babelnet results size of the topic \"Transplantation\": 8975\n",
      "ground truth size 4997\n",
      "intersection with the ground truth: 910\n",
      "babel precision:  0.10139275766\n",
      "babel recall:  0.182109265559\n",
      "initial_corpus size: 8999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0848931288491728, 0.14068441064638784, 8281, 4997, 0.9)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'organ transplant, transplant, transplantation, Organ transplantation, Medical Transplantation, Transplant Surgery, Black market organs, First transplant, First transplantation, Intestinal transplant, Live organ transplants, Mixed chimerism, Organ and Tissue Donor, Organ doner, Organ farming, Organ transplantation in different countries, Organ transplantation therapy, Organ transplants, Skin transplant, Tissue transplant, Transplant Tourism and Organ Trafficking, Transplantation medicine, Transplantation surgery, Transplantation therapy, Transplanted organs, Transplantology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Transplantation'\n",
    "res_pickle = \"results/res_Transplantation\"\n",
    "eval_all_at_thresh_lst(res_pickle, topic, synset, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6180\n",
      "results size of the topic \"Transportation\": 8297\n",
      "ground truth size 359\n",
      "intersection with the ground truth: 15\n"
     ]
    }
   ],
   "source": [
    "# left becasue of small testset\n",
    "bab = 'transit, transportation, public transport, transportation system, mass transit, Public transportation, Commuter transport, Government transportation, Mass-transit, Mass-transit system, Mass transit system, Mass transit systems, Mass transport, Mass transport system, Mass transportation, Mass transportation system, Pubic transportation, Public conveyance, Public mass transportation, public Transit, Public transit system, Public transport industry, Public transport service numbering, Public transport system, Public transportation system, Public utility vehicle, Public utility vehicles, Transit rider, Transit system, Transit Systems Management, Transit vehicle, Urban transit'\n",
    "synset = bab.split(',')\n",
    "topic = 'Transportation'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 66\n",
      "results size of the topic \"Tropical Medicine\": 78\n",
      "ground truth size 94\n",
      "intersection with the ground truth: 1\n"
     ]
    }
   ],
   "source": [
    "bab = 'tropical medicine, International medicine, Tropical Disease Research, Tropical pathology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Tropical Medicine'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 55\n",
      "results size of the topic \"Urban Studies\": 55\n",
      "ground truth size 10\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = 'Index of urban studies articles, Urban studies'\n",
    "synset = bab.split(',')\n",
    "topic = 'Urban Studies'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6\n",
      "results size of the topic \"Veterinary Sciences\": 8334\n",
      "ground truth size 0\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = 'veterinary medicine, Animal Health and Welfare, Veterinary science, Veterinary Sciences, Anatomy, veterinary, Animal diseases, Animal health, Animal health science, Animal Health Sciences, Animal hospital, Disease in animals, History of veterinary medicine, Pet hospice, Veterinary anatomy, Veterinary clinic, Veterinary hospice, Veterinary hospital, Veterinary scientist, Veterinary studies'\n",
    "synset = bab.split(',')\n",
    "topic = 'Veterinary Sciences'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 456\n",
      "results size of the topic \"Virology\": 485\n",
      "ground truth size 231\n",
      "intersection with the ground truth: 1\n"
     ]
    }
   ],
   "source": [
    "bab = 'virology, virologist, Viriology, Virologists, Virulogy'\n",
    "synset = bab.split(',')\n",
    "topic = 'Virology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 1951\n",
      "results size of the topic \"Water Resources\": 8128\n",
      "ground truth size 251\n",
      "intersection with the ground truth: 53\n"
     ]
    }
   ],
   "source": [
    "bab = 'assamesse, Water resources, Demand for water, Fresh water resources, Groundwater resources, Industrial water, Industrial water supply, Water, Water resource'\n",
    "synset = bab.split(',')\n",
    "topic = 'Water Resources'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 65\n",
      "results size of the topic \"Women's Studies\": 108\n",
      "ground truth size 4\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = \"women's studies, feminist studies, Women studies, Women’s Studies, Criticisms of women's studies, Female studies, Weemyns studies, Wimmins studies, Woman's Studies, Woman studies, Womans studies, Womens' studies, Womens studies, Womins studies, Womyns studies\"\n",
    "synset = bab.split(',')\n",
    "topic = \"Women's Studies\"\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 123\n",
      "results size of the topic \"Zoology\": 877\n",
      "ground truth size 10\n",
      "intersection with the ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "bab = 'zoology, zoological science, Animalogy, Branches of zoological study, Zooelogy, Zoography, Zoological, Zoologies, Zooology, Zoölogyi'\n",
    "synset = bab.split(',')\n",
    "topic = 'Zoology'\n",
    "babelnet_eval(topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2246\n",
      "results size of the topic \"Social Work\": 5153\n",
      "ground truth size 134\n",
      "intersection with the ground truth: 81\n"
     ]
    }
   ],
   "source": [
    "bab = 'social service, welfare work, Social work, Social care, Social workers, Care service, Careworker, Social affairs, Social care service, Social casework, Social Director, Social service program, Social service worker, Social Work Profession, social Worker, SocialWork, SOWK, Sozialarbeit'\n",
    "synset = bab.split(',')\n",
    "topic = 'Social Work'\n",
    "babelnet_eval(topic, synset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

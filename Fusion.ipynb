{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, pickle, argparse, json, os, urllib2\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "########################################################################################\n",
    "def query_from(q, f):\n",
    "    q = q+'&from='+str(f)\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    subject_ids = np.array(range(len(data['hits'])), dtype=np.object)\n",
    "    for (i, hit) in enumerate(data['hits']):\n",
    "        subject_ids[i] = hit['id']\n",
    "    return subject_ids\n",
    "\n",
    "def query(q):\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    nb_requests = 1 + data['total'] / 1000\n",
    "    if nb_requests > 10: # maximum number of pages due to API pagination restrection\n",
    "        nb_requests = 10\n",
    "    subject_ids = query_from(q, 0)\n",
    "    for i in range(nb_requests)[1:]:\n",
    "        f = i * 1000\n",
    "        next_request = query_from(q, f)\n",
    "        subject_ids = np.hstack((subject_ids, next_request))\n",
    "    return subject_ids.tolist()\n",
    "\n",
    "def find_intersection(list_a, list_b):\n",
    "    return list(set(list_a) & set(list_b))\n",
    "\n",
    "def term2url(string):\n",
    "    string = string.split(' ')\n",
    "    res = '%22'\n",
    "    for s in string:\n",
    "        res = res + s + '%20'\n",
    "    res = res[:-3]\n",
    "    res = res + '%22'\n",
    "    return res\n",
    "\n",
    "def babel_synset(synset):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    for syn in synset:\n",
    "        syn = term2url(syn)\n",
    "        q = q + 'title:' + syn + '%20OR%20abstract:' + syn + '%20OR%20'\n",
    "    q = q[:-8]\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    "\n",
    "def babel_subj_keyword(topic):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    topic = term2url(topic)\n",
    "    q = q+ 'subject.value:' + topic + '%20OR%20keywords:' + topic\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    " \n",
    "def babel_title_abst(topic):\n",
    "    q = 'https://api.istex.fr/document/?q=(('\n",
    "    topic = term2url(topic)\n",
    "    q = q+ 'title:' + topic + '%20OR%20abstract:' + topic\n",
    "    q = q + ')%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id'\n",
    "    return q\n",
    " \n",
    "def babelnet_syn_get_input(topic, synset):\n",
    "    results = query(babel_synset(synset))\n",
    "    _gs = query(babel_subj_keyword(topic))\n",
    "    results = find_intersection(results, inversed_index.keys())\n",
    "    _abst_title = query(babel_title_abst(topic))\n",
    "    test_set = _inter = {x for x in _gs if x not in _abst_title}\n",
    "    test_set = find_intersection(test_set, inversed_index.keys())\n",
    "    results = list(results)\n",
    "    test = list(test_set)\n",
    "    print 'initial_corpus size:', len(find_intersection(_abst_title, inversed_index.keys()))\n",
    "    return results, test\n",
    "\n",
    "def babelnet_eval(topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    print 'results size of the topic \"' + topic + '\":', len(babelnet_results) \n",
    "    print 'ground truth size', len(test)\n",
    "    babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "    babel_test_intersection_size = len(babel_test_intersection)\n",
    "    print 'intersection with the ground truth:', babel_test_intersection_size\n",
    "    \n",
    "def top_thresh(ordered_dict_pickle, thresh):\n",
    "    ranked_all = pickle.load(open(ordered_dict_pickle, 'rb'))\n",
    "    ranked_all_np = np.array(ranked_all.items())\n",
    "    ranked_all_df = pd.DataFrame(data=ranked_all_np, index=None, columns=['istex_id', 'score'])\n",
    "    ranked_all_df['score'] = ranked_all_df[['score']].astype(float)\n",
    "    return ranked_all_df[ranked_all_df['score'] > thresh]\n",
    "\n",
    "def top_thresh_lst(res_lst_pickle, thresh):\n",
    "    ranked_all = pickle.load(open(res_lst_pickle, 'rb'))\n",
    "    if type(ranked_all) is OrderedDict:\n",
    "        ranked_all = ranked_all.items()[:100000]\n",
    "    ranked_all_np = np.array(ranked_all)\n",
    "    ranked_all_df = pd.DataFrame(data=ranked_all_np, index=None, columns=['istex_id', 'score'])\n",
    "    ranked_all_df['score'] = ranked_all_df[['score']].astype(float)\n",
    "    return ranked_all_df[ranked_all_df['score'] > thresh]\n",
    "\n",
    "def babelnet_eval_PR(topic, synset):\n",
    "    babelnet_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    print 'babelnet results size of the topic \"' + topic + '\":', len(babelnet_results) \n",
    "    print 'ground truth size', len(test)\n",
    "    babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "    babel_test_intersection_size = len(babel_test_intersection)\n",
    "    print 'intersection with the ground truth:', babel_test_intersection_size\n",
    "    precision = babel_test_intersection_size / float(len(babelnet_results))\n",
    "    recall = babel_test_intersection_size / float(len(test))\n",
    "    if babel_test_intersection_size is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0.0 \n",
    "    print \"F1\", F1\n",
    "    print 'babel precision: ', precision\n",
    "    print 'babel recall: ', recall\n",
    "    \n",
    "\n",
    "#Evaluate 3SH results at treshold\n",
    "def eval_all_at_thresh(ordered_dict_pickle, topic, synset, thresh):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    if n > 10000:\n",
    "        thresh = thresh + 0.1\n",
    "        top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    elif n < 1000:\n",
    "        thresh = thresh - 0.1\n",
    "        top_res = top_thresh(ordered_dict_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t, thresh\n",
    "\n",
    "#Evaluate 3SH results list at treshold\n",
    "def eval_all_at_thresh_lst(res_pickle, topic, synset, thresh=0.75):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    _, test = babelnet_syn_get_input(topic, synset)\n",
    "    t = len(test)\n",
    "    top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    if n > 10000:\n",
    "        thresh = thresh + 0.1\n",
    "        top_res = top_thresh_lst(res_pickle, thresh)\n",
    "        if len(top_res) > 10000:\n",
    "            thresh = thresh + 0.05\n",
    "            top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    elif n < 1000:\n",
    "        thresh = thresh - 0.1\n",
    "        top_res = top_thresh_lst(res_pickle, thresh)\n",
    "        if len(top_res) > 1000:\n",
    "            thresh = thresh - 0.05\n",
    "            top_res = top_thresh_lst(res_pickle, thresh)\n",
    "    n = len(top_res)\n",
    "    res = list(top_res['istex_id'])\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    print \"length of s3h results: \", n, \"length of test set\", t\n",
    "    print \"F1: \", F1\n",
    "    print \"precision s3h: \", precision, \"recall s3h: \", recall\n",
    "\n",
    "def get_fusion_res3(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res4(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * 2 * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res5(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(range(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "def get_fusion_res6(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    if type(topic_s3h_results) is OrderedDict:\n",
    "        topic_s3h_results = topic_s3h_results.items()\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(range(100000))\n",
    "    fus = fus * 3 * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (2*i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res\n",
    "\n",
    "#Evaluate fusion df\n",
    "def eval_all(fusion_df, topic, synset, res_pickle, thresh=0.75):\n",
    "    eval_all_at_thresh_lst(res_pickle, topic, synset, thresh)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    t = len(test)\n",
    "\n",
    "    print \"length of results: \", n, \"length of test set\", t\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 2 * len(babelnet_res)\n",
    "    print \"for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "\n",
    "    n = 3 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "    \n",
    "    n = 4 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1 \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall\n",
    "    \n",
    "    n = 5 * len(babelnet_res)\n",
    "    print \" for length of results: \", n\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    matches = len(find_intersection(test,res))\n",
    "    recall = matches / float(t)\n",
    "    precision = matches / float(n)\n",
    "    if matches is not 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    print \"F1: \", F1  \n",
    "    print \"precision fusion: \", precision, \"recall fusion: \",recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original inversed_index\n",
      "[(u'ISTEX_D89FA3AC3521074D46F4245762153DF497BFFA1F', 2002320), (u'ISTEX_18EAF4D6A126B077EB38667801D1B7292F32FF49', 2483732), (u'ISTEX_5F91044435FCC4FABB9F02E31467DCFE75F4A7BE', 1429049)]\n",
      "processed inversed_index\n",
      "[(u'FCF1393F9B8136AC08FB67E88F94F3CF62C17288', 3517138), (u'482E1102A1114327A744FD2ADB4D9F8FF7E9A70B', 751643), (u'A81022B6295AE66F68A10222C3B94A06B033C1BA', 3983232)]\n"
     ]
    }
   ],
   "source": [
    "#loading SDV of istex articles\n",
    "inv_index = json.load(open('../RecSys_Exp_files/182_381_vec150_results/output_paragraph_inversed_index.json','rb'))\n",
    "print 'original inversed_index'\n",
    "print inv_index.items()[:3]\n",
    "inversed_index = dict()\n",
    "for (k, v) in inv_index.items():\n",
    "    key = k.split('_')[1]\n",
    "    inversed_index[key] = v\n",
    "print 'processed inversed_index'\n",
    "print inversed_index.items()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = (fus + len(babel_results)) / 2\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res3(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    topic_s3h_top100k_results = topic_s3h_results[:100000]\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = fus * len(babel_results)\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'organ transplant, transplant, transplantation, Organ transplantation, Medical Transplantation, Transplant Surgery, Black market organs, First transplant, First transplantation, Intestinal transplant, Live organ transplants, Mixed chimerism, Organ and Tissue Donor, Organ doner, Organ farming, Organ transplantation in different countries, Organ transplantation therapy, Organ transplants, Skin transplant, Tissue transplant, Transplant Tourism and Organ Trafficking, Transplantation medicine, Transplantation surgery, Transplantation therapy, Transplanted organs, Transplantology'\n",
    "synset = bab.split(',')\n",
    "topic = 'Transplantation'\n",
    "s3h_res_pickle = \"results/res_Transplantation\"\n",
    "Transplantation_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'spectroscopy, spectrographic analysis, spectrometry, spectroscopic analysis, spectrum analysis, Fingerprint region, Laser spectroscopy, Optical spectroscopy, Atomic line, Atomic line spectra, Atomic spectra, Atomic spectral line, Electromagnetic spectroscopy, Emission spectrochemical analysis, Spectrochemical Analysis, Spectrography, Spectrology, Spectroscopic, Spectroscopist, Spectroscopists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Spectroscopy'\n",
    "s3h_res_pickle = \"results/res__Spectroscopy\"\n",
    "Spectroscopy_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = ' surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "s3h_res_pickle = \"results/res__Surgery\"\n",
    "Surgery_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'optics, Classical optics, Light physics, Optical, Optical system'\n",
    "synset = bab.split(',')\n",
    "topic = 'Optics'\n",
    "s3h_res_pickle = \"results/res__Optics\"\n",
    "Optics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "s3h_res_pickle = \"results/res__Literature\"\n",
    "Literature_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "s3h_res_pickle = \"results/res__Toxicology\"\n",
    "Toxicology_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b4eb30782151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'artificial intelligence'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms3h_res_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results/AI_results.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mAI_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3h_res_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-2f95e6519d8b>\u001b[0m in \u001b[0;36mget_fusion_res\u001b[0;34m(s3h_res_pickle, topic, synset)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfus\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3h\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_s3h_top100k_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms3h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mfus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "s3h_res_pickle = \"results/AI_results.pickle\"\n",
    "AI_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "s3h_res_pickle = \"results/res__Cybernetics\"\n",
    "Cybernetics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'information system, data system, information systems, system info, Business computing, Computer information system, Computer information systems, Elements of Information System, Information in Computer Science, Information systems and technology, Information systems discipline, Information systems theory, Informationssystem'\n",
    "synset = bab.split(',')\n",
    "topic = 'Information Systems'\n",
    "s3h_res_pickle = \"results/infosys_results.pickle\"\n",
    "Information_Systems_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "s3h_res_pickle = \"results/res__Immunology\"\n",
    "Immunology_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'infection, infectious disease, communicable diseases, contagion, Infectious diseases, Acute infection, AIDS-related bacterial infections, AIDS-related viral infections, Anti-infective, Anti-infectives, Antiinfective, Bacterial Infections, Communicable disease, Contagious diseases, Definition to contagious, Infect, Infecting, Infectiology, Infections, Infectious, Infectious disease epidemiology, Infectious disease medicine, Infectology, Local infection, Primary infection, Rochalimea infections, Secondary infection, Tropical bacterial infections, Tropical infections, Viral Infections, Wound colonization, Wound infection'\n",
    "synset = bab.split(',')\n",
    "topic = 'Infectious Diseases'\n",
    "s3h_res_pickle = \"results/Infectious_results.pickle\"\n",
    "Infectious_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "s3h_res_pickle = \"results/res__Biomaterials\"\n",
    "Biomaterials_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'ceramics, Ceramic art, Art pottery, Art ware, Ceramic artist, Ceramic paint, Ceramics art, Fine art pot, Vase painting'\n",
    "synset = bab.split(',')\n",
    "topic = 'Ceramics'\n",
    "s3h_res_pickle = \"results/res__Ceramics\"\n",
    "Ceramics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic = \"biophysics\"\n",
    "synset_text = \"biophysics, Biological physics, Biophysical, Biophysicists, History of biophysics\"\n",
    "synset = synset_text.split(',')\n",
    "s3h_res_pickle = 'results/res__biophysics'\n",
    "biophysics_fusion_res = get_fusion_res(s3h_res_pickle, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_dict = OrderedDict((k,i) for i,k in enumerate(biophysics_fusion_res))\n",
    "babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "inter = set(ind_dict).intersection(babel_results)\n",
    "indices = [ind_dict[x] for x in inter ]\n",
    "cutt = np.array(indices).max()\n",
    "cutt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slic = len(indices) * 0.8\n",
    "indices_slic = indices[:int(slic)]\n",
    "#indices_slic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fusion_res = fusion_df.sort_values(\"fusin_rank\")[\"istex_id\"].tolist()\n",
    "#fusion_res[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df with manual cut\n",
    "def eval_fusin_at_thresh_lst_1k(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    if n < 1000:\n",
    "        n = 1000\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    return precision, recall, n, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_5k(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = 5000#len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_2_bablesize((fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = 2 * len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate fusion df\n",
    "def eval_fusin_at_bablesize(fusion_df, topic, synset):\n",
    "    babelnet_eval_PR(topic, synset)\n",
    "    babelnet_res, test = babelnet_syn_get_input(topic, synset)\n",
    "    n = len(babelnet_res)\n",
    "    t = len(test)\n",
    "    res = fusion_df[\"istex_id\"].tolist()[:n]\n",
    "    recall = len(find_intersection(test,res))/float(t)\n",
    "    precision = len(find_intersection(test,res))/float(n)\n",
    "    print \"precision fusion: \" precision, \"recall fusion: \",recall\n",
    "    print \"length of results: \", n, \"length of test set\", t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fusion_res_2(s3h_res_pickle, topic, synset):\n",
    "    topic_s3h_top100k_results = pickle.load(open(s3h_res_pickle,'rb'))\n",
    "    babel_results, test = babelnet_syn_get_input(topic, synset)\n",
    "    fus = np.array(np.ones(100000))\n",
    "    fus = (fus + 100000) / 2\n",
    "    for i, s3h in enumerate(topic_s3h_top100k_results):\n",
    "        for j, bab in enumerate(babel_results):\n",
    "            if s3h[0] == bab:\n",
    "                fus[i] = (i + j) / 2\n",
    "    fusion_df = pd.DataFrame(data=topic_s3h_top100k_results, columns=[\"istex_id\", \"3sh_score\"])\n",
    "    fusion_df[\"fus_rank\"] = fus\n",
    "    fusion_res = fusion_df.sort_values(\"fus_rank\")#[\"istex_id\"].tolist()\n",
    "    return fusion_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004853496314937983, 0.017396907216494846, 5563, 1552)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "get_fusion_res_2(\"results/res__Toxicology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Toxicology\": 5563\n",
      "ground truth size 1552\n",
      "intersection with the ground truth: 146\n",
      "babel precision:  0.0262448319252\n",
      "babel recall:  0.0940721649485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004853496314937983, 0.017396907216494846, 5563, 1552)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'toxicology, Chemical toxicology, History of toxicology, Toxicological, Toxicologists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Toxicology'\n",
    "get_fusion_res_2(\"results/res__Toxicology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Toxicology_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00356261989586188, 0.012745098039215686, 3649, 1020)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "get_fusion_res_2(\"results/res__Biomaterials\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Biomaterials_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n",
      "babelnet results size of the topic \"Biomaterials\": 3649\n",
      "ground truth size 1020\n",
      "intersection with the ground truth: 92\n",
      "babel precision:  0.0252123869553\n",
      "babel recall:  0.0901960784314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 7298, 1020)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'Biomaterial, Biomaterials Engineering, Bio material, Biomaterials'\n",
    "synset = bab.split(',')\n",
    "topic = 'Biomaterials'\n",
    "get_fusion_res_2(\"results/res__Biomaterials\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Biomaterials_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Toxicology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.010356208034596564, 0.03816313692598029, 17574, 4769)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "get_fusion_res_2(\"results/res__Immunology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst(Immunology_fusion_res, topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "0.0124 0.0130006290627 5000 4769\n"
     ]
    }
   ],
   "source": [
    "eval_fusin_at_thresh_lst(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Immunology\": 8787\n",
      "ground truth size 4769\n",
      "intersection with the ground truth: 203\n",
      "babel precision:  0.023102310231\n",
      "babel recall:  0.0425665758021\n",
      "0.0119494708091 0.0220171943804 8787 4769\n"
     ]
    }
   ],
   "source": [
    "eval_fusin_at_thresh_lst3(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bab = 'immunology, immunobiology, immunological, immunologist, Clinical immunology, Classical immunology, Evolutionary immunology, Immunologic, Immunologists, Imunologys'\n",
    "synset = bab.split(',')\n",
    "topic = 'Immunology'\n",
    "get_fusion_res_2(\"results/res__Immunology\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Immunology_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Cybernetics\": 511\n",
      "ground truth size 596\n",
      "intersection with the ground truth: 47\n",
      "babel precision:  0.0919765166341\n",
      "babel recall:  0.0788590604027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004, 0.006711409395973154, 1000, 596)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "get_fusion_res_2(\"\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Cybernetics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Cybernetics\": 511\n",
      "ground truth size 596\n",
      "intersection with the ground truth: 47\n",
      "babel precision:  0.0919765166341\n",
      "babel recall:  0.0788590604027\n",
      "0.0283757338552 0.0486577181208 1022 596\n"
     ]
    }
   ],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "Cybernetics_fusion_res = get_fusion_res_2(\"results/res__Cybernetics\", topic, synset)\n",
    "eval_fusin_at_thresh_lst2(Cybernetics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_all_at_thresh('results/AI_results.pickle', topic , synset, thresh = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Literature\": 7357\n",
      "ground truth size 860\n",
      "intersection with the ground truth: 107\n",
      "babel precision:  0.0145439717276\n",
      "babel recall:  0.124418604651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0020388745412532284, 0.01744186046511628, 7357, 860)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'literature, Literary art, Literary, Literary arts, Literary work, Literary works, LiteraryArt, Literature by region, Literatures, Litterature, Ltierature, Prose fiction'\n",
    "synset = bab.split(',')\n",
    "topic = 'Literature'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Literature_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Optics\": 8730\n",
      "ground truth size 3286\n",
      "intersection with the ground truth: 271\n",
      "babel precision:  0.0310423825888\n",
      "babel recall:  0.0824710894705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.015005727376861398, 0.039866098600121726, 8730, 3286)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'optics, Classical optics, Light physics, Optical, Optical system'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Optics'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Optics_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Surgery\": 8271\n",
      "ground truth size 6412\n",
      "intersection with the ground truth: 565\n",
      "babel precision:  0.0683109660259\n",
      "babel recall:  0.0881160324392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.027687099504292104, 0.03571428571428571, 8271, 6412)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = ' surgery, operation, surgical operation, surgical procedure, surgical process, Chirurgery, Chirurgical, Chirurgy, Complications of surgery, Corrective surgery, Elliptical excision, Emergency Surgery, Post-operation, Post-operative, Specialties in surgery, Sterile drapes, surgeon, Surgeons, Surgeries, Surgery in general practice, Surgery operation, Surgery specialties, Surgical, Surgical excision, Surgical excision of malignant lesions, Surgical specialties, Surgical technique, Surgically'\n",
    "synset = bab.split(',')\n",
    "topic = 'Surgery'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Surgery_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelnet results size of the topic \"Spectroscopy\": 8513\n",
      "ground truth size 7294\n",
      "intersection with the ground truth: 287\n",
      "babel precision:  0.0337131446024\n",
      "babel recall:  0.0393474088292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.025138024198284977, 0.029339182890046615, 8513, 7294)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bab = 'spectroscopy, spectrographic analysis, spectrometry, spectroscopic analysis, spectrum analysis, Fingerprint region, Laser spectroscopy, Optical spectroscopy, Atomic line, Atomic line spectra, Atomic spectra, Atomic spectral line, Electromagnetic spectroscopy, Emission spectrochemical analysis, Spectrochemical Analysis, Spectrography, Spectrology, Spectroscopic, Spectroscopist, Spectroscopists'\n",
    "synset = bab.split(',')\n",
    "topic = 'Spectroscopy'\n",
    "get_fusion_res_2(s3h_res_pickle, topic, synset)\n",
    "eval_fusin_at_thresh_lst(Spectroscopy_fusion_res, topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-24b74a48aa78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msynset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'artificial intelligence'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mAI_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/AI_results.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAI_fusion_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"results/AI_results.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-0703891d14d8>\u001b[0m in \u001b[0;36mget_fusion_res3\u001b[0;34m(s3h_res_pickle, topic, synset)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_fusion_res3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3h_res_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mtopic_s3h_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3h_res_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mtopic_s3h_top100k_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_s3h_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mbabel_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mfus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type"
     ]
    }
   ],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "AI_fusion_res = get_fusion_res3(\"results/AI_results.pickle\", topic, synset)\n",
    "eval_all(AI_fusion_res, topic, synset, \"results/AI_results.pickle\", tresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2091\n"
     ]
    }
   ],
   "source": [
    "AI_fusion_res = get_fusion_res3(\"results/AI_results.pickle\", topic, synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2091\n",
      "babelnet results size of the topic \"artificial intelligence\": 7903\n",
      "ground truth size 657\n",
      "intersection with the ground truth: 43\n",
      "babel precision:  0.00544097178287\n",
      "babel recall:  0.0654490106545\n",
      "initial_corpus size: 2091\n",
      "length of s3h results:  8566 length of test set 657\n",
      "precision s3h:  0.00933924819052 recall s3h:  0.121765601218\n",
      "initial_corpus size: 2091\n",
      "length of results:  7903 length of test set 657\n",
      "F1:  0.00514018691589\n",
      "precision fusion:  0.00278375300519 recall fusion:  0.0334855403349\n",
      "for length of results:  15806\n",
      "F1:  0.00413047439713\n",
      "precision fusion:  0.00215108186765 recall fusion:  0.0517503805175\n",
      " for length of results:  23709\n",
      "F1:  0.00418616104408\n",
      "precision fusion:  0.00215108186765 recall fusion:  0.0776255707763\n"
     ]
    }
   ],
   "source": [
    "eval_all(AI_fusion_res, topic, synset, \"results/AI_results.pickle\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 4642\n",
      "initial_corpus size: 4642\n",
      "babelnet results size of the topic \"Remote sensing\": 4685\n",
      "ground truth size 1192\n",
      "intersection with the ground truth: 3\n",
      "F1 0.00102092904543\n",
      "babel precision:  0.000640341515475\n",
      "babel recall:  0.00251677852349\n",
      "initial_corpus size: 4642\n",
      "length of s3h results:  7655 length of test set 1192\n",
      "F1:  0.0580987905505\n",
      "precision s3h:  0.0335728282169 recall s3h:  0.215604026846\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "[Errno 104] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-425cb036d006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Remote sensing'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mAI_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/Remote_sensing_results.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAI_fusion_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"results/Remote_sensing_results.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-75a801d7cb86>\u001b[0m in \u001b[0;36meval_all\u001b[0;34m(fusion_df, topic, synset, res_pickle, thresh)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0meval_all_at_thresh_lst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mbabelnet_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabelnet_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-75a801d7cb86>\u001b[0m in \u001b[0;36mbabelnet_syn_get_input\u001b[0;34m(topic, synset)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbabelnet_syn_get_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0m_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbabel_subj_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_intersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minversed_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-75a801d7cb86>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnb_requests\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# maximum number of pages due to API pagination restrection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnb_requests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msubject_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_requests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-75a801d7cb86>\u001b[0m in \u001b[0;36mquery_from\u001b[0;34m(q, f)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'&from='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msubject_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 447\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             return self.do_open(httplib.HTTPSConnection, req,\n\u001b[0;32m-> 1241\u001b[0;31m                 context=self._context)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# buffering kw not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     self.__class__)\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: [Errno 104] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "bab = \"remote sensing, Infrared remote sensing, Passive remote sensing, Remote-sensing, Remote-Sensing Image, Remote Sensing Satellites, Remote sensor\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Remote sensing'\n",
    "AI_fusion_res = get_fusion_res3(\"results/Remote_sensing_results.pickle\", topic, synset)\n",
    "eval_all(AI_fusion_res, topic, synset, \"results/Remote_sensing_results.pickle\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 4642\n",
      "babelnet results size of the topic \"Remote sensing\": 4685\n",
      "ground truth size 1192\n",
      "intersection with the ground truth: 3\n",
      "F1 0.00102092904543\n",
      "babel precision:  0.000640341515475\n",
      "babel recall:  0.00251677852349\n",
      "initial_corpus size: 4642\n",
      "length of s3h results:  7655 length of test set 1192\n",
      "F1:  0.0580987905505\n",
      "precision s3h:  0.0335728282169 recall s3h:  0.215604026846\n",
      "initial_corpus size: 4642\n",
      "length of results:  4685 length of test set 1192\n",
      "F1:  0.00204185809086\n",
      "precision fusion:  0.00128068303095 recall fusion:  0.00503355704698\n",
      "for length of results:  9370\n",
      "F1:  0.00511266805529\n",
      "precision fusion:  0.00288153681964 recall fusion:  0.0226510067114\n",
      " for length of results:  14055\n",
      "F1:  0.00511576047747\n",
      "precision fusion:  0.00277481323372 recall fusion:  0.0327181208054\n",
      " for length of results:  18740\n",
      "F1:  0.00581978727674\n",
      "precision fusion:  0.00309498399146 recall fusion:  0.0486577181208\n",
      " for length of results:  23425\n",
      "F1:  0.00544339277735\n",
      "precision fusion:  0.00286019210245 recall fusion:  0.0562080536913\n"
     ]
    }
   ],
   "source": [
    "eval_all(AI_fusion_res, topic, synset, \"results/Remote_sensing_results.pickle\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2658\n",
      "initial_corpus size: 2658\n",
      "babelnet results size of the topic \"Substance abuse\": 7893\n",
      "ground truth size 466\n",
      "intersection with the ground truth: 66\n",
      "F1 0.0157913626032\n",
      "babel precision:  0.00836183960471\n",
      "babel recall:  0.141630901288\n",
      "initial_corpus size: 2658\n",
      "length of s3h results:  4183 length of test set 466\n",
      "F1:  0.0172080017208\n",
      "precision s3h:  0.00956251494143 recall s3h:  0.0858369098712\n",
      "initial_corpus size: 2658\n",
      "length of results:  7893 length of test set 466\n",
      "F1:  0.00550305060414\n",
      "precision fusion:  0.0029139744077 recall fusion:  0.049356223176\n",
      "for length of results:  15786\n",
      "F1:  0.0040610386414\n",
      "precision fusion:  0.00209045990118 recall fusion:  0.0708154506438\n",
      " for length of results:  23679\n",
      "F1:  0.00397597846345\n",
      "precision fusion:  0.00202711263145 recall fusion:  0.103004291845\n",
      " for length of results:  31572\n",
      "F1:  0.00330857107185\n",
      "precision fusion:  0.00167870264792 recall fusion:  0.113733905579\n",
      " for length of results:  39465\n",
      "F1:  0.00315544313942\n",
      "precision fusion:  0.00159635119726 recall fusion:  0.135193133047\n"
     ]
    }
   ],
   "source": [
    "bab = \"substance abuse, drug abuse, habit, addiction, dependency, Abuse potential, Abusing drugs, Abusive drug use, Anti-drug, Cannabis abuse, Drug-abuse, Drug misuse, Drug prevention, Drugs of abuse, Illegal drug abuse, Illegal drug use, Misuse of drugs, Narcotic abuse theory, Nondependent abuse of drugs, Prescription drug abuse, Prescription Drug Misuse\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Substance abuse'\n",
    "res_pickle = \"results/Substance_results.pickle\"\n",
    "SA_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(SA_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 5033\n",
      "initial_corpus size: 5033\n",
      "babelnet results size of the topic \"Information Systems\": 8440\n",
      "ground truth size 1313\n",
      "intersection with the ground truth: 298\n",
      "F1 0.0611094022352\n",
      "babel precision:  0.035308056872\n",
      "babel recall:  0.226961157654\n",
      "initial_corpus size: 5033\n",
      "length of s3h results:  6820 length of test set 1313\n",
      "F1:  0.0174597319562\n",
      "precision s3h:  0.0104105571848 recall s3h:  0.0540746382331\n",
      "initial_corpus size: 5033\n",
      "length of results:  8440 length of test set 1313\n",
      "F1:  0.00984312519225\n",
      "precision fusion:  0.00568720379147 recall fusion:  0.036557501904\n",
      "for length of results:  16880\n",
      "F1:  0.00747540262738\n",
      "precision fusion:  0.00402843601896 recall fusion:  0.0517897943641\n",
      " for length of results:  25320\n",
      "F1:  0.00750948071941\n",
      "precision fusion:  0.00394944707741 recall fusion:  0.0761614623001\n",
      " for length of results:  33760\n",
      "F1:  0.00672882274114\n",
      "precision fusion:  0.00349526066351 recall fusion:  0.0898705255141\n",
      " for length of results:  42200\n",
      "F1:  0.00611311562062\n",
      "precision fusion:  0.00315165876777 recall fusion:  0.101294744859\n"
     ]
    }
   ],
   "source": [
    "bab = \"information system, data system, information systems, system info, Business computing, Computer information system, Computer information systems, Elements of Information System, Information in Computer Science, Information systems and technology, Information systems discipline, Information systems theory, Informationssystem\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Information Systems'\n",
    "res_pickle = \"results/infosys_results.pickle\"\n",
    "IS_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(IS_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6351\n",
      "initial_corpus size: 6351\n",
      "babelnet results size of the topic \"Thermodynamics\": 8375\n",
      "ground truth size 2727\n",
      "intersection with the ground truth: 392\n",
      "F1 0.0706179066835\n",
      "babel precision:  0.0468059701493\n",
      "babel recall:  0.143747708104\n",
      "initial_corpus size: 6351\n",
      "length of s3h results:  7119 length of test set 2727\n",
      "F1:  0.02985984156\n",
      "precision s3h:  0.0206489675516 recall s3h:  0.0539053905391\n",
      "initial_corpus size: 6351\n",
      "length of results:  8375 length of test set 2727\n",
      "F1:  0.0216177265358\n",
      "precision fusion:  0.014328358209 recall fusion:  0.04400440044\n",
      "for length of results:  16750\n",
      "F1:  0.0203316732556\n",
      "precision fusion:  0.0118208955224 recall fusion:  0.0726072607261\n",
      " for length of results:  25125\n",
      "F1:  0.0193881947436\n",
      "precision fusion:  0.0107462686567 recall fusion:  0.0990099009901\n",
      " for length of results:  33500\n",
      "F1:  0.01799762608\n",
      "precision fusion:  0.00973134328358 recall fusion:  0.119545287862\n",
      " for length of results:  41875\n",
      "F1:  0.0169947535985\n",
      "precision fusion:  0.00905074626866 recall fusion:  0.138980564723\n"
     ]
    }
   ],
   "source": [
    "bab = \"thermodynamics, Thermo-dynamics, Thermodynamic functions, Applied thermodynamics, Classical thermodynamics, entropy, Macroscopic thermodynamics, Phenomenological thermodynamics, second law of thermodynamics, Termodynamics, Thermal behavior, Thermics, Thermodymanics, Thermodynamic, Thermodynamic function, Thermodynamic law, Thermodynamic Laws\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Thermodynamics'\n",
    "res_pickle = \"results/res__Thermodynamics\"\n",
    "TD_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(TD_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8607\n",
      "initial_corpus size: 8607\n",
      "babelnet results size of the topic \"spectroscopy\": 8513\n",
      "ground truth size 7294\n",
      "intersection with the ground truth: 287\n",
      "F1 0.0363130258746\n",
      "babel precision:  0.0337131446024\n",
      "babel recall:  0.0393474088292\n",
      "initial_corpus size: 8607\n",
      "length of s3h results:  9511 length of test set 7294\n",
      "F1:  0.0443915501339\n",
      "precision s3h:  0.0392177478709 recall s3h:  0.0511379215794\n",
      "initial_corpus size: 8607\n",
      "length of results:  8513 length of test set 7294\n",
      "F1:  0.0283418738534\n",
      "precision fusion:  0.0263126982262 recall fusion:  0.0307101727447\n",
      "for length of results:  17026\n",
      "F1:  0.0352796052632\n",
      "precision fusion:  0.0251967578997 recall fusion:  0.0588154647656\n",
      " for length of results:  25539\n",
      "F1:  0.0418481405903\n",
      "precision fusion:  0.0269000352402 recall fusion:  0.0941870030162\n",
      " for length of results:  34052\n",
      "F1:  0.0417452716103\n",
      "precision fusion:  0.0253435921532 recall fusion:  0.118316424458\n",
      " for length of results:  42565\n",
      "F1:  0.0423193405403\n",
      "precision fusion:  0.0247856219899 recall fusion:  0.144639429668\n"
     ]
    }
   ],
   "source": [
    "bab = \"spectroscopy, spectrographic analysis, spectrometry, spectroscopic analysis, spectrum analysis, Fingerprint region, Laser spectroscopy, Optical spectroscopy, Atomic line, Atomic line spectra, Atomic spectra, Atomic spectral line, Electromagnetic spectroscopy, Emission spectrochemical analysis, Spectrochemical Analysis, Spectrography, Spectrology, Spectroscopic, Spectroscopist, Spectroscopists\"\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'spectroscopy'\n",
    "res_pickle = \"results/res__Spectroscopy\"\n",
    "BC_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(BC_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 5349\n",
      "initial_corpus size: 5349\n",
      "babelnet results size of the topic \"Optics\": 8730\n",
      "ground truth size 3286\n",
      "intersection with the ground truth: 271\n",
      "F1 0.0451065246338\n",
      "babel precision:  0.0310423825888\n",
      "babel recall:  0.0824710894705\n",
      "initial_corpus size: 5349\n",
      "length of s3h results:  4958 length of test set 3286\n",
      "F1:  0.0499757399321\n",
      "precision s3h:  0.0415490116983 recall s3h:  0.0626902008521\n",
      "initial_corpus size: 5349\n",
      "length of results:  8730 length of test set 3286\n",
      "F1:  0.0291278295606\n",
      "precision fusion:  0.0200458190149 recall fusion:  0.0532562385879\n",
      "for length of results:  17460\n",
      "F1:  0.0264147305505\n",
      "precision fusion:  0.0156930126002 recall fusion:  0.0833840535606\n",
      " for length of results:  26190\n",
      "F1:  0.0269371692224\n",
      "precision fusion:  0.0151584574265 recall fusion:  0.120815581254\n",
      " for length of results:  34920\n",
      "F1:  0.0245511176255\n",
      "precision fusion:  0.01343069874 recall fusion:  0.142726719416\n",
      " for length of results:  43650\n",
      "F1:  0.0236492244759\n",
      "precision fusion:  0.0127147766323 recall fusion:  0.168898356665\n"
     ]
    }
   ],
   "source": [
    "bab = 'optics, Classical optics, Light physics, Optical, Optical system'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Optics'\n",
    "res_pickle = \"results/res__Optics\"\n",
    "Opt_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(Opt_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 291\n",
      "initial_corpus size: 291\n",
      "babelnet results size of the topic \"Cybernetics\": 511\n",
      "ground truth size 596\n",
      "intersection with the ground truth: 47\n",
      "F1 0.0849141824752\n",
      "babel precision:  0.0919765166341\n",
      "babel recall:  0.0788590604027\n",
      "initial_corpus size: 291\n",
      "length of s3h results:  9797 length of test set 596\n",
      "F1:  0.0144327913018\n",
      "precision s3h:  0.00765540471573 recall s3h:  0.125838926174\n",
      "initial_corpus size: 291\n",
      "length of results:  511 length of test set 596\n",
      "F1:  0.00722673893406\n",
      "precision fusion:  0.00782778864971 recall fusion:  0.00671140939597\n",
      "for length of results:  1022\n",
      "F1:  0.0061804697157\n",
      "precision fusion:  0.00489236790607 recall fusion:  0.00838926174497\n",
      " for length of results:  1533\n",
      "F1:  0.00469704086426\n",
      "precision fusion:  0.00326157860404 recall fusion:  0.00838926174497\n",
      " for length of results:  2044\n",
      "F1:  0.00378787878788\n",
      "precision fusion:  0.00244618395303 recall fusion:  0.00838926174497\n",
      " for length of results:  2555\n",
      "F1:  0.00317359568391\n",
      "precision fusion:  0.00195694716243 recall fusion:  0.00838926174497\n"
     ]
    }
   ],
   "source": [
    "bab = 'cybernetics, Cybernetic, Cybernetic system, cybernetician, Cyberneticians, cyberneticist, Tha Masta'\n",
    "bab = bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Cybernetics'\n",
    "res_pickle = \"results/res__Cybernetics\"\n",
    "Cy_fusion_res = get_fusion_res3(res_pickle, topic, synset)\n",
    "eval_all(Cy_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2091\n",
      "initial_corpus size: 2091\n",
      "babelnet results size of the topic \"artificial intelligence\": 7903\n",
      "ground truth size 657\n",
      "intersection with the ground truth: 43\n",
      "F1 0.010046728972\n",
      "babel precision:  0.00544097178287\n",
      "babel recall:  0.0654490106545\n",
      "initial_corpus size: 2091\n",
      "length of s3h results:  8566 length of test set 657\n",
      "F1:  0.0173479345115\n",
      "precision s3h:  0.00933924819052 recall s3h:  0.121765601218\n",
      "initial_corpus size: 2091\n",
      "length of results:  7903 length of test set 657\n",
      "F1:  0.0200934579439\n",
      "precision fusion:  0.0108819435657 recall fusion:  0.130898021309\n",
      "for length of results:  15806\n",
      "F1:  0.0153070521776\n",
      "precision fusion:  0.00797165633304 recall fusion:  0.191780821918\n",
      " for length of results:  23709\n",
      "F1:  0.0125584831322\n",
      "precision fusion:  0.00645324560294 recall fusion:  0.232876712329\n",
      " for length of results:  31612\n",
      "F1:  0.0112801760203\n",
      "precision fusion:  0.00575730735164 recall fusion:  0.27701674277\n",
      " for length of results:  39515\n",
      "F1:  0.00985761226725\n",
      "precision fusion:  0.00501075540934 recall fusion:  0.301369863014\n"
     ]
    }
   ],
   "source": [
    "bab = 'artificial intelligence, AI, artilect, Artifical intelligence, Cognitive systems, Digital being, Machine intelligence, A.I., A I, AI-D, AI ethics, AI implications, AI Robotics, AI scripting, Aretificial intelligence, Artificial-intelligence, Artificial conciousness, Artificial inteligence, Artificial intellect, Artificial intellegence, Artificial Intelligence., Artificial intelligence for development, Artificial Intelligence Program, Artificial intelligences, Artificially-intelligent, Artificially intelligent, Artificual intelligence, Cognitive simulation, Cognitive system, Commonsense AI, Computational Rationality, Computer AI, Intelligent machine, Machine thought, Machine understanding, Ontology based approach, Pseudo intelligence, Semi-AI, Semi AI, Simulated intelligence, Soft AI, Sub-symbolic, Subsymbolic, The Artificial Intelligence, The Theory of Artificial Intelligence'\n",
    "synset = bab.split(',')\n",
    "topic = 'artificial intelligence'\n",
    "AI_fusion_res = get_fusion_res5(\"results/AI_results.pickle\", topic, synset)\n",
    "eval_all(AI_fusion_res, topic, synset, \"results/AI_results.pickle\", thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2658\n",
      "initial_corpus size: 2658\n",
      "babelnet results size of the topic \"Substance abuse\": 7893\n",
      "ground truth size 466\n",
      "intersection with the ground truth: 66\n",
      "F1 0.0157913626032\n",
      "babel precision:  0.00836183960471\n",
      "babel recall:  0.141630901288\n",
      "initial_corpus size: 2658\n",
      "length of s3h results:  4183 length of test set 466\n",
      "F1:  0.0172080017208\n",
      "precision s3h:  0.00956251494143 recall s3h:  0.0858369098712\n",
      "initial_corpus size: 2658\n",
      "length of results:  7893 length of test set 466\n",
      "F1:  0.0212944132073\n",
      "precision fusion:  0.0112758140124 recall fusion:  0.190987124464\n",
      "for length of results:  15786\n",
      "F1:  0.0162441545656\n",
      "precision fusion:  0.00836183960471 recall fusion:  0.283261802575\n",
      " for length of results:  23679\n",
      "F1:  0.0124249326983\n",
      "precision fusion:  0.00633472697327 recall fusion:  0.321888412017\n",
      " for length of results:  31572\n",
      "F1:  0.0106123977776\n",
      "precision fusion:  0.00538451792728 recall fusion:  0.364806866953\n",
      " for length of results:  39465\n",
      "F1:  0.00976684781248\n",
      "precision fusion:  0.00494108703915 recall fusion:  0.418454935622\n"
     ]
    }
   ],
   "source": [
    "bab = \"substance abuse, drug abuse, habit, addiction, dependency, Abuse potential, Abusing drugs, Abusive drug use, Anti-drug, Cannabis abuse, Drug-abuse, Drug misuse, Drug prevention, Drugs of abuse, Illegal drug abuse, Illegal drug use, Misuse of drugs, Narcotic abuse theory, Nondependent abuse of drugs, Prescription drug abuse, Prescription Drug Misuse\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Substance abuse'\n",
    "res_pickle = \"results/Substance_results.pickle\"\n",
    "SA_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(SA_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 5033\n",
      "initial_corpus size: 5033\n",
      "babelnet results size of the topic \"Information Systems\": 8440\n",
      "ground truth size 1313\n",
      "intersection with the ground truth: 298\n",
      "F1 0.0611094022352\n",
      "babel precision:  0.035308056872\n",
      "babel recall:  0.226961157654\n",
      "initial_corpus size: 5033\n",
      "length of s3h results:  6820 length of test set 1313\n",
      "F1:  0.0174597319562\n",
      "precision s3h:  0.0104105571848 recall s3h:  0.0540746382331\n",
      "initial_corpus size: 5033\n",
      "length of results:  8440 length of test set 1313\n",
      "F1:  0.0467548446632\n",
      "precision fusion:  0.0270142180095 recall fusion:  0.173648134044\n",
      "for length of results:  16880\n",
      "F1:  0.036057824438\n",
      "precision fusion:  0.0194312796209 recall fusion:  0.249809596344\n",
      " for length of results:  25320\n",
      "F1:  0.030188112492\n",
      "precision fusion:  0.0158767772512 recall fusion:  0.306169078446\n",
      " for length of results:  33760\n",
      "F1:  0.0262310039061\n",
      "precision fusion:  0.0136255924171 recall fusion:  0.35034272658\n",
      " for length of results:  42200\n",
      "F1:  0.0227058580194\n",
      "precision fusion:  0.0117061611374 recall fusion:  0.376237623762\n"
     ]
    }
   ],
   "source": [
    "bab = \"information system, data system, information systems, system info, Business computing, Computer information system, Computer information systems, Elements of Information System, Information in Computer Science, Information systems and technology, Information systems discipline, Information systems theory, Informationssystem\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Information Systems'\n",
    "res_pickle = \"results/infosys_results.pickle\"\n",
    "IS_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(IS_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6351\n",
      "initial_corpus size: 6351\n",
      "babelnet results size of the topic \"Thermodynamics\": 8375\n",
      "ground truth size 2727\n",
      "intersection with the ground truth: 392\n",
      "F1 0.0706179066835\n",
      "babel precision:  0.0468059701493\n",
      "babel recall:  0.143747708104\n",
      "initial_corpus size: 6351\n",
      "length of s3h results:  7119 length of test set 2727\n",
      "F1:  0.02985984156\n",
      "precision s3h:  0.0206489675516 recall s3h:  0.0539053905391\n",
      "initial_corpus size: 6351\n",
      "length of results:  8375 length of test set 2727\n",
      "F1:  0.0590884525311\n",
      "precision fusion:  0.0391641791045 recall fusion:  0.120278694536\n",
      "for length of results:  16750\n",
      "F1:  0.0507264979206\n",
      "precision fusion:  0.0294925373134 recall fusion:  0.181151448478\n",
      " for length of results:  25125\n",
      "F1:  0.0449518885538\n",
      "precision fusion:  0.0249154228856 recall fusion:  0.229556288962\n",
      " for length of results:  33500\n",
      "F1:  0.0407982996108\n",
      "precision fusion:  0.0220597014925 recall fusion:  0.270993766043\n",
      " for length of results:  41875\n",
      "F1:  0.0380700417022\n",
      "precision fusion:  0.0202746268657 recall fusion:  0.311331133113\n"
     ]
    }
   ],
   "source": [
    "bab = \"thermodynamics, Thermo-dynamics, Thermodynamic functions, Applied thermodynamics, Classical thermodynamics, entropy, Macroscopic thermodynamics, Phenomenological thermodynamics, second law of thermodynamics, Termodynamics, Thermal behavior, Thermics, Thermodymanics, Thermodynamic, Thermodynamic function, Thermodynamic law, Thermodynamic Laws\"\n",
    "synset = bab.split(',')\n",
    "topic = 'Thermodynamics'\n",
    "res_pickle = \"results/res__Thermodynamics\"\n",
    "TD_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(TD_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 7458\n",
      "initial_corpus size: 7458\n",
      "babelnet results size of the topic \"Rehabilitation\": 7449\n",
      "ground truth size 773\n",
      "intersection with the ground truth: 0\n",
      "F1 0.0\n",
      "babel precision:  0.0\n",
      "babel recall:  0.0\n",
      "initial_corpus size: 7458\n",
      "length of s3h results:  7137 length of test set 773\n",
      "F1:  0.0111251580278\n",
      "precision s3h:  0.00616505534538 recall s3h:  0.0569210866753\n",
      "initial_corpus size: 7458\n",
      "length of results:  7449 length of test set 773\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-7efcdd800adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mres_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results/res__Rehabilitation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mReh_fusion_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fusion_res5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReh_fusion_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-f6e034c09c60>\u001b[0m in \u001b[0;36meval_all\u001b[0;34m(fusion_df, topic, synset, res_pickle, thresh)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_intersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_intersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mF1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"F1: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"precision fusion: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recall fusion: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "bab = \"rehabilitation, Neurocognitive Rehabilitation, Neurological rehabilitation, Neuropsychological rehabilitation, Rehabilitation Neuropsychology\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Rehabilitation'\n",
    "res_pickle = \"results/res__Rehabilitation\"\n",
    "Reh_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Reh_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 7458\n",
      "babelnet results size of the topic \"Rehabilitation\": 7449\n",
      "ground truth size 773\n",
      "intersection with the ground truth: 0\n",
      "F1 0.0\n",
      "babel precision:  0.0\n",
      "babel recall:  0.0\n",
      "initial_corpus size: 7458\n",
      "length of s3h results:  7137 length of test set 773\n",
      "F1:  0.0111251580278\n",
      "precision s3h:  0.00616505534538 recall s3h:  0.0569210866753\n",
      "initial_corpus size: 7458\n",
      "length of results:  7449 length of test set 773\n",
      "F1:  0\n",
      "precision fusion:  0.0 recall fusion:  0.0\n",
      "for length of results:  14898\n",
      "F1:  0.010209941931\n",
      "precision fusion:  0.00536984830179 recall fusion:  0.103492884864\n",
      " for length of results:  22347\n",
      "F1:  0.0113321799308\n",
      "precision fusion:  0.00586208439612 recall fusion:  0.169469598965\n",
      " for length of results:  29796\n",
      "F1:  0.0112532303968\n",
      "precision fusion:  0.00577258692442 recall fusion:  0.222509702458\n",
      " for length of results:  37245\n",
      "F1:  0.0111000052607\n",
      "precision fusion:  0.00566518995838 recall fusion:  0.272962483829\n"
     ]
    }
   ],
   "source": [
    "eval_all(Reh_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6857\n",
      "initial_corpus size: 6857\n",
      "babelnet results size of the topic \"Psychology\": 7187\n",
      "ground truth size 1871\n",
      "intersection with the ground truth: 134\n",
      "F1 0.0295871053213\n",
      "babel precision:  0.0186447752887\n",
      "babel recall:  0.071619454837\n",
      "initial_corpus size: 6857\n",
      "length of s3h results:  12847 length of test set 1871\n",
      "F1:  0.00937627395026\n",
      "precision s3h:  0.00537090371293 recall s3h:  0.0368786745056\n",
      "initial_corpus size: 6857\n",
      "length of results:  7187 length of test set 1871\n",
      "F1:  0.0143519540737\n",
      "precision fusion:  0.00904410741617 recall fusion:  0.0347407803314\n",
      "for length of results:  14374\n",
      "F1:  0.0134195136965\n",
      "precision fusion:  0.00758313621817 recall fusion:  0.058257616248\n",
      " for length of results:  21561\n",
      "F1:  0.0123762376238\n",
      "precision fusion:  0.00672510551459 recall fusion:  0.0774986638161\n",
      " for length of results:  28748\n",
      "F1:  0.0122799568895\n",
      "precision fusion:  0.00653958536246 recall fusion:  0.100481026189\n",
      " for length of results:  35935\n",
      "F1:  0.0129080040205\n",
      "precision fusion:  0.00679003756783 recall fusion:  0.130411544629\n"
     ]
    }
   ],
   "source": [
    "bab = \"psychology, psychological science, Human psychology, Psychological, Psychologically, Criticism of psychology, Human trait, Hyde event, Phsycology, Physcology, Professional psychology, Psychogenics, Psychologic, Psychological sciences, Psychological terms, Psychological theories, Psychological theory, psychologist, Psychologists, Psychology/rewrite, Psycologic, Psycological, Psycology, Pyhscology, Self-report study, WEIRD\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Psychology'\n",
    "res_pickle = \"results/res__Psychology\"\n",
    "Psych_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Psych_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 6692\n",
      "initial_corpus size: 6692\n",
      "babelnet results size of the topic \"Philosophy\": 7116\n",
      "ground truth size 677\n",
      "intersection with the ground truth: 110\n",
      "F1 0.0282304632362\n",
      "babel precision:  0.0154581225408\n",
      "babel recall:  0.162481536189\n",
      "initial_corpus size: 6692\n",
      "length of s3h results:  11104 length of test set 677\n",
      "F1:  0.0152788388083\n",
      "precision s3h:  0.00810518731988 recall s3h:  0.1329394387\n",
      "initial_corpus size: 6692\n",
      "length of results:  7116 length of test set 677\n",
      "F1:  0.0218144488644\n",
      "precision fusion:  0.0119449128724 recall fusion:  0.125553914328\n",
      "for length of results:  14232\n",
      "F1:  0.0187806023207\n",
      "precision fusion:  0.00983698707139 recall fusion:  0.206794682422\n",
      " for length of results:  21348\n",
      "F1:  0.0174347332577\n",
      "precision fusion:  0.00899381675098 recall fusion:  0.283604135894\n",
      " for length of results:  28464\n",
      "F1:  0.016059846951\n",
      "precision fusion:  0.00822091062395 recall fusion:  0.34564254062\n",
      " for length of results:  35580\n",
      "F1:  0.0141765727997\n",
      "precision fusion:  0.00722315907813 recall fusion:  0.379615952733\n"
     ]
    }
   ],
   "source": [
    "bab = \"philosophy, Philosophic, Philosophical, Philosophical Subdisciplines, Philosophically, Philosophized, Applied philosophy, Branch of philosophy, Branches of philosophy, DefinitionOfPhilosophy, Definitions of philosophy, Filosofy, Philisophical, Philisophy, Philo-sophy, Philosophae, philosopher, Philosophers, Philosophhy, Philosophiae, PhilosophicalSubdisciplines, Philosophicians, Philosophies, Philosophise, Philosophised, Philosophiser, Philosophisers, Philosophises, Philosophising, Philosophize, Philosophizer, Philosophizers, Philosophizes, Philosophizing, PhilosophyAndLogic, Philosphical, Philosphy, Phylosophy, Roman ideals, Sage\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Philosophy'\n",
    "res_pickle = \"results/res__Philosophy\"\n",
    "Phil_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Phil_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 958\n",
      "initial_corpus size: 958\n",
      "babelnet results size of the topic \"Ophthalmology\": 2096\n",
      "ground truth size 459\n",
      "intersection with the ground truth: 25\n",
      "F1 0.0195694716243\n",
      "babel precision:  0.011927480916\n",
      "babel recall:  0.0544662309368\n",
      "initial_corpus size: 958\n",
      "length of s3h results:  5793 length of test set 459\n",
      "F1:  0.00447856685861\n",
      "precision s3h:  0.0024167098222 recall s3h:  0.0305010893246\n",
      "initial_corpus size: 958\n",
      "length of results:  2096 length of test set 459\n",
      "F1:  0.0133072407045\n",
      "precision fusion:  0.0081106870229 recall fusion:  0.037037037037\n",
      "for length of results:  4192\n",
      "F1:  0.0107503762632\n",
      "precision fusion:  0.00596374045802 recall fusion:  0.0544662309368\n",
      " for length of results:  6288\n",
      "F1:  0.00800355713651\n",
      "precision fusion:  0.00429389312977 recall fusion:  0.0588235294118\n",
      " for length of results:  8384\n",
      "F1:  0.00633269252516\n",
      "precision fusion:  0.00333969465649 recall fusion:  0.0610021786492\n",
      " for length of results:  10480\n",
      "F1:  0.00639912240607\n",
      "precision fusion:  0.00333969465649 recall fusion:  0.0762527233115\n"
     ]
    }
   ],
   "source": [
    "bab = \"ophthalmology, All India Ophthalmological Conference, Clincial ophthalmology, Clincial opthalmology, Clinical ophtalmology, Clinical Ophthalmology, Clinical Opthalmology, General ophthalmic services, Oculists, Oftamology, Ofthamology, Ophthalmic surgeon, Ophthalmologic, Ophthalmological, Ophthalmologicals, Ophthalmologists, Ophthamologist, Ophthamology, Optamology, Opthalmological, Opthalmologist, Opthalmology, Opthamologist, Opthamology, Opthomologist, Society for clincial ophthalmology, Society for clincial opthalmology, Society for clinical ophthalmology, Society for clinical opthalmology, Vision care\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Ophthalmology'\n",
    "res_pickle = \"results/res__Ophthalmology\"\n",
    "Oph_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Oph_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8418\n",
      "initial_corpus size: 8418\n",
      "babelnet results size of the topic \"Microscopy\": 8547\n",
      "ground truth size 6819\n",
      "intersection with the ground truth: 253\n",
      "F1 0.0329298451126\n",
      "babel precision:  0.029601029601\n",
      "babel recall:  0.0371022144009\n",
      "initial_corpus size: 8418\n",
      "length of s3h results:  10573 length of test set 6819\n",
      "F1:  0.0501379944802\n",
      "precision s3h:  0.0412371134021 recall s3h:  0.0639389939874\n",
      "initial_corpus size: 8418\n",
      "length of results:  8547 length of test set 6819\n",
      "F1:  0.039958349603\n",
      "precision fusion:  0.035919035919 recall fusion:  0.045021264115\n",
      "for length of results:  17094\n",
      "F1:  0.0637310249655\n",
      "precision fusion:  0.044577044577 recall fusion:  0.111746590409\n",
      " for length of results:  25641\n",
      "F1:  0.0711645101664\n",
      "precision fusion:  0.045045045045 recall fusion:  0.169379674439\n",
      " for length of results:  34188\n",
      "F1:  0.0751578998708\n",
      "precision fusion:  0.0450742950743 recall fusion:  0.225986214988\n",
      " for length of results:  42735\n",
      "F1:  0.0747871009404\n",
      "precision fusion:  0.0433602433602 recall fusion:  0.271740724446\n"
     ]
    }
   ],
   "source": [
    "bab = \"microscopy, 3D-SIM-microscopy, Amateur microscopy, Bioimaging, Infrared microscopy, IR microscopy, Laser microscopy, Light microscopy, Microscopic examination, Microscopically, Microscopist, Oblique illumination, Polarized light microscope\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Microscopy'\n",
    "res_pickle = \"results/res__Microscopy\"\n",
    "Micr_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Micr_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 8256\n",
      "initial_corpus size: 8256\n",
      "babelnet results size of the topic \"Ceramics\": 8249\n",
      "ground truth size 1276\n",
      "intersection with the ground truth: 0\n",
      "F1 0.0\n",
      "babel precision:  0.0\n",
      "babel recall:  0.0\n",
      "initial_corpus size: 8256\n",
      "length of s3h results:  8245 length of test set 1276\n",
      "F1:  0.0212162587963\n",
      "precision s3h:  0.012249848393 recall s3h:  0.0791536050157\n",
      "initial_corpus size: 8256\n",
      "length of results:  8249 length of test set 1276\n",
      "F1:  0\n",
      "precision fusion:  0.0 recall fusion:  0.0\n",
      "for length of results:  16498\n",
      "F1:  0.0178913019017\n",
      "precision fusion:  0.00963753182204 recall fusion:  0.12460815047\n",
      " for length of results:  24747\n",
      "F1:  0.0198286131499\n",
      "precision fusion:  0.010425506122 recall fusion:  0.202194357367\n",
      " for length of results:  32996\n",
      "F1:  0.0196078431373\n",
      "precision fusion:  0.0101830524912 recall fusion:  0.263322884013\n",
      " for length of results:  41245\n",
      "F1:  0.0185790550551\n",
      "precision fusion:  0.00957691841435 recall fusion:  0.309561128527\n"
     ]
    }
   ],
   "source": [
    "bab = \"ceramics, Ceramic art, Art pottery, Art ware, Ceramic artist, Ceramic paint, Ceramics art, Fine art pot, Vase painting\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Ceramics'\n",
    "res_pickle = \"results/res__Ceramics\"\n",
    "Cer_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(Cer_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_corpus size: 2588\n",
      "initial_corpus size: 2588\n",
      "babelnet results size of the topic \"Infectious Diseases\": 8343\n",
      "ground truth size 1375\n",
      "intersection with the ground truth: 50\n",
      "F1 0.0102901831653\n",
      "babel precision:  0.00599304806425\n",
      "babel recall:  0.0363636363636\n",
      "initial_corpus size: 2588\n",
      "length of s3h results:  1957 length of test set 1375\n",
      "F1:  0.00180072028812\n",
      "precision s3h:  0.00153295861012 recall s3h:  0.00218181818182\n",
      "initial_corpus size: 2588\n",
      "length of results:  8343 length of test set 1375\n",
      "F1:  0.00967277217534\n",
      "precision fusion:  0.00563346518039 recall fusion:  0.0341818181818\n",
      "for length of results:  16686\n",
      "F1:  0.015060074193\n",
      "precision fusion:  0.00815054536737 recall fusion:  0.0989090909091\n",
      " for length of results:  25029\n",
      "F1:  0.0168156339948\n",
      "precision fusion:  0.00886971113508 recall fusion:  0.161454545455\n",
      " for length of results:  33372\n",
      "F1:  0.0166345295997\n",
      "precision fusion:  0.00865995445283 recall fusion:  0.210181818182\n",
      " for length of results:  41715\n",
      "F1:  0.0157345091669\n",
      "precision fusion:  0.00812657317512 recall fusion:  0.246545454545\n"
     ]
    }
   ],
   "source": [
    "bab = \"infection, infectious disease, communicable diseases, contagion, Infectious diseases, Acute infection, AIDS-related bacterial infections, AIDS-related viral infections, Anti-infective, Anti-infectives, Antiinfective, Bacterial Infections, Communicable disease, Contagious diseases, Definition to contagious, Infect, Infecting, Infectiology, Infections, Infectious, Infectious disease epidemiology, Infectious disease medicine, Infectology, Local infection, Primary infection, Rochalimea infections, Secondary infection, Tropical bacterial infections, Tropical infections, Viral Infections, Wound colonization, Wound infection\"\n",
    "bab.replace('&', 'and')\n",
    "synset = bab.split(',')\n",
    "topic = 'Infectious Diseases'\n",
    "res_pickle = \"results/Infectious_results.pickle\"\n",
    "InfD_fusion_res = get_fusion_res5(res_pickle, topic, synset)\n",
    "eval_all(InfD_fusion_res, topic, synset, res_pickle, thresh=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

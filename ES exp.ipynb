{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, pickle, argparse, json, os, urllib2\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_from(q, f):\n",
    "    q = q+'&from='+str(f)\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    subject_ids = np.array(range(len(data['hits'])), dtype=np.object)\n",
    "    for (i, hit) in enumerate(data['hits']):\n",
    "        subject_ids[i] = hit['id']\n",
    "    return subject_ids\n",
    "\n",
    "def query(q):\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    nb_requests = 1 + data['total'] / 1000\n",
    "    if nb_requests > 10: # maximum number of pages due to API pagination restrection\n",
    "        nb_requests = 10\n",
    "    subject_ids = query_from(q, 0)\n",
    "    for i in range(nb_requests)[1:]:\n",
    "        f = i * 1000\n",
    "        next_request = query_from(q, f)\n",
    "        subject_ids = np.hstack((subject_ids, next_request))\n",
    "    return subject_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['al-natsheh', 'hussein']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_intersection(list_a, list_b):\n",
    "    return list(set(list_a) & set(list_b))\n",
    "a = ['hussein', 'tayseer', 'al-natsheh']\n",
    "b = ['loay', 'hussein', 'al-natsheh']\n",
    "find_intersection(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original inversed_index\n",
      "[(u'ISTEX_D89FA3AC3521074D46F4245762153DF497BFFA1F', 2002320), (u'ISTEX_18EAF4D6A126B077EB38667801D1B7292F32FF49', 2483732), (u'ISTEX_5F91044435FCC4FABB9F02E31467DCFE75F4A7BE', 1429049)]\n",
      "processed inversed_index\n",
      "[(u'FCF1393F9B8136AC08FB67E88F94F3CF62C17288', 3517138), (u'482E1102A1114327A744FD2ADB4D9F8FF7E9A70B', 751643), (u'A81022B6295AE66F68A10222C3B94A06B033C1BA', 3983232)]\n"
     ]
    }
   ],
   "source": [
    "inv_index = json.load(open('../RecSys_Exp_files/182_381_vec150_results/output_paragraph_inversed_index.json','rb'))\n",
    "print 'original inversed_index'\n",
    "print inv_index.items()[:3]\n",
    "inversed_index = dict()\n",
    "for (k, v) in inv_index.items():\n",
    "    key = k.split('_')[1]\n",
    "    inversed_index[key] = v\n",
    "print 'processed inversed_index'\n",
    "print inversed_index.items()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_get_input(topic, categories=0):\n",
    "    if not categories:\n",
    "        initial_corpus = query('https://api.istex.fr/document/?q=((title:%22'+topic+'%22%20OR%20abstract:%22'+topic+'%22)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id')\n",
    "        _gs = query('https://api.istex.fr/document/?q=((subject.value:%22'+topic+'%22%20OR%20keywords:%22'+topic+'%22%20)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id')\n",
    "    else:\n",
    "        initial_corpus = query('https://api.istex.fr/document/?q=((title:%22'+topic+'%22%20OR%20subject.value:%22'+topic+'%22%20OR%20keywords:%22'+topic+'%22%20OR%20abstract:%22'+topic+'%22)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id')\n",
    "        _gs = query('https://api.istex.fr/document/?q=((categories.wos:%22'+topic+'%22%20AND%20corpusName:%22elsevier%22)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id')\n",
    "    initial_corpus = find_intersection(initial_corpus, inversed_index.keys())\n",
    "    test_set = {x for x in _gs if x not in initial_corpus}\n",
    "    test_set = find_intersection(test_set, inversed_index.keys())\n",
    "    initial_corpus = list(initial_corpus)\n",
    "    test = list(test_set)\n",
    "    return initial_corpus, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of initial corpus: 344\n",
      "size of ground_truth: 55\n"
     ]
    }
   ],
   "source": [
    "initial_corpus, test = topic_get_input('Biodiversity%20Conservation')\n",
    "print 'size of initial corpus:', len(initial_corpus)\n",
    "print 'size of ground_truth:', len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries =[]\n",
    "for istex_id in initial_corpus:\n",
    "    q = 'https://api.istex.fr/document/'+istex_id\n",
    "    queries.append(q)\n",
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing abstract of istex_id: 248B68665A4BF1FC3C2A59BC9323988D8ACDE51C\n",
      "missing abstract count: 0\n"
     ]
    }
   ],
   "source": [
    "def generate_json_output(queries):\n",
    "    res = []\n",
    "    for q in queries:\n",
    "        response = urllib2.urlopen(q)\n",
    "        hit = json.load(response)\n",
    "        missing_abst_count = 0\n",
    "        article = dict()\n",
    "        article['doi'] = hit['doi']\n",
    "        article['istex_id'] = hit['id']\n",
    "        article['title'] = hit['title']\n",
    "        try:\n",
    "            article['abstract'] = hit['abstract']\n",
    "        except:\n",
    "            print 'missing abstract of istex_id:', hit['id']\n",
    "            missing_abst_count = missing_abst_count + 1\n",
    "            article['abstract'] = ' '\n",
    "        article['source'] = ['istex']\n",
    "        #article['host'] = hit['host']\n",
    "        article['publicationDate'] = hit['publicationDate']\n",
    "        #article['authors'] = hit['author']\n",
    "        try:\n",
    "            article['KeyWords'] = hit['keywords']['teeft']\n",
    "        except:\n",
    "            article['KeyWords'] = []\n",
    "        res.append(article)\n",
    "    print 'missing abstract count:', missing_abst_count\n",
    "    return res\n",
    "queries_results = generate_json_output(queries)\n",
    "queries_results\n",
    "json.dump(queries_results, open('initial_corpus_BC.json', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'KeyWords': [u'tanzania',\n",
       "   u'usambara',\n",
       "   u'rewood',\n",
       "   u'munishi',\n",
       "   u'biodiversity',\n",
       "   u'building poles',\n",
       "   u'tree resources',\n",
       "   u'ecol',\n",
       "   u'agroforestry',\n",
       "   u'catchment',\n",
       "   u'usambaras',\n",
       "   u'biodiversity conservation',\n",
       "   u'blackwell publishing',\n",
       "   u'basal',\n",
       "   u'journal compilation',\n",
       "   u'suppl',\n",
       "   u'natural forests',\n",
       "   u'tree species',\n",
       "   u'firewood',\n",
       "   u'fodder',\n",
       "   u'species diversity',\n",
       "   u'west usambaras',\n",
       "   u'rewood timber',\n",
       "   u'west usambara mountains',\n",
       "   u'farmland',\n",
       "   u'basal area',\n",
       "   u'west usambara',\n",
       "   u'sokoine university',\n",
       "   u'local people',\n",
       "   u'tree management',\n",
       "   u'tree species composition',\n",
       "   u'diversity index',\n",
       "   u'montane forests',\n",
       "   u'forestry',\n",
       "   u'agroforestry systems',\n",
       "   u'agricultural crops',\n",
       "   u'diversity indices',\n",
       "   u'agricultural landscapes',\n",
       "   u'species richness',\n",
       "   u'usambara mountains',\n",
       "   u'conservation timber',\n",
       "   u'natural forest',\n",
       "   u'species composition',\n",
       "   u'biodiversity management',\n",
       "   u'dominant species',\n",
       "   u'albizia gummifera',\n",
       "   u'munishi shear',\n",
       "   u'drier areas',\n",
       "   u'different tree species',\n",
       "   u'newtonia buchananii',\n",
       "   u'syzygium guineense',\n",
       "   u'other hand',\n",
       "   u'ficus capensis',\n",
       "   u'tree crops',\n",
       "   u'land acquisition',\n",
       "   u'forest biology',\n",
       "   u'high tree species richness',\n",
       "   u'botanical family reasons',\n",
       "   u'retention planning',\n",
       "   u'agricultural landscape',\n",
       "   u'west usambara tanzania',\n",
       "   u'timber timber',\n",
       "   u'edible fruits',\n",
       "   u'medicine timber',\n",
       "   u'high diversity',\n",
       "   u'population pressure',\n",
       "   u'other studies',\n",
       "   u'various purposes',\n",
       "   u'tree diversity',\n",
       "   u'uluguru forests',\n",
       "   u'household income',\n",
       "   u'traditional medicine',\n",
       "   u'primary health care',\n",
       "   u'medicinal plants',\n",
       "   u'sustainable management',\n",
       "   u'farmers tree',\n",
       "   u'agricultural university',\n",
       "   u'plant handbook',\n",
       "   u'afromontane rain forests',\n",
       "   u'fuel wood'],\n",
       "  'abstract': u'Species richness and diversity in agricultural landscapes are important characteristics of agro\\u2010ecosystems that determine their potential for biodiversity conservation. This study assessed species composition, richness, diversity and local use of on\\u2010farm trees in predominantly agricultural landscape in the Usambaras, north\\u2010eastern Tanzania. Assessments were carried out in 90 randomly selected individual farmlands. We identified 47 tree species belonging to 23 families and used by local people for various purposes. The most dominant species were Albizia gummifera (Gmel) C. A. Sm. (mkenge), Parinari excelsa Sabine ssp holstii (Engl.) R. Grah. (muula), Newtonia buchananii (Bak.) Gilb. and Bout (mkufi), Syzygium guineense (Willd) D.C. (mshihwi), Ficus capensis L. (mvumo) and Casearia engleri (Gilg) (mkokoko). These species are indigenous and were retained from existing ones during farm clearing. The species diversity (by the Simpson\\u2019s and Shannon\\u2013Wiener diversity indices of 0.07 and 2.8 respectively) is comparable with that of adjacent natural forests. There are apparently high on\\u2010farm tree species richness and diversity, the conservation of which can contribute to ex\\u2010situ biodiversity conservation. On\\u2010farm tree management should be encouraged, especially where there is population pressure on the existing natural forests. Research to enhance sustainability of the farming system is important for biodiversity management on farmlands.',\n",
       "  'doi': [u'10.1111/j.1365-2028.2008.00931.x'],\n",
       "  'istex_id': u'3380E08507039046989A79332DE2D846BBBDBA94',\n",
       "  'publicationDate': u'2008',\n",
       "  'source': ['istex'],\n",
       "  'title': u'Tree species composition and local use in agricultural landscapes of west Usambaras Tanzania'},\n",
       " {'KeyWords': [u'biodiversity',\n",
       "   u'lindenmayer',\n",
       "   u'ecosystem',\n",
       "   u'ecol',\n",
       "   u'conserv',\n",
       "   u'biol',\n",
       "   u'climate change',\n",
       "   u'forest biodiversity',\n",
       "   u'connectivity',\n",
       "   u'fischer',\n",
       "   u'biodiversity conservation',\n",
       "   u'biota',\n",
       "   u'rapid climate change',\n",
       "   u'hobbs',\n",
       "   u'ecology',\n",
       "   u'york academy',\n",
       "   u'environ',\n",
       "   u'canberra',\n",
       "   u'native forests',\n",
       "   u'natural disturbances',\n",
       "   u'taxon',\n",
       "   u'appl',\n",
       "   u'invasive',\n",
       "   u'sustainable',\n",
       "   u'island press',\n",
       "   u'british columbia',\n",
       "   u'forest conversion',\n",
       "   u'proc',\n",
       "   u'felton',\n",
       "   u'plantation establishment',\n",
       "   u'forest ecosystems',\n",
       "   u'aust',\n",
       "   u'parmesan',\n",
       "   u'noss',\n",
       "   u'forman',\n",
       "   u'ecological processes',\n",
       "   u'forestry',\n",
       "   u'species richness',\n",
       "   u'ecosystem processes',\n",
       "   u'acad',\n",
       "   u'ipcc',\n",
       "   u'boreal',\n",
       "   u'conservation biology',\n",
       "   u'colonization',\n",
       "   u'pulpwood',\n",
       "   u'ecological',\n",
       "   u'logging',\n",
       "   u'millennium ecosystem assessment',\n",
       "   u'natural forests',\n",
       "   u'forest biodiversity conservation',\n",
       "   u'cambridge university press',\n",
       "   u'melbourne',\n",
       "   u'forest wildlife management conservation',\n",
       "   u'biological legacies',\n",
       "   u'australian forestry',\n",
       "   u'forest reserves',\n",
       "   u'invasive species',\n",
       "   u'spatial pattern',\n",
       "   u'csiro publishing',\n",
       "   u'resilience',\n",
       "   u'many species',\n",
       "   u'world commission',\n",
       "   u'landscape change',\n",
       "   u'forest trees',\n",
       "   u'habitat fragmentation',\n",
       "   u'biodiversity values',\n",
       "   u'sustainable development',\n",
       "   u'aquatic ecosystems',\n",
       "   u'tree plantations',\n",
       "   u'habitat loss',\n",
       "   u'forest management',\n",
       "   u'habitat connectivity',\n",
       "   u'conservation',\n",
       "   u'fauna',\n",
       "   u'hartley',\n",
       "   u'regeneration',\n",
       "   u'nature conservation',\n",
       "   u'tropical forests',\n",
       "   u'disturbance regimes',\n",
       "   u'agricultural land',\n",
       "   u'forest biota',\n",
       "   u'hydrological regimes',\n",
       "   u'soil erosion',\n",
       "   u'long rotations',\n",
       "   u'high levels',\n",
       "   u'species loss',\n",
       "   u'negative impacts',\n",
       "   u'landscape connectivity',\n",
       "   u'forest resilience',\n",
       "   u'biodiversity hotspots',\n",
       "   u'native vegetation',\n",
       "   u'exotic species',\n",
       "   u'spatial distribution',\n",
       "   u'global change biol',\n",
       "   u'trends ecol',\n",
       "   u'structural complexity',\n",
       "   u'spatial scale',\n",
       "   u'many jurisdictions',\n",
       "   u'such kinds',\n",
       "   u'major changes',\n",
       "   u'range shifts',\n",
       "   u'other kinds',\n",
       "   u'biodiversity loss',\n",
       "   u'management objectives',\n",
       "   u'agriculture organisation',\n",
       "   u'knowledge gaps',\n",
       "   u'oxford university press',\n",
       "   u'global warming',\n",
       "   u'native forest',\n",
       "   u'landscape heterogeneity',\n",
       "   u'reserve design',\n",
       "   u'corresponding impacts',\n",
       "   u'domestic livestock',\n",
       "   u'many cases',\n",
       "   u'profound effects',\n",
       "   u'plantation management',\n",
       "   u'sensu bennett',\n",
       "   u'gill',\n",
       "   u'temperate',\n",
       "   u'vegetation',\n",
       "   u'global',\n",
       "   u'forest',\n",
       "   u'plantation',\n",
       "   u'australia',\n",
       "   u'disturbance',\n",
       "   u'native fauna',\n",
       "   u'other approaches',\n",
       "   u'urgent need',\n",
       "   u'relative merits',\n",
       "   u'conservation biologists',\n",
       "   u'reserve selection',\n",
       "   u'southeastern australia',\n",
       "   u'support species',\n",
       "   u'structural retention',\n",
       "   u'regeneration harvest',\n",
       "   u'understory thickets',\n",
       "   u'fuel loads',\n",
       "   u'biodiversity management',\n",
       "   u'forest wildlife management conservation logging',\n",
       "   u'many parts',\n",
       "   u'management practices',\n",
       "   u'silvicultural systems',\n",
       "   u'considerable merit',\n",
       "   u'traditional logging',\n",
       "   u'boreal forests',\n",
       "   u'potential effects',\n",
       "   u'management strategies',\n",
       "   u'american foresters',\n",
       "   u'logging effects',\n",
       "   u'carbon dioxide',\n",
       "   u'other greenhouse gases',\n",
       "   u'major driver',\n",
       "   u'particular groups',\n",
       "   u'forest landscape',\n",
       "   u'seed dispersal',\n",
       "   u'mountain pine beetle',\n",
       "   u'dendroctonus ponderosae',\n",
       "   u'lodgepole pine',\n",
       "   u'natural disturbance regimes',\n",
       "   u'natural resources',\n",
       "   u'many investigations',\n",
       "   u'logging impacts',\n",
       "   u'robust studies',\n",
       "   u'effective strategies',\n",
       "   u'important part',\n",
       "   u'resource managers',\n",
       "   u'many decades',\n",
       "   u'negative effects',\n",
       "   u'northern hemisphere',\n",
       "   u'tropical nations',\n",
       "   u'feral predators',\n",
       "   u'human populations',\n",
       "   u'biodiversity value',\n",
       "   u'various groups',\n",
       "   u'forest composition',\n",
       "   u'vegetation types',\n",
       "   u'cavity trees',\n",
       "   u'species interactions',\n",
       "   u'invasive plant species',\n",
       "   u'sequester carbon',\n",
       "   u'pasture lands',\n",
       "   u'higher species richness',\n",
       "   u'landscape context',\n",
       "   u'other cases',\n",
       "   u'harvest units',\n",
       "   u'pulp production',\n",
       "   u'such approaches',\n",
       "   u'fossil fuels',\n",
       "   u'plantation trees',\n",
       "   u'carbon storage',\n",
       "   u'food production',\n",
       "   u'carbon emissions',\n",
       "   u'forest mosaics',\n",
       "   u'temperate forests',\n",
       "   u'species persistence',\n",
       "   u'extensive areas',\n",
       "   u'large areas',\n",
       "   u'bird communities',\n",
       "   u'other factors',\n",
       "   u'australian capital territory',\n",
       "   u'plantation forests',\n",
       "   u'forest subject',\n",
       "   u'annual rates',\n",
       "   u'habitat destruction',\n",
       "   u'agricultural landscapes',\n",
       "   u'boreal forest',\n",
       "   u'land conversion',\n",
       "   u'natural disturbance',\n",
       "   u'forest diversity',\n",
       "   u'flammable australia',\n",
       "   u'williams gill',\n",
       "   u'conservation priorities',\n",
       "   u'perverse outcomes',\n",
       "   u'forest fauna',\n",
       "   u'royal zoological society',\n",
       "   u'cumulative effects',\n",
       "   u'other land',\n",
       "   u'broad kinds',\n",
       "   u'other issues',\n",
       "   u'forested landscapes',\n",
       "   u'broad strategies',\n",
       "   u'fischer lindenmayer',\n",
       "   u'climate change impacts',\n",
       "   u'management implications',\n",
       "   u'agricultural organization',\n",
       "   u'broad topics',\n",
       "   u'such problems',\n",
       "   u'forest sustainability',\n",
       "   u'national forest',\n",
       "   u'kohm franklin',\n",
       "   u'ecological consequences',\n",
       "   u'blackwell publishing',\n",
       "   u'environment australia',\n",
       "   u'novel ecosystems',\n",
       "   u'human activity',\n",
       "   u'nations environment programme',\n",
       "   u'reserves',\n",
       "   u'mosaic'],\n",
       "  'abstract': u\"Forests are critical for the world's biodiversity, the regulation of the Earth's climate, and the provision of goods and services for humans. This review focuses on four broad topics: (1) key processes threatening forest biodiversity; (2) broad strategies for mitigating threatening processes; (3) climate change and forest biodiversity; and, (4) plantations and biodiversity. How key issues within these broad topics are addressed will have profound effects on forest biodiversity and the Earth's climate. A significant global problem for biodiversity conservation is the conversion of natural forests to other land uses, both in developing and developed nations; ways must be urgently identified to halt forest conversion. When forests are logged for timber or pulpwood and then regenerated, impacts on biodiversity are harder to quantify than when forests are converted to other land uses. Hence, the effectiveness of efforts to mitigate such impacts (where they occur) is frequently not well known. Climate change may result in substantial changes to forest ecosystems, and its effects may interact in additive or cumulative ways with other human disturbances in forests, although work on such combinations of impacts is in its infancy. The establishment of plantations of trees is frequently proposed to sequester large amounts of carbon and/or produce biofuels to mitigate the climate\\u2010change effects. However, there is potential for perverse outcomes, such as biodiversity loss where plantation establishment is narrowly focused and other environmental values are ignored.\",\n",
       "  'doi': [u'10.1111/j.1749-6632.2009.04148.x'],\n",
       "  'istex_id': u'B1325BFE2FCE17A6CFF7081288F2CC9E529A636C',\n",
       "  'publicationDate': u'2009',\n",
       "  'source': ['istex'],\n",
       "  'title': u'Forest Wildlife Management and Conservation'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_results[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(initial_corpus, open('initial_corpus_BC.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_output(queries):\n",
    "    q = q+'&from='+str(f)\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    article = dict()\n",
    "    res = np.array(range(len(data['hits'])), dtype=np.object)\n",
    "    missing_abst_count = 0\n",
    "    for (i, hit) in enumerate(data['hits']):\n",
    "        article = dict()\n",
    "        article['doi'] = hit['doi']\n",
    "        article['istex_id'] = hit['id']\n",
    "        article['title'] = hit['title']\n",
    "        try:\n",
    "            article['abstract'] = hit['abstract']\n",
    "        except:\n",
    "            print 'missing abstract of istex_id:', hit['id']\n",
    "            missing_abst_count = missing_abst_count + 1\n",
    "            article['abstract'] = ' '\n",
    "        article['source'] = ['istex']\n",
    "        #article['host'] = hit['host']\n",
    "        article['publicationDate'] = hit['publicationDate']\n",
    "        #article['authors'] = hit['author']\n",
    "        try:\n",
    "            article['KeyWords'] = hit['keywords']['teeft']\n",
    "        except:\n",
    "            article['KeyWords'] = []\n",
    "        res[i] = article\n",
    "    print 'page '+str(f)+' missing results count:', missing_abst_count\n",
    "    return res\n",
    "\n",
    "def json_query_from(q, f):\n",
    "    q = q+'&from='+str(f)\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    article = dict()\n",
    "    res = np.array(range(len(data['hits'])), dtype=np.object)\n",
    "    missing_abst_count = 0\n",
    "    for (i, hit) in enumerate(data['hits']):\n",
    "        article = dict()\n",
    "        article['doi'] = hit['doi']\n",
    "        article['istex_id'] = hit['id']\n",
    "        article['title'] = hit['title']\n",
    "        try:\n",
    "            article['abstract'] = hit['abstract']\n",
    "        except:\n",
    "            print 'missing abstract of istex_id:', hit['id']\n",
    "            missing_abst_count = missing_abst_count + 1\n",
    "            article['abstract'] = ' '\n",
    "        article['source'] = ['istex']\n",
    "        #article['host'] = hit['host']\n",
    "        article['publicationDate'] = hit['publicationDate']\n",
    "        #article['authors'] = hit['author']\n",
    "        try:\n",
    "            article['KeyWords'] = hit['keywords']['teeft']\n",
    "        except:\n",
    "            article['KeyWords'] = []\n",
    "        res[i] = article\n",
    "    print 'page '+str(f)+' missing results count:', missing_abst_count\n",
    "    return res\n",
    "\n",
    "def json_query(q, file_name):\n",
    "    response = urllib2.urlopen(q)\n",
    "    data = json.load(response)\n",
    "    nb_requests = 1 + data['total'] / 1000\n",
    "    if nb_requests > 10: # maximum number of pages due to API pagination restrection\n",
    "        nb_requests = 10\n",
    "    page_results = json_query_from(q, 0)\n",
    "    for i in range(nb_requests)[1:]:\n",
    "        f = i * 1000\n",
    "        next_request = json_query_from(q, f)\n",
    "        page_results = np.hstack((page_results, next_request))\n",
    "    page_results = page_results.tolist()\n",
    "    json.dump(page_results, open(file_name, 'wb'))\n",
    "    print 'json file was dumped successfully at;'+file_name\n",
    "    return page_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing abstract of istex_id: 62C727AE41EF251290DD4CCBD614916AC09C818C\n",
      "missing abstract of istex_id: C717A79D0C721B2D0DD2570E2DC90DDA00A31E36\n",
      "missing abstract of istex_id: FA39C2494878D3A76117709E0383173AB29128AF\n",
      "missing abstract of istex_id: 83CE7A03CDCEECADBCDA31AEE54A8065D744AEC1\n",
      "missing abstract of istex_id: 4CF992BA93D47667B4923333817304DE803141F6\n",
      "missing abstract of istex_id: FD80A5067300EBCBB5F9A3B252961D77A70F3AD1\n",
      "missing abstract of istex_id: 292B0D3441B1E3B283066B67C6E79A149D6C6FE4\n",
      "missing abstract of istex_id: D6480A3B45FDD9F18DBFDC72B98B6CC95745C713\n",
      "missing abstract of istex_id: 80A196DF94682C904CB47D23778698CFB6ED7986\n",
      "missing abstract of istex_id: 343318EE74BA1C6E1CE573AB0B501BBAC413A2BA\n",
      "missing abstract of istex_id: B407EFE7684D5E086829D93ACD3CAED3413DFFC0\n",
      "missing abstract of istex_id: 87449EC9F085F42CA889920624379CCC8F1BD428\n",
      "missing abstract of istex_id: 94D846D4602C83206C33A78270904A71385C5A22\n",
      "missing abstract of istex_id: BA831D079AF57A331E050F1A3DC75DC587E832EC\n",
      "missing abstract of istex_id: BC4CFB975413D21FC9E03BEAD5BEA21722099E5F\n",
      "missing abstract of istex_id: 2B976F46CBCD3B5DBDC8DBD20B395B5529E3A460\n",
      "missing abstract of istex_id: 246AB6E7C8AF2DBDDA66CF07C5315A3E160AEEE0\n",
      "missing abstract of istex_id: 4999315ABCB555CCFE4615593ACC971CCB4961C9\n",
      "missing abstract of istex_id: 19AA01C9CC3CB9905DA1E5465121D13646B61D01\n",
      "missing abstract of istex_id: F3C230AAEA9B3EEAEA40930862A6B84678CCE4F9\n",
      "missing abstract of istex_id: A5E80D07A07819723D64A552310B01D15F89F9B4\n",
      "missing abstract of istex_id: 389EA449A49729AF586DD3A6F2917E201263B97D\n",
      "missing abstract of istex_id: 34D97BC3585364AAF647C7600080C4675CD3CB1D\n",
      "missing abstract of istex_id: 50D9327B255D9062B10E7086113F167943C89BA0\n",
      "missing abstract of istex_id: C0595314CDC1C5A241D1007E168962325220497C\n",
      "missing abstract of istex_id: 0040FF733B0B68D2FB05102F5D7FE630B3667BCE\n",
      "missing abstract of istex_id: B31D5A5C2506632628D547DB00CE60E3AB61DD61\n",
      "missing abstract of istex_id: 609A5DDC907DE807D5AF98E6A633631FA98BCC46\n",
      "missing abstract of istex_id: 9771DF84C1F4376EA630B9732D3E0668705D8160\n",
      "missing abstract of istex_id: CF006E73372ADDEA37334A1F2DDEB072C128B3CA\n",
      "missing abstract of istex_id: D4C81271DA2BE6DFBF5C3478B953FB2F2557B1EF\n",
      "page 0 missing results count: 31\n",
      "json file was dumped successfully at;test_articles.json\n"
     ]
    }
   ],
   "source": [
    "q = 'https://api.istex.fr/document/?q=((subject.value:%22Biodiversity%20Conservation%22%20OR%20keywords:%22Biodiversity%20Conservation%22%20)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id,doi,author,keywords,title,publicationDate,abstract,host'\n",
    "file_name = 'test_articles.json'\n",
    "test_json = json_query(q, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = 'https://api.istex.fr/document/?q=((subject.value:%22Biodiversity%20Conservation%22%20OR%20keywords:%22Biodiversity%20Conservation%22%20)%20AND%20(qualityIndicators.abstractWordCount:[35%20500]%20AND%20qualityIndicators.pdfPageCount:[3%2060]%20AND%20publicationDate:[1990%202016]%20AND%20language:(%22eng%22%20OR%20%22unknown%22)%20AND%20genre:(%22research_article%22%20OR%20%22conference[eBooks]%22%20OR%20%22article%22%20)%20))&size=1000&output=id,doi,author,keywords,title,publicationDate,abstract,host'\n",
    "file_name = 'initial__articles.json'\n",
    "test_json = json_query(q, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3h_results = pickle.load(open('results/BC0t_results.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3h_syn_results = pickle.load(open('results/BC0t_1_RFC_syn_results.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "babel_test_intersection = find_intersection(test,babelnet_results)\n",
    "babel_test_intersection_size = len(babel_test_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "213\n",
      "169\n",
      "163\n",
      "137\n",
      "96\n",
      "98\n",
      "103\n",
      "103\n",
      "59\n",
      "531\n",
      "321\n",
      "220\n",
      "190\n",
      "136\n",
      "133\n",
      "130\n",
      "102\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:1000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[1000:2000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[2000:3000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[3000:4000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[4000:5000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[5000:6000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[6000:7000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[7000:8000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[8000:9000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[9000:10000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[10000:20000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[20000:30000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[30000:40000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[40000:50000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[50000:60000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[60000:70000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[70000:80000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[80000:90000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[90000:100000])[0], babelnet_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649\n",
      "272\n",
      "221\n",
      "173\n",
      "179\n",
      "182\n",
      "181\n",
      "151\n",
      "177\n",
      "1356\n",
      "852\n",
      "506\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[100000:200000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[200000:300000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[300000:400000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[400000:500000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[500000:600000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[600000:700000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[700000:800000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[800000:900000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[900000:1000000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[1000000:2000000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[2000000:3000000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[3000000:4000000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[4000000:])[0], babelnet_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "517\n",
      "686\n",
      "849\n",
      "986\n",
      "1082\n",
      "1180\n",
      "1283\n",
      "1386\n",
      "1445\n",
      "1976\n",
      "2297\n",
      "2517\n",
      "2707\n",
      "2843\n",
      "2976\n",
      "3106\n",
      "3208\n",
      "3308\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:1000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:2000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:3000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:4000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:5000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:6000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:7000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:8000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:9000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:10000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:20000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:30000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:40000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:50000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:60000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:70000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:80000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:90000])[0], babelnet_results))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:100000])[0], babelnet_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:1000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[1000:2000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[2000:3000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[3000:4000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[4000:5000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[5000:6000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[6000:7000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[7000:8000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[8000:9000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[9000:10000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[10000:20000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[20000:30000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[30000:40000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[40000:50000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[50000:60000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[60000:70000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[70000:80000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[80000:90000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[90000:100000])[0], babel_test_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[100000:200000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[200000:300000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[300000:400000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[400000:500000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[500000:600000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[600000:700000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[700000:800000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[800000:900000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[900000:1000000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[1000000:2000000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[2000000:3000000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[3000000:4000000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[4000000:])[0], babel_test_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:1000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:2000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:3000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:4000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:5000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:6000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:7000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:8000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:9000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:10000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:20000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:30000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:40000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:50000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:60000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:70000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:80000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:90000])[0], babel_test_intersection))\n",
    "print len (find_intersection(pd.DataFrame(s3h_results.items()[:100000])[0], babel_test_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0A8071F67FEE1A6BF14E5069288C9E67B8A3F2AA</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2946E8F7A320DEF1BEFBB26A21CB95E0492FE580</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4B1F119CA2211850D0FA755EB5D43D0C26A0358B</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>396205226F7DDC8B48E8FBEF16684C7703D905D1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FA9D1081F58F45F8168813C84C6D9223BBB1873E</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7E4EF8002D4FEA0DE518902EC3D79AE81B29D79A</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>277C9AC0AF7CB753DBF8609EE57C946AAEF0A172</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FAD58615F61C6D0F70C3F054B27D8D7B75AC7C34</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D037DE25D3FAE7E3D1F7EC618602687C735224C8</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71ADE7014F39A00FDE2D4530BCDD5DF7E509B179</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>E89D1B5A74E9E95A422B7FAEB5415FFABB96BB24</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>82D1903BD41762184DD13673F1C2EE81492146BA</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4823D2BB6AEC5BFC2714F37562BD4879B24D813B</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>978161F59C993E54C21EB389B9740DD2823F9B56</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A46E2E793CCE401D1904EE6304631705283723A0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>73E187A48A0D7E5A60113A5F383CF353C5EFD77B</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>E4B8155B65A0EE4AAD33F5DBEEF565F31ED172DE</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>609C84191239074E21B3B408C6E8AA2BA3FDD3E7</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C0796575BA62A06426215C74C9F15B8664ABA59C</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>941079769D5D18B62443DF6547176AF2A7E09E15</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2D7E81BAF55D1C892830A3336E81862749469B3D</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>882F423AB19B984B34432EAC394F30FDEC1D9241</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>724B816DB338A9C1EDFBD11BC2853868876E3D75</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>05A795293716CEB99D83F7E811AD0695C58E5EF3</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0EA19C5868F0280525CFF1EBC2BEA234F1115CE7</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>630BE4C3887ECF7ACBA1B8BBE1D134F2B5D703CC</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>80B73E520426E3ADE858FC94F82EDD4F44B8D892</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>FC492A54848826AD03E4BA419E4E72114C8B5706</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A47E372634664B30F15AEB6E21327276FB83C0C8</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>039D68A9EF11CA73D637AF4E337E7A6932B7CA37</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>BC2FAD48CC3BDD96A5CF6AA696A82336213F055B</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>ACA116610910FAC9F1ADE2AD98BB15590B98D9C1</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>9F6CE09AFA4422FBFDAF710AEFDE57DEBF4E55C6</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>743C338E7C4EEDA43E4454F754B54E1F2FE0BA2E</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>5BED038D76384C4174CEC2DA5317B4252FA93088</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>411776A76EDD2891CF1B16344E52E9CA0D5AECC4</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>84A89DB335AE21CDA850C8586B9E9DF3DADB3FC9</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>8606C23B15811A9F250D3FECE3A375795411B0D9</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>A915A7293C81CC4EB08FBCFCD3D0CF745EE2CA7B</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>1E07B4853FAE1077514C63BC670C6A0D368FD524</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0D23A60FB7BAA8D4036D80D6BC6D6FA068DC1A1D</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>4477463EE8BD6BE1E64CCADEFC0E3A5B80C3A176</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>4484E046DF3F1DC85F799031334D77503665C372</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>6C395372036972FEF2426C76A58E85B4CFF06D88</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>B66B7827F6ED7A519A0C29B60AAA3A217E2F187D</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>08EC4113EC9DF743EB4A4C782B99062E902FDEF8</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>62E702370DA675EBD0FEF256E8D8798D6E4325DA</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>5F5917074C6C5D499F9D3BAE5913C2D8497218CC</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>ABEA4C3756BEF631D831474E8AE3B4F285434CE9</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>F6F233A944469CB9337A9EE49B2166A7B5A11A86</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>576A8BB5A9D983A9A1F38951997573348B74A53C</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>8BF001293B2879298D858778AE49BC83E2C58A51</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>F2BA5D0BC2EB269FA3ACA2EF1C4A531E46F50BCC</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>8E6BA8026CF4A4697C8E2DD55564E614538E3D79</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>20DD2FB52AAE6A75A9E37185D323CB172F5EE1F3</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>A61B5B3D43D4069FB0A47387CB1A2474E27CBE15</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>BDC7E9A899C239C57861783999F459BB56AC652A</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>57C2F168EA01528A006460E95BA4FE3D654A5776</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>CC81B839CD5A59EB1C34CBB2342C7FB6C2BDC62C</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>51968CA1D613439E800031A09FBED20B7D3F793F</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            0      1\n",
       "0    0A8071F67FEE1A6BF14E5069288C9E67B8A3F2AA  1.000\n",
       "1    2946E8F7A320DEF1BEFBB26A21CB95E0492FE580  1.000\n",
       "2    4B1F119CA2211850D0FA755EB5D43D0C26A0358B  1.000\n",
       "3    396205226F7DDC8B48E8FBEF16684C7703D905D1  1.000\n",
       "4    FA9D1081F58F45F8168813C84C6D9223BBB1873E  1.000\n",
       "5    7E4EF8002D4FEA0DE518902EC3D79AE81B29D79A  1.000\n",
       "6    277C9AC0AF7CB753DBF8609EE57C946AAEF0A172  1.000\n",
       "7    FAD58615F61C6D0F70C3F054B27D8D7B75AC7C34  1.000\n",
       "8    D037DE25D3FAE7E3D1F7EC618602687C735224C8  1.000\n",
       "9    71ADE7014F39A00FDE2D4530BCDD5DF7E509B179  1.000\n",
       "10   E89D1B5A74E9E95A422B7FAEB5415FFABB96BB24  1.000\n",
       "11   82D1903BD41762184DD13673F1C2EE81492146BA  1.000\n",
       "12   4823D2BB6AEC5BFC2714F37562BD4879B24D813B  1.000\n",
       "13   978161F59C993E54C21EB389B9740DD2823F9B56  1.000\n",
       "14   A46E2E793CCE401D1904EE6304631705283723A0  1.000\n",
       "15   73E187A48A0D7E5A60113A5F383CF353C5EFD77B  1.000\n",
       "16   E4B8155B65A0EE4AAD33F5DBEEF565F31ED172DE  1.000\n",
       "17   609C84191239074E21B3B408C6E8AA2BA3FDD3E7  1.000\n",
       "18   C0796575BA62A06426215C74C9F15B8664ABA59C  1.000\n",
       "19   941079769D5D18B62443DF6547176AF2A7E09E15  1.000\n",
       "20   2D7E81BAF55D1C892830A3336E81862749469B3D  1.000\n",
       "21   882F423AB19B984B34432EAC394F30FDEC1D9241  1.000\n",
       "22   724B816DB338A9C1EDFBD11BC2853868876E3D75  1.000\n",
       "23   05A795293716CEB99D83F7E811AD0695C58E5EF3  1.000\n",
       "24   0EA19C5868F0280525CFF1EBC2BEA234F1115CE7  1.000\n",
       "25   630BE4C3887ECF7ACBA1B8BBE1D134F2B5D703CC  1.000\n",
       "26   80B73E520426E3ADE858FC94F82EDD4F44B8D892  1.000\n",
       "27   FC492A54848826AD03E4BA419E4E72114C8B5706  1.000\n",
       "28   A47E372634664B30F15AEB6E21327276FB83C0C8  1.000\n",
       "29   039D68A9EF11CA73D637AF4E337E7A6932B7CA37  1.000\n",
       "..                                        ...    ...\n",
       "970  BC2FAD48CC3BDD96A5CF6AA696A82336213F055B  0.986\n",
       "971  ACA116610910FAC9F1ADE2AD98BB15590B98D9C1  0.986\n",
       "972  9F6CE09AFA4422FBFDAF710AEFDE57DEBF4E55C6  0.986\n",
       "973  743C338E7C4EEDA43E4454F754B54E1F2FE0BA2E  0.986\n",
       "974  5BED038D76384C4174CEC2DA5317B4252FA93088  0.986\n",
       "975  411776A76EDD2891CF1B16344E52E9CA0D5AECC4  0.986\n",
       "976  84A89DB335AE21CDA850C8586B9E9DF3DADB3FC9  0.986\n",
       "977  8606C23B15811A9F250D3FECE3A375795411B0D9  0.986\n",
       "978  A915A7293C81CC4EB08FBCFCD3D0CF745EE2CA7B  0.986\n",
       "979  1E07B4853FAE1077514C63BC670C6A0D368FD524  0.986\n",
       "980  0D23A60FB7BAA8D4036D80D6BC6D6FA068DC1A1D  0.986\n",
       "981  4477463EE8BD6BE1E64CCADEFC0E3A5B80C3A176  0.986\n",
       "982  4484E046DF3F1DC85F799031334D77503665C372  0.986\n",
       "983  6C395372036972FEF2426C76A58E85B4CFF06D88  0.986\n",
       "984  B66B7827F6ED7A519A0C29B60AAA3A217E2F187D  0.986\n",
       "985  08EC4113EC9DF743EB4A4C782B99062E902FDEF8  0.986\n",
       "986  62E702370DA675EBD0FEF256E8D8798D6E4325DA  0.986\n",
       "987  5F5917074C6C5D499F9D3BAE5913C2D8497218CC  0.986\n",
       "988  ABEA4C3756BEF631D831474E8AE3B4F285434CE9  0.986\n",
       "989  F6F233A944469CB9337A9EE49B2166A7B5A11A86  0.986\n",
       "990  576A8BB5A9D983A9A1F38951997573348B74A53C  0.986\n",
       "991  8BF001293B2879298D858778AE49BC83E2C58A51  0.986\n",
       "992  F2BA5D0BC2EB269FA3ACA2EF1C4A531E46F50BCC  0.986\n",
       "993  8E6BA8026CF4A4697C8E2DD55564E614538E3D79  0.986\n",
       "994  20DD2FB52AAE6A75A9E37185D323CB172F5EE1F3  0.986\n",
       "995  A61B5B3D43D4069FB0A47387CB1A2474E27CBE15  0.986\n",
       "996  BDC7E9A899C239C57861783999F459BB56AC652A  0.986\n",
       "997  57C2F168EA01528A006460E95BA4FE3D654A5776  0.986\n",
       "998  CC81B839CD5A59EB1C34CBB2342C7FB6C2BDC62C  0.986\n",
       "999  51968CA1D613439E800031A09FBED20B7D3F793F  0.986\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(s3h_results.items()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_babelnet(results, top_n, test):\n",
    "\ttop = results[:top_n]\n",
    "\tprint 'matched rate at '+str(top_n)+':' , len(find_intersection(test,top))/float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BabelNet Method Evaluation:\n",
      "matched rate at 1000: 0.0181818181818\n",
      "matched rate at 2000: 0.0181818181818\n",
      "matched rate at 3000: 0.0363636363636\n",
      "matched rate at 4000: 0.0727272727273\n",
      "matched rate at 5000: 0.109090909091\n",
      "matched rate at 6000: 0.109090909091\n",
      "matched rate at 7000: 0.145454545455\n",
      "matched rate at 8000: 0.218181818182\n",
      "matched rate at 9000: 0.218181818182\n",
      "matched rate at 10000: 0.218181818182\n"
     ]
    }
   ],
   "source": [
    "print 'BabelNet Method Evaluation:'\n",
    "for i in range(1000,11000,1000):\n",
    "    eval_babelnet(babelnet_results, i, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_use_case(sorted_results, top_n, test):\n",
    "\ttop = pd.DataFrame(sorted_results.items()[:top_n])\n",
    "\tprint 'matched rate at '+str(top_n)+':' , len(find_intersection(test,top[0]))/float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "matched rate at 1000: 0.127272727273\n",
      "matched rate at 2000: 0.236363636364\n",
      "matched rate at 3000: 0.290909090909\n",
      "matched rate at 4000: 0.309090909091\n",
      "matched rate at 5000: 0.327272727273\n",
      "matched rate at 6000: 0.345454545455\n",
      "matched rate at 7000: 0.363636363636\n",
      "matched rate at 8000: 0.436363636364\n",
      "matched rate at 9000: 0.436363636364\n",
      "matched rate at 10000: 0.454545454545\n"
     ]
    }
   ],
   "source": [
    "print 'Evaluation:'\n",
    "for i in range(1000,11000,1000):\n",
    "    eval_use_case(s3h_results, i, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Hyprid:\n",
      "matched rate at 1000: 0.109090909091\n",
      "matched rate at 2000: 0.218181818182\n",
      "matched rate at 3000: 0.236363636364\n",
      "matched rate at 4000: 0.327272727273\n",
      "matched rate at 5000: 0.363636363636\n",
      "matched rate at 6000: 0.381818181818\n",
      "matched rate at 7000: 0.381818181818\n",
      "matched rate at 8000: 0.418181818182\n",
      "matched rate at 9000: 0.454545454545\n",
      "matched rate at 10000: 0.472727272727\n"
     ]
    }
   ],
   "source": [
    "print 'Evaluation Hyprid:'\n",
    "for i in range(1000,11000,1000):\n",
    "    eval_use_case(s3h_syn_results, i, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
